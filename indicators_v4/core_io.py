# core_io.py ‚Äî –≤–æ—Ä–∫–µ—Ä –¥–ª—è —á—Ç–µ–Ω–∏—è indicator_stream_core –∏ –±—ã—Å—Ç—Ä–æ–π –∑–∞–ø–∏—Å–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ PostgreSQL (multi-consumer + bulk upsert)

# üî∏ –ò–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
import os
import json
import logging
import asyncio
import socket
from datetime import datetime
from decimal import Decimal, ROUND_HALF_UP


# üî∏ –ö–æ–Ω—Ñ–∏–≥ (env)
CORE_STREAM = "indicator_stream_core"
CORE_GROUP = os.getenv("IV4_CORE_IO_GROUP", "group_core_io")
CONSUMER_PREFIX = os.getenv("IV4_CORE_IO_CONSUMER_PREFIX", "core_io")

CORE_IO_CONSUMERS = int(os.getenv("IV4_CORE_IO_CONSUMERS", "3"))
CORE_IO_BATCH = int(os.getenv("IV4_CORE_IO_BATCH", "2000"))
CORE_IO_BLOCK_MS = int(os.getenv("IV4_CORE_IO_BLOCK_MS", "500"))


# üî∏ –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º core_io (—Å —É—á—ë—Ç–æ–º angle=5)
def _quantize_value(str_value: str, precision: int, is_angle: bool) -> float:
    p_prec = 5 if is_angle else int(precision)
    quantize_str = "1." + "0" * p_prec
    val = Decimal(str(str_value)).quantize(Decimal(quantize_str), rounding=ROUND_HALF_UP)
    return float(val)


# üî∏ Bulk upsert –≤ indicator_values_v4 (–æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º)
async def _bulk_upsert_indicator_values(conn, rows: list[tuple[int, str, datetime, str, float]]):
    if not rows:
        return

    instance_ids = [r[0] for r in rows]
    symbols = [r[1] for r in rows]
    open_times = [r[2] for r in rows]
    param_names = [r[3] for r in rows]
    values = [r[4] for r in rows]

    await conn.execute(
        """
        INSERT INTO indicator_values_v4 (instance_id, symbol, open_time, param_name, value)
        SELECT *
        FROM UNNEST(
            $1::int4[],
            $2::text[],
            $3::timestamp[],
            $4::text[],
            $5::float8[]
        )
        ON CONFLICT (instance_id, symbol, open_time, param_name)
        DO UPDATE SET value = EXCLUDED.value, updated_at = NOW()
        """,
        instance_ids,
        symbols,
        open_times,
        param_names,
        values,
    )


# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π stream ‚Üí rows + iv4_inserted keys
def _parse_core_messages(messages, log: logging.Logger):
    records: list[tuple[int, str, datetime, str, float]] = []
    to_ack: list[str] = []

    # –¥–ª—è iv4_inserted –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ (symbol, interval, open_time, instance_id)
    event_counts: dict[tuple[str, str, datetime, int], int] = {}

    msg_total = 0
    msg_bad = 0
    params_total = 0

    for msg_id, data in messages:
        msg_total += 1
        to_ack.append(msg_id)

        try:
            symbol = data["symbol"]
            interval = data["interval"]
            instance_id = int(data["instance_id"])
            open_time = datetime.fromisoformat(data["open_time"])
            precision = int(data.get("precision", 8))

            # –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: values_json = {param_name: value_str, ...}
            if "values_json" in data:
                values = json.loads(data.get("values_json") or "{}")
                if not isinstance(values, dict) or not values:
                    msg_bad += 1
                    continue

                added = 0
                for param_name, str_value in values.items():
                    try:
                        is_angle = "angle" in str(param_name)
                        value = _quantize_value(str(str_value), precision, is_angle)
                        records.append((instance_id, symbol, open_time, str(param_name), value))
                        added += 1
                        params_total += 1
                    except Exception as e:
                        log.error(f"–û—à–∏–±–∫–∞ param (values_json): {e}")

                if added:
                    key = (symbol, interval, open_time, instance_id)
                    event_counts[key] = event_counts.get(key, 0) + added

            # —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: param_name/value (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            else:
                param_name = data["param_name"]
                str_value = data["value"]

                is_angle = "angle" in str(param_name)
                value = _quantize_value(str(str_value), precision, is_angle)
                records.append((instance_id, symbol, open_time, str(param_name), value))
                params_total += 1

                key = (symbol, interval, open_time, instance_id)
                event_counts[key] = event_counts.get(key, 0) + 1

        except Exception as e:
            msg_bad += 1
            log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —Å–æ–æ–±—â–µ–Ω–∏—è core stream: {e}", exc_info=True)

    return records, to_ack, event_counts, msg_total, msg_bad, params_total


# üî∏ –û–¥–∏–Ω consumer-loop (—á–∏—Ç–∞–µ—Ç stream –∏ –ø–∏—à–µ—Ç –≤ PG)
async def _core_io_consumer_loop(pg, redis, consumer: str):
    log = logging.getLogger(f"CORE_IO:{consumer}")

    while True:
        try:
            resp = await redis.xreadgroup(
                groupname=CORE_GROUP,
                consumername=consumer,
                streams={CORE_STREAM: ">"},
                count=CORE_IO_BATCH,
                block=CORE_IO_BLOCK_MS,
            )
            if not resp:
                continue

            # —Ä–∞—Å–ø–∞–∫—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –æ–¥–Ω–∞ –∑–∞–ø–∏—Å—å stream ‚Üí —Å–ø–∏—Å–æ–∫ messages)
            all_messages = []
            for _, messages in resp:
                all_messages.extend(messages)

            records, to_ack, event_counts, msg_total, msg_bad, params_total = _parse_core_messages(all_messages, log)

            # –ø–∏—à–µ–º –≤ PG
            if records:
                async with pg.acquire() as conn:
                    async with conn.transaction():
                        await _bulk_upsert_indicator_values(conn, records)

                # —Å—É–º–º–∏—Ä—É—é—â–∏–π –ª–æ–≥ (info)
                log.debug(
                    "batch stored (msgs=%s bad=%s params=%s records=%s keys=%s)",
                    msg_total,
                    msg_bad,
                    params_total,
                    len(records),
                    len(event_counts),
                )

                # –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Ñ–∞–∫—Ç–∞ –≤—Å—Ç–∞–≤–∫–∏ (—Ç—Ä–∏–≥–≥–µ—Ä –¥–ª—è –∞—É–¥–∏—Ç–æ—Ä–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤)
                for (symbol, interval, open_time, instance_id), cnt in event_counts.items():
                    try:
                        await redis.xadd("iv4_inserted", {
                            "symbol": symbol,
                            "interval": interval,
                            "open_time": open_time.isoformat(),
                            "instance_id": str(instance_id),
                            "param_count": str(cnt),
                        })
                    except Exception as e:
                        log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–±—ã—Ç–∏–µ –≤ iv4_inserted: {e}")

            # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            if to_ack:
                await redis.xack(CORE_STREAM, CORE_GROUP, *to_ack)

        except Exception as e:
            log.error(f"CORE_IO loop error: {e}", exc_info=True)
            await asyncio.sleep(1)


# üî∏ –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞: —Å—Ç–∞—Ä—Ç—É–µ—Ç N consumers –≤ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø–µ
async def run_core_io(pg, redis):
    log = logging.getLogger("CORE_IO")

    # —Å–æ–∑–¥–∞—Ç—å consumer-group (–µ—Å–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å)
    try:
        await redis.xgroup_create(CORE_STREAM, CORE_GROUP, id="$", mkstream=True)
        log.debug(f"Consumer group '{CORE_GROUP}' —Å–æ–∑–¥–∞–Ω–∞")
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä—É–ø–ø—ã: {e}")

    host = os.getenv("HOSTNAME") or socket.gethostname()
    pid = os.getpid()

    # —Å—Ç–∞—Ä—Ç—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ consumers
    tasks = []
    for i in range(1, max(1, CORE_IO_CONSUMERS) + 1):
        consumer = f"{CONSUMER_PREFIX}:{host}:{pid}:{i}"
        tasks.append(asyncio.create_task(_core_io_consumer_loop(pg, redis, consumer)))

    log.debug(
        "CORE_IO: started (group=%s consumers=%s batch=%s block_ms=%s)",
        CORE_GROUP,
        len(tasks),
        CORE_IO_BATCH,
        CORE_IO_BLOCK_MS,
    )

    await asyncio.gather(*tasks)