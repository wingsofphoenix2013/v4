# indicator_mw_states_back.py ‚Äî –±—ç–∫–∞–ø–æ–≤—ã–π —Ä–∞—Å—á—ë—Ç market_state –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ indicator_marketwatch_values

import asyncio
import json
import logging
from datetime import datetime, timedelta

# üî∏ –ò–º–ø–æ—Ä—Ç –ø—Ä–∞–≤–∏–ª –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–æ–¥—É–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
from indicator_mw_states import compute_direction_and_quality

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å—Ç—Ä–∏–º–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–≤–µ—á–µ–π (Bybit PG insert)
CANDLE_STREAM = "bb:pg_candle_inserted"
BACK_GROUP = "mw_state_back_group"
BACK_CONSUMER = "mw_state_back_1"

# üî∏ –û–∫–Ω–æ –æ–∂–∏–¥–∞–Ω–∏—è –∏ –±—ç–∫–∞–ø–∞
BACKFILL_START_DELAY_SEC = 60          # –∂–¥–∞—Ç—å –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞ —Å–∏—Å—Ç–µ–º—ã
BACKFILL_LOOKBACK_DAYS = 10            # —Å–∫–æ–ª—å–∫–æ —Å—É—Ç–æ–∫ –Ω–∞–∑–∞–¥ —Å–º–æ—Ç—Ä–µ—Ç—å
BACKFILL_SKIP_RECENT_HOURS = 1         # –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º live-–≤–æ—Ä–∫–µ—Ä—É)
BACKFILL_BATCH_LIMIT = 50_000          # —Å—Ç—Ä–∞—Ö–æ–≤–æ—á–Ω—ã–π –ª–∏–º–∏—Ç —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω

# üî∏ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ TF
VALID_TF = {"m5", "m15", "h1"}

# üî∏ –õ–æ–≥–≥–µ—Ä –º–æ–¥—É–ª—è
log = logging.getLogger("MW_STATE_BACK")


# üî∏ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è timestamp(ms) ‚Üí datetime UTC
def ms_to_dt(ms: int) -> datetime:
    return datetime.utcfromtimestamp(ms / 1000)


# üî∏ –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ bb:pg_candle_inserted –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å—Ç–∞—Ä—Ç–∞
async def wait_first_candle(redis) -> datetime:
    """
    –ñ–¥—ë—Ç –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ CANDLE_STREAM —á–µ—Ä–µ–∑ consumer-group BACK_GROUP.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç datetime(anchor) –ø–æ –ø–æ–ª—é timestamp (UTC), –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Ç–æ–º
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ–ø–æ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –æ–∫–Ω–∞ –±—ç–∫–∞–ø–∞.
    """
    # —Å–æ–∑–¥–∞—ë–º consumer-group, –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
    try:
        await redis.xgroup_create(CANDLE_STREAM, BACK_GROUP, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"MW_STATE_BACK: xgroup_create error: {e}")

    log.debug("MW_STATE_BACK: –æ–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ bb:pg_candle_inserted")

    while True:
        try:
            resp = await redis.xreadgroup(
                groupname=BACK_GROUP,
                consumername=BACK_CONSUMER,
                streams={CANDLE_STREAM: ">"},
                count=1,
                block=10_000,  # 10 —Å–µ–∫—É–Ω–¥
            )
            if not resp:
                continue

            to_ack = []
            anchor_dt = None

            for _, messages in resp:
                for msg_id, data in messages:
                    to_ack.append(msg_id)
                    ts_raw = data.get("timestamp")
                    try:
                        ts_ms = int(ts_raw)
                        anchor_dt = ms_to_dt(ts_ms)
                    except Exception:
                        # –µ—Å–ª–∏ timestamp –∫—Ä–∏–≤–æ–π ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏ –∂–¥—ë–º —Å–ª–µ–¥—É—é—â–µ–µ
                        anchor_dt = None

            if to_ack:
                try:
                    await redis.xack(CANDLE_STREAM, BACK_GROUP, *to_ack)
                except Exception as e:
                    log.warning(f"MW_STATE_BACK: ack error: {e}")

            if anchor_dt is not None:
                log.info(f"MW_STATE_BACK: –ø–æ–ª—É—á–µ–Ω anchor –∏–∑ bb:pg_candle_inserted ‚Üí {anchor_dt.isoformat()}")
                return anchor_dt

        except Exception as e:
            log.error(f"MW_STATE_BACK: read error: {e}", exc_info=True)
            await asyncio.sleep(1)


# üî∏ –û—Å–Ω–æ–≤–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞ –∏–∑ indicator_marketwatch_values
async def fetch_backfill_candidates(pg, start_dt: datetime, end_dt: datetime):
    """
    –ò—â–µ—Ç –±–∞—Ä—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –≤—Å–µ 4 MW-—Å–æ—Å—Ç–æ—è–Ω–∏—è (trend/volatility/momentum/extremes),
    –Ω–æ –µ—â—ë –Ω–µ—Ç kind='market_state'.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –≤–∏–¥–∞:
      {
        "symbol": str,
        "timeframe": str,
        "open_time": datetime,
        "trend_state": str,
        "vol_state": str,
        "mom_state": str,
        "ext_state": str,
      }
    """
    log.debug(
        f"MW_STATE_BACK: –≤—ã–±–æ—Ä–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ–∫–Ω–∞ "
        f"{start_dt.isoformat()} .. {end_dt.isoformat()}"
    )

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT
              symbol,
              timeframe,
              open_time,
              max(CASE WHEN kind = 'trend'      THEN state END) AS trend_state,
              max(CASE WHEN kind = 'volatility' THEN state END) AS vol_state,
              max(CASE WHEN kind = 'momentum'   THEN state END) AS mom_state,
              max(CASE WHEN kind = 'extremes'   THEN state END) AS ext_state,
              bool_or(kind = 'market_state')    AS has_market_state,
              count(DISTINCT kind)              AS kind_count
            FROM indicator_marketwatch_values
            WHERE
              open_time BETWEEN $1 AND $2
              AND timeframe = ANY($3::text[])
              AND kind IN ('trend','volatility','momentum','extremes','market_state')
            GROUP BY symbol, timeframe, open_time
            HAVING
              bool_or(kind = 'market_state') = false
              AND count(DISTINCT kind) >= 4
            ORDER BY open_time ASC
            LIMIT $4
            """,
            start_dt,
            end_dt,
            list(VALID_TF),
            BACKFILL_BATCH_LIMIT,
        )

    candidates = []
    for r in rows:
        # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç NULL-—Å–æ—Å—Ç–æ—è–Ω–∏–π
        if not (r["trend_state"] and r["vol_state"] and r["mom_state"] and r["ext_state"]):
            continue
        candidates.append(
            {
                "symbol": r["symbol"],
                "timeframe": r["timeframe"],
                "open_time": r["open_time"],
                "trend_state": r["trend_state"],
                "vol_state": r["vol_state"],
                "mom_state": r["mom_state"],
                "ext_state": r["ext_state"],
            }
        )

    return candidates


# üî∏ –ó–∞–ø–∏—Å—å –±—ç–∫–∞–ø–Ω–æ–≥–æ market_state –≤ indicator_marketwatch_values
async def write_backfilled_states(pg, records: list[dict]):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö market_state-–∑–∞–ø–∏—Å–µ–π –∏
    –ø–∏—à–µ—Ç –∏—Ö –≤ indicator_marketwatch_values —Å kind='market_state'.

    details —Å–æ–¥–µ—Ä–∂–∏—Ç direction, quality, score, components, open_time_iso.
    status —Å—Ç–∞–≤–∏–º 'healed', source = 'backfill'.
    """
    if not records:
        return 0

    async with pg.acquire() as conn:
        async with conn.transaction():
            params = []
            for rec in records:
                symbol = rec["symbol"]
                tf = rec["timeframe"]
                open_time = rec["open_time"]
                direction = rec["direction"]
                details = {
                    "direction": rec["direction"],
                    "quality": rec["quality"],
                    "score": rec["score"],
                    "components": rec["components"],
                    "open_time_iso": rec["open_time_iso"],
                }
                params.append((symbol, tf, open_time, direction, json.dumps(details)))

            await conn.executemany(
                """
                INSERT INTO indicator_marketwatch_values
                  (symbol, timeframe, open_time, kind, state, status, details, version, source, computed_at, updated_at)
                VALUES ($1,$2,$3,'market_state',$4,'healed',$5,1,'backfill',NOW(),NOW())
                ON CONFLICT (symbol, timeframe, open_time, kind)
                DO UPDATE SET
                  state   = EXCLUDED.state,
                  status  = EXCLUDED.status,
                  details = EXCLUDED.details,
                  version = EXCLUDED.version,
                  source  = EXCLUDED.source,
                  updated_at = NOW()
                """,
                params,
            )

    return len(records)


# üî∏ –û–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –±—ç–∫–∞–ø–∞ –ø–æ –æ–∫–Ω—É [start_dt .. end_dt]
async def run_backfill_window(pg, start_dt: datetime, end_dt: datetime):
    # –≤—ã–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates = await fetch_backfill_candidates(pg, start_dt, end_dt)
    if not candidates:
        log.info(
            f"MW_STATE_BACK: –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –æ–∫–Ω–µ "
            f"{start_dt.isoformat()} .. {end_dt.isoformat()}"
        )
        return 0, {}

    records_to_write = []
    per_tf = {tf: 0 for tf in VALID_TF}

    for c in candidates:
        symbol = c["symbol"]
        tf = c["timeframe"]
        ot = c["open_time"]
        trend_state = c["trend_state"]
        vol_state = c["vol_state"]
        mom_state = c["mom_state"]
        ext_state = c["ext_state"]

        direction, quality, score, components = compute_direction_and_quality(
            trend_state, vol_state, mom_state, ext_state
        )

        rec = {
            "symbol": symbol,
            "timeframe": tf,
            "open_time": ot,
            "direction": direction,
            "quality": quality,
            "score": round(float(score), 4),
            "components": components,
            "open_time_iso": ot.isoformat(),
        }
        records_to_write.append(rec)
        per_tf[tf] = per_tf.get(tf, 0) + 1

    written = await write_backfilled_states(pg, records_to_write)

    log.info(
        f"MW_STATE_BACK: –≤ –æ–∫–Ω–µ {start_dt.isoformat()} .. {end_dt.isoformat()} "
        f"–Ω–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤={len(candidates)}, –∑–∞–ø–∏—Å–∞–Ω–æ={written} "
        f"(m5={per_tf.get('m5',0)}, m15={per_tf.get('m15',0)}, h1={per_tf.get('h1',0)})"
    )

    return written, per_tf


# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä –±—ç–∫–∞–ø–∞: –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å—É—Ç–æ–∫
async def run_indicator_mw_states_back(pg, redis):
    log.debug("MW_STATE_BACK: –≤–æ—Ä–∫–µ—Ä –∑–∞–ø—É—â–µ–Ω (–æ–∂–∏–¥–∞–Ω–∏–µ —Å—Ç–∞—Ä—Ç–∞)")

    # –∂–¥—ë–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –ª–∞–≥, —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å –∂–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    await asyncio.sleep(BACKFILL_START_DELAY_SEC)

    # –∂–¥—ë–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç feed_bb, —á—Ç–æ–±—ã –≤–∑—è—Ç—å –æ–ø–æ—Ä–Ω–æ–µ –≤—Ä–µ–º—è
    anchor_dt = await wait_first_candle(redis)

    # —Å–¥–≤–∏–≥–∞–µ–º –Ω–∞–∑–∞–¥ –Ω–∞ BACKFILL_SKIP_RECENT_HOURS
    effective_now = anchor_dt - timedelta(hours=BACKFILL_SKIP_RECENT_HOURS)
    start_dt = effective_now - timedelta(days=BACKFILL_LOOKBACK_DAYS)
    end_dt = effective_now

    log.info(
        f"MW_STATE_BACK: —Å—Ç–∞—Ä—Ç –±—ç–∫–∞–ø–∞ market_state –¥–ª—è –æ–∫–Ω–∞ "
        f"{start_dt.isoformat()} .. {end_dt.isoformat()} "
        f"(anchor={anchor_dt.isoformat()}, skip_recent_hours={BACKFILL_SKIP_RECENT_HOURS})"
    )

    # –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ø–æ –æ–∫–Ω—É; –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –¥–Ω—è–º
    total_written, per_tf = await run_backfill_window(pg, start_dt, end_dt)

    log.info(
        f"MW_STATE_BACK: –±—ç–∫–∞–ø –∑–∞–≤–µ—Ä—à—ë–Ω, –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–∞–Ω–æ={total_written} "
        f"(m5={per_tf.get('m5',0)}, m15={per_tf.get('m15',0)}, h1={per_tf.get('h1',0)})"
    )

    # –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≤–æ—Ä–∫–µ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è; –µ—Å–ª–∏ –µ–≥–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –±–µ–∑ run_safe_loop,
    # –æ–Ω –æ—Ç—Ä–∞–±–æ—Ç–∞–µ—Ç –æ–¥–∏–Ω —Ä–∞–∑ –∏ –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ç—å—Å—è.