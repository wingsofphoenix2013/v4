# indicator_pack.py ‚Äî –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Ä–∞—Å—á—ë—Ç–∞ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π (ind_pack)

# üî∏ –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime
from typing import Any

# üî∏ –ò–º–ø–æ—Ä—Ç pack-–≤–æ—Ä–∫–µ—Ä–æ–≤ (–ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
from packs.rsi_bin import RsiBinPack
from packs.mfi_bin import MfiBinPack

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã Redis
INDICATOR_STREAM = "indicator_stream"          # –≤—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–∏–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
IND_PACK_PREFIX = "ind_pack"                   # –ø—Ä–µ—Ñ–∏–∫—Å –∫–ª—é—á–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–∫–∞–∫ –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å)
IND_PACK_GROUP = "ind_pack_group_v4"           # consumer-group –¥–ª—è indicator_stream
IND_PACK_CONSUMER = "ind_pack_consumer_1"      # consumer name

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ë–î
PACK_INSTANCES_TABLE = "indicator_pack_instances_v4"
ANALYSIS_INSTANCES_TABLE = "bt_analysis_instances"
ANALYSIS_PARAMETERS_TABLE = "bt_analysis_parameters"
BINS_DICT_TABLE = "bt_analysis_bins_dict"
BB_TICKERS_TABLE = "tickers_bb"

# üî∏ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á—Ç–µ–Ω–∏—è –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ stream
STREAM_READ_COUNT = 500          # —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π —á–∏—Ç–∞—Ç—å –∑–∞ —Ä–∞–∑
STREAM_BLOCK_MS = 2000           # –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ XREADGROUP (–º—Å)
MAX_PARALLEL_MESSAGES = 200      # —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ

# üî∏ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ (bootstrap)
BOOTSTRAP_MAX_PARALLEL = 300     # —Å–∫–æ–ª—å–∫–æ —Ç–∏–∫–µ—Ä–æ–≤/–ø–∞–∫–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ

# üî∏ TTL –ø–æ TF (–∫–∞–∫ –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å)
TTL_BY_TF_SEC = {
    "m5": 120,      # 2 –º–∏–Ω—É—Ç—ã (—Ç—ã –ø–æ—Å—Ç–∞–≤–∏–ª)
    "m15": 960,     # 16 –º–∏–Ω—É—Ç
    "h1": 3660,     # 61 –º–∏–Ω—É—Ç–∞
}

# üî∏ –†–µ–µ—Å—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö pack-–≤–æ—Ä–∫–µ—Ä–æ–≤ (key –±–µ—Ä—ë–º –∏–∑ bt_analysis_instances.key)
PACK_WORKERS = {
    "rsi_bin": RsiBinPack,
    "mfi_bin": MfiBinPack,
}

# üî∏ –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä pack-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Ä–∞–±–æ—Ç–µ
pack_registry: dict[tuple[str, str], list["PackRuntime"]] = {}
# key: (timeframe, indicator_from_stream) -> list[PackRuntime]


@dataclass(frozen=True)
class BinRule:
    direction: str
    timeframe: str
    bin_type: str
    bin_order: int
    bin_name: str
    val_from: float | None
    val_to: float | None
    to_inclusive: bool


@dataclass
class PackRuntime:
    analysis_id: int
    analysis_key: str
    analysis_name: str
    timeframe: str
    source_param_name: str
    bins_policy: dict[str, Any] | None
    bins_by_direction: dict[str, list[BinRule]]
    ttl_sec: int
    worker: Any


# üî∏ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –±–∏–Ω–æ–≤ –∏–∑ bins_policy (–ø–æ–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ static)
def get_bins_source(bins_policy: dict[str, Any] | None, timeframe: str) -> str:
    # –¥–µ—Ñ–æ–ª—Ç ‚Äî static
    if not bins_policy:
        return "static"

    try:
        # —Ñ–æ—Ä–º–∞ 1) {"default":"static","by_tf":{"m5":"adaptive",...}}
        if "by_tf" in bins_policy:
            by_tf = bins_policy.get("by_tf") or {}
            return str(by_tf.get(timeframe) or bins_policy.get("default") or "static")

        # —Ñ–æ—Ä–º–∞ 2) {"m5":"adaptive","m15":"static","h1":"static"} –∏–ª–∏ {"default":"static"}
        return str(bins_policy.get(timeframe) or bins_policy.get("default") or "static")
    except Exception:
        return "static"


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö pack-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤
async def load_enabled_packs(pg) -> list[dict[str, Any]]:
    log = logging.getLogger("PACK_INIT")

    async with pg.acquire() as conn:
        rows = await conn.fetch(f"""
            SELECT id, analysis_id, enabled, bins_policy, enabled_at
            FROM {PACK_INSTANCES_TABLE}
            WHERE enabled = true
        """)

    packs: list[dict[str, Any]] = []
    for r in rows:
        packs.append({
            "id": int(r["id"]),
            "analysis_id": int(r["analysis_id"]),
            "bins_policy": r["bins_policy"],  # jsonb -> dict (asyncpg)
            "enabled_at": r["enabled_at"],
        })

    log.debug(f"PACK_INIT: –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö pack-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(packs)}")
    return packs


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
async def load_analysis_instances(pg, analysis_ids: list[int]) -> dict[int, dict[str, Any]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(f"""
            SELECT id, family_key, "key", "name", enabled
            FROM {ANALYSIS_INSTANCES_TABLE}
            WHERE id = ANY($1::int[])
        """, analysis_ids)

    out: dict[int, dict[str, Any]] = {}
    for r in rows:
        out[int(r["id"])] = {
            "family_key": str(r["family_key"]),
            "key": str(r["key"]),
            "name": str(r["name"]),
            "enabled": bool(r["enabled"]),
        }

    log.debug(f"PACK_INIT: analysis-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(out)}")
    return out


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (–Ω—É–∂–Ω—ã tf –∏ param_name)
async def load_analysis_parameters(pg, analysis_ids: list[int]) -> dict[int, dict[str, str]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(f"""
            SELECT analysis_id, param_name, param_value
            FROM {ANALYSIS_PARAMETERS_TABLE}
            WHERE analysis_id = ANY($1::int[])
        """, analysis_ids)

    params: dict[int, dict[str, str]] = {}
    for r in rows:
        aid = int(r["analysis_id"])
        pname = str(r["param_name"])
        pval = str(r["param_value"])
        params.setdefault(aid, {})[pname] = pval

    # —Å—É–º–º–∞—Ä–Ω—ã–π –ª–æ–≥ –ø–æ –ø–æ–ª–Ω–æ—Ç–µ (tf/param_name)
    ok = 0
    missing = 0
    for aid in analysis_ids:
        p = params.get(aid, {})
        if "tf" in p and "param_name" in p:
            ok += 1
        else:
            missing += 1

    log.debug(f"PACK_INIT: –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (tf+param_name) OK={ok}, missing={missing}")
    return params


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –±–∏–Ω–æ–≤ (bt_analysis_bins_dict)
async def load_bins_dict(pg, analysis_ids: list[int]) -> dict[int, dict[str, dict[str, list[BinRule]]]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(f"""
            SELECT analysis_id, direction, timeframe, bin_type, bin_order, bin_name,
                   val_from, val_to, to_inclusive
            FROM {BINS_DICT_TABLE}
            WHERE analysis_id = ANY($1::int[])
              AND bin_type = 'bins'
        """, analysis_ids)

    # —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: analysis_id -> timeframe -> direction -> [BinRule...]
    out: dict[int, dict[str, dict[str, list[BinRule]]]] = {}
    total_rules = 0

    for r in rows:
        aid = int(r["analysis_id"])
        direction = str(r["direction"])
        timeframe = str(r["timeframe"])
        bin_type = str(r["bin_type"])

        rule = BinRule(
            direction=direction,
            timeframe=timeframe,
            bin_type=bin_type,
            bin_order=int(r["bin_order"]),
            bin_name=str(r["bin_name"]),
            val_from=float(r["val_from"]) if r["val_from"] is not None else None,
            val_to=float(r["val_to"]) if r["val_to"] is not None else None,
            to_inclusive=bool(r["to_inclusive"]),
        )

        out.setdefault(aid, {}).setdefault(timeframe, {}).setdefault(direction, []).append(rule)
        total_rules += 1

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ bin_order –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞
    for aid in out:
        for tf in out[aid]:
            for direction in out[aid][tf]:
                out[aid][tf][direction].sort(key=lambda x: x.bin_order)

    log.debug(f"PACK_INIT: –ø—Ä–∞–≤–∏–ª –±–∏–Ω–æ–≤ (static) –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {total_rules}")
    return out


# üî∏ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–µ–µ—Å—Ç—Ä–∞ pack-–≤–æ—Ä–∫–µ—Ä–æ–≤ (match –ø–æ indicator_stream.indicator + timeframe)
def build_pack_registry(
    packs: list[dict[str, Any]],
    analysis_meta: dict[int, dict[str, Any]],
    analysis_params: dict[int, dict[str, str]],
    bins_dict: dict[int, dict[str, dict[str, list[BinRule]]]],
) -> dict[tuple[str, str], list[PackRuntime]]:
    log = logging.getLogger("PACK_INIT")

    registry: dict[tuple[str, str], list[PackRuntime]] = {}

    active = 0
    skipped = 0
    no_bins = 0
    not_supported = 0

    for pack in packs:
        analysis_id = int(pack["analysis_id"])
        meta = analysis_meta.get(analysis_id)
        params = analysis_params.get(analysis_id, {})

        if not meta:
            skipped += 1
            log.warning(f"PACK_INIT: analysis_id={analysis_id} –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ bt_analysis_instances")
            continue

        if not bool(meta.get("enabled", True)):
            skipped += 1
            log.warning(f"PACK_INIT: analysis_id={analysis_id} –ø—Ä–æ–ø—É—â–µ–Ω: bt_analysis_instances.enabled=false")
            continue

        analysis_key = str(meta["key"])
        analysis_name = str(meta["name"])

        timeframe = params.get("tf")
        source_param_name = params.get("param_name")

        if not timeframe or not source_param_name:
            skipped += 1
            log.warning(
                f"PACK_INIT: analysis_id={analysis_id} ({analysis_key}) –ø—Ä–æ–ø—É—â–µ–Ω: "
                f"–Ω–µ—Ç tf/param_name –≤ bt_analysis_parameters"
            )
            continue

        ttl_sec = int(TTL_BY_TF_SEC.get(timeframe, 60))

        bins_policy = pack.get("bins_policy")
        bins_source = get_bins_source(bins_policy, timeframe)

        # –ø–æ–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ static
        if bins_source != "static":
            not_supported += 1
            log.warning(
                f"PACK_INIT: analysis_id={analysis_id} ({analysis_key}) –ø—Ä–æ–ø—É—â–µ–Ω: "
                f"bins_source={bins_source} –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω (—Ç–æ–ª—å–∫–æ static)"
            )
            continue

        worker_cls = PACK_WORKERS.get(analysis_key)
        if worker_cls is None:
            skipped += 1
            log.warning(f"PACK_INIT: analysis_id={analysis_id} –ø—Ä–æ–ø—É—â–µ–Ω: –≤–æ—Ä–∫–µ—Ä –¥–ª—è key='{analysis_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            continue

        # —Å–æ–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –±–∏–Ω–æ–≤
        bins_tf = bins_dict.get(analysis_id, {}).get(timeframe, {})
        if not bins_tf:
            no_bins += 1
            # –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º ‚Äî –ø–∞–∫ –º–æ–∂–µ—Ç –∂–∏—Ç—å, –ø—Ä–æ—Å—Ç–æ –Ω–µ –±—É–¥–µ—Ç –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª
            log.warning(
                f"PACK_INIT: analysis_id={analysis_id} ({analysis_key}) ‚Äî –Ω–µ—Ç bin-–ø—Ä–∞–≤–∏–ª "
                f"–≤ bt_analysis_bins_dict –¥–ª—è tf={timeframe}"
            )

        bins_by_direction = {
            "long": bins_tf.get("long", []),
            "short": bins_tf.get("short", []),
        }

        runtime = PackRuntime(
            analysis_id=analysis_id,
            analysis_key=analysis_key,
            analysis_name=analysis_name,
            timeframe=timeframe,
            source_param_name=source_param_name,
            bins_policy=bins_policy,
            bins_by_direction=bins_by_direction,
            ttl_sec=ttl_sec,
            worker=worker_cls(),
        )

        # –º–∞—Ç—á–∏–º—Å—è –ø–æ indicator_stream.indicator == source_param_name
        registry.setdefault((timeframe, source_param_name), []).append(runtime)
        active += 1

    log.debug(
        f"PACK_INIT: registry –ø–æ—Å—Ç—Ä–æ–µ–Ω ‚Äî active={active}, skipped={skipped}, "
        f"no_bins={no_bins}, not_supported={not_supported}, routes={len(registry)}"
    )
    return registry


# üî∏ –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ pack –≤ Redis (–∫–ª—é—á ind_pack:analysis_id:direction:symbol:tf)
async def publish_pack_state(redis, analysis_id: int, direction: str, symbol: str, timeframe: str, bin_name: str, ttl_sec: int):
    key = f"{IND_PACK_PREFIX}:{analysis_id}:{direction}:{symbol}:{timeframe}"
    await redis.set(key, bin_name, ex=ttl_sec)


# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–±—ã—Ç–∏—è indicator_stream (status=ready)
async def handle_indicator_ready(redis, msg: dict[str, str]) -> None:
    log = logging.getLogger("PACK_SET")

    symbol = msg.get("symbol")
    timeframe = msg.get("timeframe")
    indicator_key = msg.get("indicator")
    status = msg.get("status")
    open_time = msg.get("open_time")

    # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏
    if status != "ready" or not symbol or not timeframe or not indicator_key:
        return

    runtimes = pack_registry.get((timeframe, indicator_key))
    if not runtimes:
        return

    for rt in runtimes:
        # –ø–æ–ª—É—á–∏—Ç—å raw –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ Redis KV –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        raw_key = f"ind:{symbol}:{rt.timeframe}:{rt.source_param_name}"
        raw_value = await redis.get(raw_key)

        # –µ—Å–ª–∏ –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if raw_value is None:
            continue

        try:
            value = float(raw_value)
        except Exception:
            continue

        # —Å—á–∏—Ç–∞–µ–º –±–∏–Ω—ã (long/short) –∏ –ø—É–±–ª–∏–∫—É–µ–º –¥–≤–∞ –∫–ª—é—á–∞
        publish_tasks = []
        published_items: list[tuple[str, str]] = []

        for direction in ("long", "short"):
            rules = rt.bins_by_direction.get(direction) or []
            # –µ—Å–ª–∏ –ø—Ä–∞–≤–∏–ª –Ω–µ—Ç ‚Äî –Ω–µ—á–µ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å
            if not rules:
                continue

            bin_name = rt.worker.bin_value(value=value, rules=rules)
            if not bin_name:
                continue

            publish_tasks.append(
                publish_pack_state(
                    redis=redis,
                    analysis_id=rt.analysis_id,
                    direction=direction,
                    symbol=symbol,
                    timeframe=rt.timeframe,
                    bin_name=bin_name,
                    ttl_sec=rt.ttl_sec,
                )
            )
            published_items.append((direction, bin_name))

        if publish_tasks:
            await asyncio.gather(*publish_tasks, return_exceptions=True)

            # –∏—Ç–æ–≥–æ–≤—ã–π –ª–æ–≥: —á—Ç–æ –ø—Ä–∏—Å–≤–æ–∏–ª–∏ "—Å–µ–π—á–∞—Å"
            for direction, bin_name in published_items:
                log.debug(
                    f"analysis_id={rt.analysis_id} symbol={symbol} tf={rt.timeframe} "
                    f"direction={direction} bin_name={bin_name} open_time={open_time} ttl={rt.ttl_sec}"
                )


# üî∏ –°–æ–∑–¥–∞–Ω–∏–µ consumer-group (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–æ–±—ã—Ç–∏—è –≤–æ –≤—Ä–µ–º—è bootstrap)
async def ensure_indicator_stream_group(redis):
    log = logging.getLogger("PACK_STREAM")
    try:
        await redis.xgroup_create(INDICATOR_STREAM, IND_PACK_GROUP, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create error: {e}")


# üî∏ –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ indicator_stream –∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è pack-–æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
async def watch_indicator_stream(redis):
    log = logging.getLogger("PACK_STREAM")

    await ensure_indicator_stream_group(redis)

    sem = asyncio.Semaphore(MAX_PARALLEL_MESSAGES)

    async def _process_one(data: dict) -> None:
        # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        async with sem:
            msg = {
                "symbol": data.get("symbol"),
                "timeframe": data.get("timeframe"),
                "indicator": data.get("indicator"),
                "open_time": data.get("open_time"),
                "status": data.get("status"),
            }
            await handle_indicator_ready(redis, msg)

    while True:
        try:
            resp = await redis.xreadgroup(
                IND_PACK_GROUP,
                IND_PACK_CONSUMER,
                streams={INDICATOR_STREAM: ">"},
                count=STREAM_READ_COUNT,
                block=STREAM_BLOCK_MS,
            )

            if not resp:
                continue

            flat: list[tuple[str, dict]] = []
            for _, messages in resp:
                for msg_id, data in messages:
                    flat.append((msg_id, data))

            if not flat:
                continue

            to_ack = [msg_id for msg_id, _ in flat]

            # –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—á–∫–∏
            tasks = [asyncio.create_task(_process_one(data)) for _, data in flat]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # –ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ –±—ã–ª–∏)
            for r in results:
                if isinstance(r, Exception):
                    log.warning(f"PACK_STREAM: message processing error: {r}", exc_info=True)

            # ack –ø–∞—á–∫–æ–π
            await redis.xack(INDICATOR_STREAM, IND_PACK_GROUP, *to_ack)

        except Exception as e:
            log.error(f"PACK_STREAM loop error: {e}", exc_info=True)
            await asyncio.sleep(2)


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ (–¥–ª—è bootstrap)
async def load_active_symbols(pg) -> list[str]:
    log = logging.getLogger("PACK_BOOT")

    async with pg.acquire() as conn:
        rows = await conn.fetch(f"""
            SELECT symbol
            FROM {BB_TICKERS_TABLE}
            WHERE status = 'enabled' AND tradepermission = 'enabled'
        """)

    symbols = [str(r["symbol"]) for r in rows if r and r.get("symbol")]
    log.debug(f"PACK_BOOT: –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(symbols)}")
    return symbols


# üî∏ –•–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç: –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ Redis KV (–±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è next ready)
async def bootstrap_current_state(pg, redis):
    log = logging.getLogger("PACK_BOOT")

    # —Å–æ–±—Ä–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞–∫–æ–≤ (runtime)
    runtimes: list[PackRuntime] = []
    for lst in pack_registry.values():
        runtimes.extend(lst)

    if not runtimes:
        log.debug("PACK_BOOT: –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö pack-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤, bootstrap –ø—Ä–æ–ø—É—â–µ–Ω")
        return

    symbols = await load_active_symbols(pg)
    if not symbols:
        log.debug("PACK_BOOT: –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤, bootstrap –ø—Ä–æ–ø—É—â–µ–Ω")
        return

    sem = asyncio.Semaphore(BOOTSTRAP_MAX_PARALLEL)

    published = 0
    misses = 0
    errors = 0

    async def _process_one(symbol: str, rt: PackRuntime):
        nonlocal published, misses, errors

        async with sem:
            # –ø–æ–ª—É—á–∏—Ç—å raw –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ Redis KV –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            raw_key = f"ind:{symbol}:{rt.timeframe}:{rt.source_param_name}"
            raw_value = await redis.get(raw_key)

            # –µ—Å–ª–∏ –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if raw_value is None:
                misses += 1
                return

            try:
                value = float(raw_value)
            except Exception:
                misses += 1
                return

            # —Å—á–∏—Ç–∞–µ–º –±–∏–Ω—ã (long/short) –∏ –ø—É–±–ª–∏–∫—É–µ–º –¥–≤–∞ –∫–ª—é—á–∞
            publish_tasks = []
            published_items: list[tuple[str, str]] = []

            for direction in ("long", "short"):
                rules = rt.bins_by_direction.get(direction) or []
                # –µ—Å–ª–∏ –ø—Ä–∞–≤–∏–ª –Ω–µ—Ç ‚Äî –Ω–µ—á–µ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å
                if not rules:
                    continue

                bin_name = rt.worker.bin_value(value=value, rules=rules)
                if not bin_name:
                    continue

                publish_tasks.append(
                    publish_pack_state(
                        redis=redis,
                        analysis_id=rt.analysis_id,
                        direction=direction,
                        symbol=symbol,
                        timeframe=rt.timeframe,
                        bin_name=bin_name,
                        ttl_sec=rt.ttl_sec,
                    )
                )
                published_items.append((direction, bin_name))

            if publish_tasks:
                res = await asyncio.gather(*publish_tasks, return_exceptions=True)
                # –µ—Å–ª–∏ –±—ã–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ publish ‚Äî —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ errors
                for r in res:
                    if isinstance(r, Exception):
                        errors += 1

                # –ª–æ–≥: —á—Ç–æ –≤—ã—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ
                for direction, bin_name in published_items:
                    log.debug(
                        f"analysis_id={rt.analysis_id} symbol={symbol} tf={rt.timeframe} "
                        f"direction={direction} bin_name={bin_name} open_time=startup ttl={rt.ttl_sec}"
                    )

                published += len(published_items)

    # –∑–∞–ø—É—Å–∫–∞–µ–º bootstrap –ø–∞—á–∫–æ–π
    tasks = []
    for rt in runtimes:
        for symbol in symbols:
            tasks.append(asyncio.create_task(_process_one(symbol, rt)))

    await asyncio.gather(*tasks, return_exceptions=True)

    log.debug(
        f"PACK_BOOT: bootstrap –∑–∞–≤–µ—Ä—à—ë–Ω ‚ÄîCELY ‚Äî packs={len(runtimes)}, symbols={len(symbols)}, "
        f"published={published}, misses={misses}, errors={errors}"
    )


# üî∏ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞ –∏ —Ä–µ–µ—Å—Ç—Ä–∞ pack-–≤–æ—Ä–∫–µ—Ä–æ–≤
async def init_pack_runtime(pg):
    global pack_registry
    log = logging.getLogger("PACK_INIT")

    packs = await load_enabled_packs(pg)
    analysis_ids = sorted({int(p["analysis_id"]) for p in packs})

    analysis_meta = await load_analysis_instances(pg, analysis_ids)
    analysis_params = await load_analysis_parameters(pg, analysis_ids)
    bins_dict = await load_bins_dict(pg, analysis_ids)

    pack_registry = build_pack_registry(
        packs=packs,
        analysis_meta=analysis_meta,
        analysis_params=analysis_params,
        bins_dict=bins_dict,
    )

    # –∏—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞
    total_routes = sum(len(v) for v in pack_registry.values())
    log.debug(f"PACK_INIT: pack_registry –≥–æ—Ç–æ–≤ ‚Äî routes_total={total_routes}, match_keys={len(pack_registry)}")


# üî∏ –í–Ω–µ—à–Ω—è—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ (–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ indicators_v4_main.py –∏ run_safe_loop)
async def run_indicator_pack(pg, redis):
    await init_pack_runtime(pg)

    # —Å–æ–∑–¥–∞—ë–º group —Å—Ä–∞–∑—É, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –≤–æ –≤—Ä–µ–º—è bootstrap
    await ensure_indicator_stream_group(redis)

    # —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç: –≤—ã—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è next ready
    await bootstrap_current_state(pg, redis)

    # –¥–∞–ª—å—à–µ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
    await watch_indicator_stream(redis)