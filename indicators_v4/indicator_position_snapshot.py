# indicator_position_snapshot.py â€” Ð²Ð¾Ñ€ÐºÐµÑ€ ÑÐ½Ð¸Ð¼ÐºÐ° Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ð½Ð° Ð¼Ð¾Ð¼ÐµÐ½Ñ‚ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ (ÑÑ‚Ð°Ð¿ 2: m5+m15+h1, param_type=indicator)

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional

# ðŸ”¸ ÐšÐ¾Ð½ÑÑ‚Ð°Ð½Ñ‚Ñ‹ ÑÑ‚Ñ€Ð¸Ð¼Ð¾Ð² Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
POSITIONS_OPEN_STREAM = "positions_open_stream"      # Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¸Ð¼ Ð²Ð½ÐµÑˆÐ½ÐµÐ³Ð¾ Ð¼Ð¾Ð´ÑƒÐ»Ñ (Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸)

IND_REQ_STREAM   = "indicator_request"               # on-demand Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð²
IND_RESP_STREAM  = "indicator_response"              # on-demand Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² (Ð¾Ð±Ñ‰Ð¸Ð¹ ÑÑ‚Ñ€Ð¸Ð¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹)
RESP_GROUP       = "ips_resp_group"                  # Ð½Ð°ÑˆÐ° consumer-group Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ñ… (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ð¾Ñ‚ Ð²Ð¾Ñ€ÐºÐµÑ€)
RESP_CONSUMER    = "ips_resp_consumer_1"

IPS_GROUP        = "ips_group"                       # consumer-group Ð´Ð»Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹
IPS_CONSUMER     = "ips_consumer_1"

# ðŸ”¸ Ð¢Ð°Ð¹Ð¼Ð¸Ð½Ð³Ð¸/Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ
READ_BLOCK_MS             = 1500                     # Ð±Ð»Ð¾ÐºÐ¸Ñ€ÑƒÑŽÑ‰ÐµÐµ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ Ð¸Ð· ÑÑ‚Ñ€Ð¸Ð¼Ð¾Ð² (Ð¼Ñ)
REQ_RESPONSE_TIMEOUT_MS   = 5000                     # Ð¾Ð±Ñ‰Ð¸Ð¹ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° (Ð¼Ñ) Ð½Ð° Ð¾Ð´Ð¸Ð½ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð°
SECOND_TRY_TIMEOUT_MS     = 3000                     # Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ (Ð¼Ñ) Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½Ð¾Ð¼ timeout
PARALLEL_REQUESTS_LIMIT   = 24                       # Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ on-demand
BATCH_INSERT_MAX          = 500                      # Ð¼Ð°ÐºÑ. Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¿Ð°Ñ‡ÐºÐ¸ Ð´Ð»Ñ INSERT

# ðŸ”¸ Ð¢Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ñ‹ Ð¸ Ñ‚Ð°Ð¹Ð¼ÑˆÐ°Ð³Ð¸ (ÑÑ‚Ð°Ð¿ 2 â€” m5, m15, h1)
TF_ORDER = ["m5", "m15", "h1"]
STEP_MIN = {"m5": 5, "m15": 15, "h1": 60}
STEP_MS  = {k: v * 60_000 for k, v in STEP_MIN.items()}

# ðŸ”¸ ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ñ€Ð¾ÑƒÑ‚ÐµÑ€Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²
PENDING_TTL_SEC           = 15                       # TTL Ð´Ð»Ñ Â«Ð¾Ð¶Ð¸Ð´Ð°ÑŽÑ‰Ð¸Ñ…Â» ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ (Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸)
XAUTOCLAIM_MIN_IDLE_MS    = 2000                     # ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ€ÑˆÐµ ÑÑ‚Ð¾Ð³Ð¾ idle ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Â«Ð¿Ñ€Ð¾Ñ‚ÑƒÑ…ÑˆÐ¸Ð¼Ð¸Â» Ð¸ Ð·Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼
XAUTOCLAIM_BATCH          = 200                      # Ð·Ð° Ñ€Ð°Ð· Ð¿Ð¾Ð´Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ð½Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ N ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
PENDING_MAX_IN_MEMORY     = 20000                    # Ð¼ÑÐ³ÐºÐ¸Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð½Ð° Ð±ÑƒÑ„ÐµÑ€ pending

# ðŸ”¸ Ð›Ð¾Ð³Ð³ÐµÑ€
log = logging.getLogger("IND_POSSTAT")


# ðŸ”¸ Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸/Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
def floor_to_bar(ts_ms: int, tf: str) -> int:
    step = STEP_MS[tf]
    return (ts_ms // step) * step

def parse_iso_to_ms(iso_str: str) -> Optional[int]:
    try:
        dt = datetime.fromisoformat(iso_str)  # UTC-naive ISO Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹
        return int(dt.timestamp() * 1000)
    except Exception:
        return None

def to_float_safe(s: str) -> Optional[float]:
    try:
        return float(s)
    except Exception:
        return None

# ðŸ”¸ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð³Ð¾ Ð¸Ð¼ÐµÐ½Ð¸ Ð´Ð»Ñ param_type='indicator'
def indicator_base_from_param_name(param_name: str) -> str:
    """
    Ð’ param_base Ð´Ð»Ñ indicators Ð¿Ð¸ÑˆÐµÐ¼ ÑƒÐºÐ¾Ñ€Ð¾Ñ‡ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð±ÐµÐ· Ð´Ð»Ð¸Ð½Ñ‹:
      ema21 -> ema, rsi14 -> rsi, atr14 -> atr, kama30 -> kama,
      bb20_2_0_upper -> bb, macd12_macd -> macd, adx_dmi21_plus_di -> adx_dmi, lr50_angle -> lr.
    """
    if param_name.startswith("adx_dmi"):
        return "adx_dmi"
    for prefix in ("ema", "rsi", "mfi", "atr", "kama", "macd", "bb", "lr"):
        if param_name.startswith(prefix):
            return prefix
    return param_name.split("_", 1)[0]

def indicator_base_from_instance(inst: Dict[str, Any]) -> str:
    ind = str(inst.get("indicator", "indicator"))
    return "adx_dmi" if ind.startswith("adx_dmi") else ind


# ðŸ”¸ Ð Ð¾ÑƒÑ‚ÐµÑ€ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² indicator_response (Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð¾ÑÑ‚Ð°Ð²ÐºÐ° Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ)
class IndicatorResponseRouter:
    """
    ÐžÐ´Ð¸Ð½ Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¹ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÐµÐ»ÑŒ XREADGROUP Ð¿Ð¾ IND_RESP_STREAM/RESP_GROUP.
    Ð”Ð¸ÑÐ¿ÐµÑ‚Ñ‡ÐµÑ€Ð¸Ð·ÑƒÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾ req_id Ð² asyncio.Queue.
    Â«ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹ÐµÂ» req_id Ð½Ðµ ACK-Ð°ÐµÐ¼; ÑÐºÐ»Ð°Ð´Ñ‹Ð²Ð°ÐµÐ¼ Ð² pending Ð¸ Ð¾Ñ‚Ð´Ð°Ñ‘Ð¼ Ð¿Ñ€Ð¸ register(req_id), Ð¿Ð¾ÑÐ»Ðµ Ñ‡ÐµÐ³Ð¾ ACK.
    ÐŸÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸ Ð·Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ Â«Ð¿Ñ€Ð¾Ñ‚ÑƒÑ…ÑˆÐ¸ÐµÂ» ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· XAUTOCLAIM.
    """
    def __init__(self, redis):
        self.redis = redis
        self._queues: Dict[str, asyncio.Queue] = {}                 # req_id -> Queue
        self._pending: Dict[str, Tuple[str, Dict[str, Any], float]] = {}  # req_id -> (msg_id, data, put_ts)
        self._reader_task: Optional[asyncio.Task] = None
        self._reclaimer_task: Optional[asyncio.Task] = None
        self._janitor_task: Optional[asyncio.Task] = None
        self._started = False
        self._lock = asyncio.Lock()

    async def start(self):
        if self._started:
            return
        # ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ (Ð¸Ð´ÐµÐ¼Ð¿Ð¾Ñ‚ÐµÐ½Ñ‚Ð½Ð¾)
        try:
            await self.redis.xgroup_create(IND_RESP_STREAM, RESP_GROUP, id="$", mkstream=True)
        except Exception as e:
            if "BUSYGROUP" not in str(e):
                log.warning(f"xgroup_create (resp) error: {e}")
        self._reader_task   = asyncio.create_task(self._reader_loop())
        self._reclaimer_task= asyncio.create_task(self._reclaim_loop())
        self._janitor_task  = asyncio.create_task(self._janitor_loop())
        self._started = True
        log.debug("IND_POSSTAT: response router started")

    async def _reader_loop(self):
        while True:
            try:
                resp = await self.redis.xreadgroup(
                    groupname=RESP_GROUP,
                    consumername=RESP_CONSUMER,
                    streams={IND_RESP_STREAM: ">"},
                    count=200,
                    block=READ_BLOCK_MS
                )
                if not resp:
                    continue

                ack_ids: List[str] = []
                now_mono = asyncio.get_event_loop().time()

                for _, messages in resp:
                    for msg_id, data in messages:
                        req_id = data.get("req_id")
                        if not req_id:
                            ack_ids.append(msg_id)
                            continue

                        q = self._queues.get(req_id)
                        if q is not None:
                            try:
                                q.put_nowait((msg_id, data))
                                ack_ids.append(msg_id)
                            except Exception:
                                self._pending[req_id] = (msg_id, data, now_mono)
                        else:
                            if len(self._pending) < PENDING_MAX_IN_MEMORY:
                                self._pending[req_id] = (msg_id, data, now_mono)
                            else:
                                log.warning("IND_POSSTAT: pending buffer full, buffered without ACK; size=%d", len(self._pending))
                                self._pending[req_id] = (msg_id, data, now_mono)

                if ack_ids:
                    await self.redis.xack(IND_RESP_STREAM, RESP_GROUP, *ack_ids)

            except Exception as e:
                log.error(f"IND_POSSTAT: resp router read error: {e}", exc_info=True)
                await asyncio.sleep(0.3)

    async def _reclaim_loop(self):
        last_id = "0-0"
        while True:
            try:
                claimed = await self.redis.execute_command(
                    "XAUTOCLAIM", IND_RESP_STREAM, RESP_GROUP, RESP_CONSUMER,
                    XAUTOCLAIM_MIN_IDLE_MS, last_id, "COUNT", XAUTOCLAIM_BATCH
                )
                next_id, entries = claimed[0], claimed[1]
                now_mono = asyncio.get_event_loop().time()
                for msg_id, data in entries:
                    req_id = data.get("req_id")
                    if not req_id:
                        await self.redis.xack(IND_RESP_STREAM, RESP_GROUP, msg_id)
                        continue
                    if req_id in self._queues:
                        try:
                            self._queues[req_id].put_nowait((msg_id, data))
                            await self.redis.xack(IND_RESP_STREAM, RESP_GROUP, msg_id)
                        except Exception:
                            self._pending[req_id] = (msg_id, data, now_mono)
                    else:
                        self._pending[req_id] = (msg_id, data, now_mono)

                last_id = next_id or last_id
            except Exception as e:
                log.warning(f"IND_POSSTAT: resp router reclaim error: {e}", exc_info=True)
            finally:
                await asyncio.sleep(1.0)

    async def _janitor_loop(self):
        while True:
            try:
                now_mono = asyncio.get_event_loop().time()
                stale = [rid for rid, (_, _, put_ts) in self._pending.items()
                         if now_mono - put_ts > PENDING_TTL_SEC]
                if stale:
                    log.warning("IND_POSSTAT: pending entries stale=%d (kept unacked, waiting for register)", len(stale))
            except Exception:
                pass
            finally:
                await asyncio.sleep(5.0)

    async def register(self, req_id: str) -> asyncio.Queue:
        async with self._lock:
            q = asyncio.Queue(maxsize=1)
            self._queues[req_id] = q
            pending = self._pending.pop(req_id, None)
            if pending:
                msg_id, data, _ = pending
                try:
                    q.put_nowait((msg_id, data))
                    await self.redis.xack(IND_RESP_STREAM, RESP_GROUP, msg_id)
                except Exception:
                    self._pending[req_id] = (msg_id, data, asyncio.get_event_loop().time())
            return q

    async def unregister(self, req_id: str):
        async with self._lock:
            self._queues.pop(req_id, None)


# ðŸ”¸ ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° on-demand Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ‰Ð¸Ð¹ Ñ€Ð¾ÑƒÑ‚ÐµÑ€ (Ñ 1 Ñ€ÐµÑ‚Ñ€Ð°ÐµÐ¼)
async def request_indicator_snapshot(
    redis,
    router: IndicatorResponseRouter,
    *,
    symbol: str,
    timeframe: str,
    instance_id: int,
    timestamp_ms: int
) -> Tuple[str, Dict[str, Any]]:
    async def one_try(wait_ms: int) -> Tuple[str, Dict[str, Any]]:
        fields = {
            "symbol": symbol,
            "timeframe": timeframe,
            "instance_id": str(instance_id),
            "timestamp_ms": str(timestamp_ms),
        }
        try:
            req_id = await redis.xadd(IND_REQ_STREAM, fields)
        except Exception:
            log.warning("stream_error: XADD indicator_request failed", exc_info=True)
            return "stream_error", {}

        q = await router.register(req_id)
        try:
            try:
                msg_id, data = await asyncio.wait_for(q.get(), timeout=wait_ms / 1000.0)
            except asyncio.TimeoutError:
                return "timeout", {}

            status = data.get("status", "error")
            if status != "ok":
                return data.get("error", "exception"), {}

            try:
                open_time = data.get("open_time") or ""
                results_raw = data.get("results") or "{}"
                results = json.loads(results_raw)
                return "ok", {"open_time": open_time, "results": results}
            except Exception:
                return "exception", {}
        finally:
            await router.unregister(req_id)

    st, pl = await one_try(REQ_RESPONSE_TIMEOUT_MS)
    if st != "timeout":
        return st, pl
    log.info("IND_POSSTAT: retry on timeout for instance_id=%s symbol=%s tf=%s", instance_id, symbol, timeframe)
    return await one_try(SECOND_TRY_TIMEOUT_MS)


# ðŸ”¸ Ð¡Ð±Ð¾Ñ€ ÑÑ‚Ñ€Ð¾Ðº Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ° Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð° (Ð¾Ð±Ñ‰Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð´Ð»Ñ Ð²ÑÐµÑ… TF)
async def build_rows_for_instance(
    redis,
    router: IndicatorResponseRouter,
    *,
    symbol: str,
    tf: str,
    instance: Dict[str, Any],
    bar_open_ms: int,
    strategy_id: int,
    position_uid: str
) -> List[Tuple]:
    instance_id = int(instance["id"])
    status, payload = await request_indicator_snapshot(
        redis,
        router,
        symbol=symbol,
        timeframe=tf,
        instance_id=instance_id,
        timestamp_ms=bar_open_ms
    )

    rows: List[Tuple] = []

    if status != "ok":
        base_short = indicator_base_from_instance(instance)
        open_time_dt = datetime.utcfromtimestamp(bar_open_ms / 1000)
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "indicator", base_short, base_short,
            None, status,
            open_time_dt, "error", status
        ))
        return rows

    ot_raw = payload.get("open_time")
    try:
        open_time_dt = datetime.fromisoformat(ot_raw) if ot_raw else datetime.utcfromtimestamp(bar_open_ms / 1000)
    except Exception:
        open_time_dt = datetime.utcfromtimestamp(bar_open_ms / 1000)

    results: Dict[str, str] = payload.get("results", {})
    for param_name, str_value in results.items():
        base_short = indicator_base_from_param_name(param_name)
        fval = to_float_safe(str_value)
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "indicator", base_short, param_name,
            fval if fval is not None else None,
            None if fval is not None else str_value,
            open_time_dt, "ok", None
        ))
    return rows


# ðŸ”¸ ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² PostgreSQL
async def run_insert_batch(pg, rows: List[Tuple]) -> None:
    if not rows:
        return
    sql = """
        INSERT INTO indicator_position_stat
        (position_uid, strategy_id, symbol, timeframe, param_type, param_base, param_name, value_num, value_text, open_time, status, error_code)
        VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12)
        ON CONFLICT (position_uid, timeframe, param_type, param_base, param_name)
        DO UPDATE SET
          value_num   = EXCLUDED.value_num,
          value_text  = EXCLUDED.value_text,
          open_time   = EXCLUDED.open_time,
          status      = EXCLUDED.status,
          error_code  = EXCLUDED.error_code,
          captured_at = NOW()
    """
    async with pg.acquire() as conn:
        async with conn.transaction():
            await conn.executemany(sql, rows)


# ðŸ”¸ ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð²Ð¾Ñ€ÐºÐµÑ€ ÑÐ½Ð¸Ð¼ÐºÐ° (ÑÑ‚Ð°Ð¿ 2: m5 â†’ m15/h1, Ð²ÑÐµ param_type=indicator)
async def run_indicator_position_snapshot(pg, redis, get_instances_by_tf):
    log.info("IND_POSSTAT: Ð²Ð¾Ñ€ÐºÐµÑ€ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ (phase=2 indicators m5+m15+h1)")

    # ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ consumer-group Ð´Ð»Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹ (Ð¸Ð´ÐµÐ¼Ð¿Ð¾Ñ‚ÐµÐ½Ñ‚Ð½Ð¾)
    try:
        await redis.xgroup_create(POSITIONS_OPEN_STREAM, IPS_GROUP, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create (positions) error: {e}")

    # Ð¿Ð¾Ð´Ð½ÑÑ‚ÑŒ Ñ€Ð¾ÑƒÑ‚ÐµÑ€ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¸ ÐµÐ³Ð¾ consumer-group
    router = IndicatorResponseRouter(redis)
    await router.start()

    sem = asyncio.Semaphore(PARALLEL_REQUESTS_LIMIT)

    async def process_tf(tf: str, position_uid: str, strategy_id: int, symbol: str, bar_open_ms: int) -> int:
        # ÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¸Ð½ÑÑ‚Ð°Ð½ÑÑ‹ Ð¿Ð¾ TF
        instances = [i for i in get_instances_by_tf(tf)]
        if not instances:
            log.info(f"IND_POSSTAT: no_instances_{tf} symbol={symbol}")
            return 0

        # Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð² Ñ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð¼
        rows_all: List[Tuple] = []

        async def run_one(inst):
            async with sem:
                try:
                    r = await build_rows_for_instance(
                        redis, router,
                        symbol=symbol, tf=tf, instance=inst,
                        bar_open_ms=bar_open_ms,
                        strategy_id=strategy_id,
                        position_uid=position_uid
                    )
                    return r
                except Exception:
                    log.warning(f"IND_POSSTAT: exception in build_rows_for_instance tf={tf}", exc_info=True)
                    base_short = indicator_base_from_instance(inst)
                    open_time_dt = datetime.utcfromtimestamp(bar_open_ms / 1000)
                    return [(
                        position_uid, strategy_id, symbol, tf,
                        "indicator", base_short, base_short,
                        None, "exception",
                        open_time_dt, "error", "exception"
                    )]

        tasks = [asyncio.create_task(run_one(inst)) for inst in instances]
        results_batches = await asyncio.gather(*tasks, return_exceptions=False)
        for batch in results_batches:
            rows_all.extend(batch)

        # Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¿Ð°Ñ‡ÐºÐ°Ð¼Ð¸
        for i in range(0, len(rows_all), BATCH_INSERT_MAX):
            await run_insert_batch(pg, rows_all[i:i + BATCH_INSERT_MAX])

        return len(rows_all)

    async def process_position(msg_id: str, data: Dict[str, Any]) -> Optional[str]:
        # ÑƒÑÐ»Ð¾Ð²Ð¸Ñ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸
        position_uid = data.get("position_uid")
        strategy_id_s = data.get("strategy_id")
        symbol = data.get("symbol")
        created_at_iso = data.get("created_at")

        if not position_uid or not strategy_id_s or not symbol or not created_at_iso:
            log.info(f"IND_POSSTAT: bad_event msg_id={msg_id} data_keys={list(data.keys())}")
            return msg_id

        try:
            strategy_id = int(strategy_id_s)
        except Exception:
            strategy_id = 0

        ts_ms = parse_iso_to_ms(created_at_iso)
        if ts_ms is None:
            log.info(f"IND_POSSTAT: bad_event_time position_uid={position_uid}")
            return msg_id

        total_rows = 0

        # m5 â€” ÑÐ½Ð°Ñ‡Ð°Ð»Ð° (Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚)
        tf = "m5"
        t0 = asyncio.get_event_loop().time()
        rows_m5 = await process_tf(tf, position_uid, strategy_id, symbol, floor_to_bar(ts_ms, tf))
        total_rows += rows_m5
        t1 = asyncio.get_event_loop().time()
        log.info(f"IND_POSSTAT: {tf} indicators done position_uid={position_uid} symbol={symbol} rows={rows_m5} elapsed_ms={int((t1-t0)*1000)}")

        # m15 Ð¸ h1 â€” Ð¿Ð¾Ñ‚Ð¾Ð¼ (Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾)
        async def run_tf(tf2: str):
            t_start = asyncio.get_event_loop().time()
            rows = await process_tf(tf2, position_uid, strategy_id, symbol, floor_to_bar(ts_ms, tf2))
            t_end = asyncio.get_event_loop().time()
            log.info(f"IND_POSSTAT: {tf2} indicators done position_uid={position_uid} symbol={symbol} rows={rows} elapsed_ms={int((t_end-t_start)*1000)}")
            return rows

        rows_m15, rows_h1 = await asyncio.gather(run_tf("m15"), run_tf("h1"))
        total_rows += rows_m15 + rows_h1

        # Ð¸Ñ‚Ð¾Ð³ Ð¿Ð¾ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸
        log.info(f"IND_POSSTAT: indicators all TF done position_uid={position_uid} symbol={symbol} total_rows={total_rows}")

        return msg_id

    # ðŸ”¸ ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹: Ð¿Ð°Ñ‡ÐºÐ¾Ð¹ + Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° + Ð±Ð°Ñ‚Ñ‡-ACK
    while True:
        try:
            resp = await redis.xreadgroup(
                groupname=IPS_GROUP,
                consumername=IPS_CONSUMER,
                streams={POSITIONS_OPEN_STREAM: ">"},
                count=100,
                block=2000
            )
        except Exception as e:
            log.error(f"IND_POSSTAT: read error: {e}", exc_info=True)
            await asyncio.sleep(0.5)
            continue

        if not resp:
            continue

        try:
            tasks = []
            for _, messages in resp:
                for msg_id, data in messages:
                    tasks.append(asyncio.create_task(process_position(msg_id, data)))

            done_ids = await asyncio.gather(*tasks, return_exceptions=False)
            ack_ids = [mid for mid in done_ids if mid]
            if ack_ids:
                await redis.xack(POSITIONS_OPEN_STREAM, IPS_GROUP, *ack_ids)
        except Exception as e:
            log.error(f"IND_POSSTAT: batch error: {e}", exc_info=True)
            await asyncio.sleep(0.5)