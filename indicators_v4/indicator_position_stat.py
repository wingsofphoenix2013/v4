# indicator_position_stat.py ‚Äî –≤–æ—Ä–∫–µ—Ä on-demand —Å–Ω–∏–º–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ (—ç—Ç–∞–ø 2+3+4: m5+m15+h1; –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã + packs + marketwatch; –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏; –ø–µ—Ä-TF –ª–æ–≥)

import asyncio
import json
import logging
from datetime import datetime, timedelta

# üî∏ –í—Ä–µ–º—è –±–∞—Ä–∞ (floor –∫ –Ω–∞—á–∞–ª—É)
from packs.pack_utils import floor_to_bar

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å—Ç—Ä–∏–º–æ–≤ –∏ —Ç–∞–±–ª–∏—Ü
POSITIONS_OPEN_STREAM = "positions_open_stream"
INDICATOR_REQ_STREAM  = "indicator_request"
INDICATOR_RESP_STREAM = "indicator_response"
GW_REQ_STREAM         = "indicator_gateway_request"
GW_RESP_STREAM        = "indicator_gateway_response"
TARGET_TABLE          = "indicator_position_stat"

# üî∏ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ—Ä–∫–µ—Ä–∞ / –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
REQUIRED_TFS           = ("m5", "m15", "h1")  # —Ç–µ–ø–µ—Ä—å –≤—Å–µ 3 –¢–§
POLL_INTERVAL_SEC      = 1                    # —á–∞—Å—Ç–æ—Ç–∞ —Ä–µ—Ç—Ä–∞–µ–≤
RESP_BLOCK_MS          = 300                  # –∫–æ—Ä–æ—Ç–∫–∏–π –±–ª–æ–∫ –Ω–∞ —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
GLOBAL_TIMEOUT_SEC     = 600                  # 10 –º–∏–Ω—É—Ç –Ω–∞ –ø–æ–∑–∏—Ü–∏—é
BATCH_SIZE_POS_OPEN    = 20                   # —á—Ç–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
BATCH_SIZE_RESP        = 200                  # —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ (indicator/gateway)
CONCURRENCY_PER_TF     = {"m5": 80, "m15": 40, "h1": 30}  # –ª–∏–º–∏—Ç—ã on-demand per TF (m5 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)
POSITIONS_CONCURRENCY  = 16                   # –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–æ–∑–∏—Ü–∏–π

# üî∏ –ü–∞–∫–µ—Ç—ã –∏ MW
PACK_INDS = ("ema", "rsi", "mfi", "bb", "lr", "atr", "adx_dmi", "macd")
MW_KINDS  = ("trend", "volatility", "momentum", "extremes")

# üî∏ –ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏ –ø–æ–ª–µ–π –ø–∞–∫–æ–≤ (—Å—Ç—Ä–æ–≥–æ –∫–∞–∫ –∑–∞–¥–∞–Ω–æ)
PACK_FIELD_WHITELIST = {
    "rsi":     ["bucket_low", "trend"],
    "mfi":     ["bucket_low", "trend"],
    "bb":      ["bucket", "bucket_delta", "bw_trend_strict", "bw_trend_smooth"],
    "lr":      ["bucket", "bucket_delta", "angle_trend"],
    "atr":     ["bucket", "bucket_delta"],
    "adx_dmi": ["adx_bucket_low", "adx_dynamic_strict", "adx_dynamic_smooth",
                "gap_bucket_low", "gap_dynamic_strict", "gap_dynamic_smooth"],
    "ema":     ["side", "dynamic", "dynamic_strict", "dynamic_smooth"],
    "macd":    ["mode", "cross", "zero_side", "hist_bucket_low_pct",
                "hist_trend_strict", "hist_trend_smooth"],
}

# üî∏ –õ–æ–≥–≥–µ—Ä
log = logging.getLogger("IND_POS_STAT")


# üî∏ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è created_at ‚Üí open_time –±–∞—Ä–∞ (ms)
def to_bar_open_ms(created_at_iso: str, tf: str) -> int:
    dt = datetime.fromisoformat(created_at_iso)
    return floor_to_bar(int(dt.timestamp() * 1000), tf)


# üî∏ –ü–∞—Ä—Å ISO ‚Üí datetime
def parse_iso(s: str) -> datetime:
    return datetime.fromisoformat(s)


# üî∏ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (param_type='indicator')
def build_rows_for_indicator_response(position_uid: str,
                                      strategy_id: int,
                                      symbol: str,
                                      tf: str,
                                      indicator_name: str,
                                      open_time_iso: str,
                                      results_json: str) -> list[tuple]:
    rows = []
    try:
        results = json.loads(results_json)
    except Exception:
        results = {}
    if not isinstance(results, dict) or not results:
        return rows
    open_time = parse_iso(open_time_iso)
    for param_name, str_val in results.items():
        try:
            value_num = float(str_val)
        except Exception:
            continue
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "indicator", indicator_name, param_name,
            value_num, None,
            open_time,
            "ok", None
        ))
    return rows


# üî∏ –í—Å—Ç–∞–≤–∫–∞ –ø–∞—á–∫–∏ —Å—Ç—Ä–æ–∫ –≤ PG (UPSERT); –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç upsert_count
async def insert_rows_pg(pg, rows: list[tuple]) -> int:
    if not rows:
        return 0
    async with pg.acquire() as conn:
        async with conn.transaction():
            await conn.executemany(f"""
                INSERT INTO {TARGET_TABLE}
                (position_uid, strategy_id, symbol, timeframe, param_type, param_base, param_name,
                 value_num, value_text, open_time, status, error_code, captured_at)
                VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12, NOW())
                ON CONFLICT (position_uid, timeframe, param_type, param_base, param_name)
                DO UPDATE SET
                    value_num = EXCLUDED.value_num,
                    value_text = EXCLUDED.value_text,
                    open_time = EXCLUDED.open_time,
                    status = EXCLUDED.status,
                    error_code = EXCLUDED.error_code,
                    captured_at = NOW()
            """, rows)
    return len(rows)


# üî∏ –ü–æ–¥—Å—á—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –∏/–∏–ª–∏ –¢–§
async def count_unique_rows(pg, position_uid: str, tf: str | None = None) -> int:
    async with pg.acquire() as conn:
        if tf is None:
            rec = await conn.fetchrow(
                f"SELECT COUNT(*) AS cnt FROM {TARGET_TABLE} WHERE position_uid = $1", position_uid
            )
        else:
            rec = await conn.fetchrow(
                f"SELECT COUNT(*) AS cnt FROM {TARGET_TABLE} WHERE position_uid = $1 AND timeframe = $2",
                position_uid, tf
            )
        return int(rec["cnt"]) if rec else 0


# üî∏ –ê–Ω—Ç–∏–¥—É–±–ª–∏: –ª–æ–∫–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∫–æ—Ä—Ç–µ–∂–µ–π –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∫–ª—é—á—É
def dedup_rows(rows: list[tuple]) -> list[tuple]:
    seen = set()
    out = []
    for r in rows:
        key = (r[0], r[3], r[4], r[5], r[6])  # position_uid,timeframe,param_type,param_base,param_name
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out


# üî∏ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –±–∞–∑—ã –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É (ema50 ‚Üí ema, bb20_2_0 ‚Üí bb, ...)
def base_kind(base: str) -> str | None:
    for k in PACK_FIELD_WHITELIST.keys():
        if base.startswith(k):
            return k
    return None


# üî∏ –£—Ç–∏–ª–∏—Ç–∞: –ø–ª–æ—Å–∫–∏–π –æ–±—Ö–æ–¥ pack['pack'] (—Ñ–∏–ª—å—Ç—Ä –º–µ—Ç–∞-–ø–æ–ª–µ–π)
def flatten_pack_dict(d: dict):
    for k, v in d.items():
        if k in ("open_time", "ref", "used_bases", "prev_state", "raw_state",
                 "streak_preview", "strong", "direction", "max_adx", "deltas"):
            continue
        yield (k, v)


# üî∏ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–∞–∫–æ–≤ (param_type='pack') –ø–æ whitelist
def build_rows_for_pack_response(position_uid: str,
                                 strategy_id: int,
                                 symbol: str,
                                 tf: str,
                                 base: str,
                                 open_time_iso: str,
                                 pack_payload: dict) -> list[tuple]:
    rows = []
    kind = base_kind(base)
    if not kind:
        return rows
    allowed = set(PACK_FIELD_WHITELIST.get(kind, []))
    if not allowed:
        return rows
    open_time = parse_iso(open_time_iso)
    for pname, pval in flatten_pack_dict(pack_payload):
        if pname not in allowed:
            continue
        val_num = None
        val_text = None
        if isinstance(pval, (int, float)):
            val_num = float(pval)
        else:
            try:
                val_num = float(pval)
            except Exception:
                if pval is None:
                    continue
                if isinstance(pval, bool):
                    val_text = "true" if pval else "false"
                else:
                    val_text = str(pval)
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "pack", base, pname,
            val_num, val_text,
            open_time,
            "ok", None
        ))
    return rows


# üî∏ –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–µ –±–∞–∑—ã –ø–∞–∫–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ (–ø–æ TF)
def build_expected_pack_bases(instances: list[dict]) -> dict[str, set[str]]:
    expected: dict[str, set[str]] = {ind: set() for ind in PACK_INDS}
    for inst in instances:
        ind = inst["indicator"]
        params = inst["params"]
        if ind not in expected:
            continue
        if ind in ("ema", "rsi", "mfi", "atr", "adx_dmi", "lr"):
            try:
                L = int(params["length"])
                expected[ind].add(f"{ind}{L}")
            except Exception:
                pass
        elif ind == "macd":
            try:
                F = int(params["fast"])
                expected[ind].add(f"macd{F}")
            except Exception:
                pass
        elif ind == "bb":
            try:
                L = int(params["length"])
                S = round(float(params["std"]), 2)
                std_str = str(S).replace(".", "_")
                expected[ind].add(f"bb{L}_{std_str}")
            except Exception:
                pass
    return expected


# üî∏ –†–æ—É—Ç–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤: —á–∏—Ç–∞–µ—Ç indicator_response –∏ gateway_response, –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ req_id
async def run_response_router(redis, req_routes: dict, req_lock: asyncio.Lock,
                              stop_event: asyncio.Event):
    last_ind_id = "0-0"
    last_gw_id  = "0-0"

    async def drain(stream_key: str, last_id: str):
        try:
            got = await redis.xread(streams={stream_key: last_id},
                                    count=BATCH_SIZE_RESP, block=RESP_BLOCK_MS)
        except Exception:
            return last_id, []
        if not got:
            return last_id, []
        out = []
        for _, msgs in got:
            for rid, payload in msgs:
                out.append((rid, payload))
                last_id = rid
        return last_id, out

    while not stop_event.is_set():
        last_ind_id, ind_items = await drain(INDICATOR_RESP_STREAM, last_ind_id)
        for rid, payload in ind_items:
            req_id = payload.get("req_id")
            if not req_id:
                continue
            queue = None
            async with req_lock:
                queue = req_routes.get(req_id)
            if queue:
                try:
                    await queue.put(("indicator", payload))
                except Exception:
                    pass

        last_gw_id, gw_items = await drain(GW_RESP_STREAM, last_gw_id)
        for rid, payload in gw_items:
            req_id = payload.get("req_id")
            if not req_id:
                continue
            queue = None
            async with req_lock:
                queue = req_routes.get(req_id)
            if queue:
                try:
                    await queue.put(("gateway", payload))
                except Exception:
                    pass

        await asyncio.sleep(0.01)


# üî∏ –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ–¥–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ (–æ—Ç–¥–µ–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞; m5 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, m15/h1 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
async def handle_position(pg, redis, get_instances_by_tf,
                          position_uid: str, strategy_id: int, symbol: str, created_at_iso: str,
                          req_routes: dict, req_lock: asyncio.Lock,
                          tf_semaphores: dict[str, asyncio.Semaphore]):

    instances_by_tf = {tf: [i for i in get_instances_by_tf(tf)] for tf in REQUIRED_TFS}
    expected_bases_by_tf = {tf: build_expected_pack_bases(instances_by_tf[tf]) for tf in REQUIRED_TFS}
    bar_open_ms_by_tf = {tf: to_bar_open_ms(created_at_iso, tf) for tf in REQUIRED_TFS}

    # —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ TF
    start_ts_global = datetime.utcnow()
    tf_start_ts = {tf: None for tf in REQUIRED_TFS}
    tf_done = {tf: False for tf in REQUIRED_TFS}
    deadline = start_ts_global + timedelta(seconds=GLOBAL_TIMEOUT_SEC)

    # –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    ind_ctx = {
        tf: {
            inst["id"]: {"inflight": False, "state": "pending", "last_err": None, "req_ids": set(), "indicator": inst["indicator"]}
            for inst in instances_by_tf[tf]
        } for tf in REQUIRED_TFS
    }
    # –ø–∞–∫–∏
    pack_ctx = {
        tf: {
            ind: {"inflight": False, "state": "pending", "last_err": None,
                  "req_ids": set(), "done_bases": set(), "expected_bases": set(expected_bases_by_tf[tf].get(ind, set()))}
            for ind in PACK_INDS if expected_bases_by_tf[tf].get(ind)
        } for tf in REQUIRED_TFS
    }
    # marketwatch
    mw_ctx = {
        tf: {kind: {"inflight": False, "state": "pending", "last_err": None, "req_ids": set()}
             for kind in MW_KINDS} for tf in REQUIRED_TFS
    }

    # –æ—á–µ—Ä–µ–¥—å –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏
    resp_queue: asyncio.Queue = asyncio.Queue()

    # —Ä–µ—Ç—Ä–∞–∏–±–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞?
    def is_retriable(err: str) -> bool:
        return err not in ("instance_not_active", "exception")

    # –æ—Ç–ø—Ä–∞–≤–∫–∞ indicator_request
    async def send_indicator(tf: str, inst_id: int):
        s = ind_ctx[tf][inst_id]
        if s["inflight"] or s["state"] != "pending":
            return
        if tf_start_ts[tf] is None:
            tf_start_ts[tf] = datetime.utcnow()
        async with tf_semaphores[tf]:
            rid = await redis.xadd(INDICATOR_REQ_STREAM, {
                "symbol": symbol,
                "timeframe": tf,
                "instance_id": str(inst_id),
                "timestamp_ms": str(bar_open_ms_by_tf[tf])
            })
        async with req_lock:
            req_routes[rid] = resp_queue
        s["inflight"] = True
        s["req_ids"].add(rid)

    # –æ—Ç–ø—Ä–∞–≤–∫–∞ gateway_request (pack –∏–ª–∏ mw)
    async def send_gateway(tf: str, indicator_or_kind: str):
        if indicator_or_kind in pack_ctx[tf]:
            s = pack_ctx[tf][indicator_or_kind]
            ok_states = ("pending", "ok_part")
        else:
            s = mw_ctx[tf][indicator_or_kind]
            ok_states = ("pending",)
        if s["inflight"] or s["state"] not in ok_states:
            return
        if tf_start_ts[tf] is None:
            tf_start_ts[tf] = datetime.utcnow()
        async with tf_semaphores[tf]:
            rid = await redis.xadd(GW_REQ_STREAM, {
                "symbol": symbol,
                "timeframe": tf,
                "indicator": indicator_or_kind,
                "timestamp_ms": str(bar_open_ms_by_tf[tf])
            })
        async with req_lock:
            req_routes[rid] = resp_queue
        s["inflight"] = True
        s["req_ids"].add(rid)

    # –ø–µ—Ä–≤–∞—è –≤–æ–ª–Ω–∞: m5 —Å–Ω–∞—á–∞–ª–∞, –∑–∞—Ç–µ–º m15+h1 (–ø–æ—á—Ç–∏ —Å—Ä–∞–∑—É, –Ω–æ –ø–æ—Å–ª–µ m5)
    if "m5" in REQUIRED_TFS:
        await asyncio.gather(*[send_indicator("m5", inst["id"]) for inst in instances_by_tf["m5"]])
        await asyncio.gather(*[send_gateway("m5", ind) for ind in pack_ctx["m5"].keys()])
        await asyncio.gather(*[send_gateway("m5", kind) for kind in MW_KINDS])

    for tf in ("m15", "h1"):
        if tf in REQUIRED_TFS:
            await asyncio.gather(*[send_indicator(tf, inst["id"]) for inst in instances_by_tf[tf]])
            await asyncio.gather(*[send_gateway(tf, ind) for ind in pack_ctx[tf].keys()])
            await asyncio.gather(*[send_gateway(tf, kind) for kind in MW_KINDS])

    total_upserts = 0
    upserts_by_tf = {tf: 0 for tf in REQUIRED_TFS}

    # –≥–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –¥–æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö TF –∏–ª–∏ —Ç–∞–π–º–∞—É—Ç–∞
    while True:
        now = datetime.utcnow()
        if now >= deadline:
            # –ª–æ–≥ –ø–æ —Ç–∞–π–º–∞—É—Ç—É
            for tf in REQUIRED_TFS:
                if tf_done[tf]:
                    continue
                for inst_id, s in ind_ctx[tf].items():
                    if s["state"] == "pending":
                        log.debug(f"[TIMEOUT] IND {symbol}/{tf} inst_id={inst_id} {s['indicator']} last_err={s['last_err']}")
                for ind, s in pack_ctx[tf].items():
                    if s["state"] in ("pending", "ok_part"):
                        missing = sorted(list(s["expected_bases"] - s["done_bases"]))
                        log.debug(f"[TIMEOUT] PACK {symbol}/{tf} {ind} missing_bases={missing} last_err={s['last_err']}")
                for kind, s in mw_ctx[tf].items():
                    if s["state"] == "pending":
                        log.debug(f"[TIMEOUT] MW {symbol}/{tf} {kind} last_err={s['last_err']}")
            break

        # –æ–∫–Ω–æ POLL_INTERVAL_SEC: —Å–æ–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç—ã, –ø–∏—à–µ–º –≤ –ë–î –ø–æ—Ä—Ü–∏—è–º–∏
        end_wait = now + timedelta(seconds=POLL_INTERVAL_SEC)
        collected_rows: list[tuple] = []

        while datetime.utcnow() < end_wait:
            try:
                src, payload = await asyncio.wait_for(resp_queue.get(), timeout=RESP_BLOCK_MS / 1000.0)
            except asyncio.TimeoutError:
                break

            status = payload.get("status")
            req_id = payload.get("req_id")
            r_symbol = payload.get("symbol")
            tf = payload.get("timeframe")

            if req_id:
                async with req_lock:
                    req_routes.pop(req_id, None)

            if r_symbol != symbol or tf not in REQUIRED_TFS:
                continue

            if src == "indicator":
                instance_id_raw = payload.get("instance_id")
                if not instance_id_raw:
                    continue
                iid = int(instance_id_raw)
                s = ind_ctx[tf].get(iid)
                if not s or req_id not in s["req_ids"]:
                    continue
                s["req_ids"].discard(req_id)
                s["inflight"] = False

                if status == "ok":
                    indicator_name = s["indicator"]
                    rows = build_rows_for_indicator_response(
                        position_uid=position_uid,
                        strategy_id=strategy_id,
                        symbol=symbol,
                        tf=tf,
                        indicator_name=indicator_name,
                        open_time_iso=payload.get("open_time"),
                        results_json=payload.get("results", "{}"),
                    )
                    collected_rows.extend(rows)
                    s["state"] = "ok"
                    s["last_err"] = None
                else:
                    err = payload.get("error") or "unknown"
                    s["last_err"] = err
                    if not is_retriable(err):
                        s["state"] = "error"

            else:  # gateway (packs/MW)
                ind = payload.get("indicator")
                ctx_slot = None
                ctx_type = None
                if ind in pack_ctx[tf]:
                    ctx_slot = pack_ctx[tf][ind]
                    ctx_type = "pack"
                elif ind in mw_ctx[tf]:
                    ctx_slot = mw_ctx[tf][ind]
                    ctx_type = "mw"
                else:
                    continue
                if req_id not in ctx_slot["req_ids"]:
                    continue
                ctx_slot["req_ids"].discard(req_id)
                ctx_slot["inflight"] = False

                if status == "ok":
                    try:
                        results = json.loads(payload.get("results", "[]"))
                    except Exception:
                        results = []

                    if ctx_type == "pack":
                        if not isinstance(results, list):
                            ctx_slot["last_err"] = "bad_results"
                        else:
                            for item in results:
                                base = item.get("base")
                                p = item.get("pack", {})
                                if not base or not isinstance(p, dict):
                                    continue
                                open_time_iso = p.get("open_time") or datetime.utcfromtimestamp(bar_open_ms_by_tf[tf] / 1000).isoformat()
                                rows = build_rows_for_pack_response(position_uid, strategy_id, symbol, tf, base, open_time_iso, p)
                                if rows:
                                    collected_rows.extend(rows)
                                    ctx_slot["done_bases"].add(base)
                            ctx_slot["state"] = "ok" if (ctx_slot["expected_bases"] <= ctx_slot["done_bases"]) else "ok_part"
                            ctx_slot["last_err"] = None
                    else:
                        if not isinstance(results, list) or not results:
                            ctx_slot["last_err"] = "bad_results"
                        else:
                            item = results[0]
                            base = item.get("base", ind)  # trend|volatility|momentum|extremes
                            p = item.get("pack", {})
                            state_val = p.get("state")
                            if state_val is not None:
                                open_time_iso = p.get("open_time") or datetime.utcfromtimestamp(bar_open_ms_by_tf[tf] / 1000).isoformat()
                                rows = [(
                                    position_uid, strategy_id, symbol, tf,
                                    "marketwatch", base, "state",
                                    None, str(state_val),
                                    parse_iso(open_time_iso),
                                    "ok", None
                                )]
                                collected_rows.extend(rows)
                                ctx_slot["state"] = "ok"
                                ctx_slot["last_err"] = None
                            else:
                                ctx_slot["last_err"] = "no_state"
                else:
                    err = payload.get("error") or "unknown"
                    ctx_slot["last_err"] = err
                    if not is_retriable(err):
                        ctx_slot["state"] = "error"

        # –∑–∞–ø–∏—Å—å –≤ –ë–î (–ª–æ–∫–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)
        if collected_rows:
            deduped = dedup_rows(collected_rows)
            # —Ä–∞–∑–¥–µ–ª–∏–º –ø–æ TF, —á—Ç–æ–±—ã –∞–∫–∫—É–º—É–ª–∏—Ä–æ–≤–∞—Ç—å per-TF upserts
            by_tf = {}
            for r in deduped:
                by_tf.setdefault(r[3], []).append(r)  # r[3] = timeframe
            for tf, chunk in by_tf.items():
                n = await insert_rows_pg(pg, chunk)
                total_upserts += n
                upserts_by_tf[tf] += n

        # —Ä–µ—Ç—Ä–∞–∏ ¬´—Ä–∞–∑ –≤ —Å–µ–∫—É–Ω–¥—É¬ª: –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–º, –∫—Ç–æ pending –∏ –Ω–µ inflight (—Å —É—á—ë—Ç–æ–º retriable)
        for tf in REQUIRED_TFS:
            # m5 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ: –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –≤ –ø–æ—Ä—è–¥–∫–µ m5‚Üím15‚Üíh1 (–ª–∏–º–∏—Ç—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –≤ —Å–µ–º–∞—Ñ–æ—Ä–∞—Ö)
            for inst_id, s in ind_ctx[tf].items():
                if s["state"] == "pending" and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                    await send_indicator(tf, inst_id)
            for ind, s in pack_ctx[tf].items():
                if s["state"] in ("pending", "ok_part") and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                    await send_gateway(tf, ind)
            for kind, s in mw_ctx[tf].items():
                if s["state"] == "pending" and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                    await send_gateway(tf, kind)

        # –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–º—É TF ‚Üí –ª–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä-TF –∏ –æ—Ç–º–µ—á–∞–µ–º done
        def all_done_tf(tf: str) -> bool:
            if any(s["state"] in ("pending", "error") for s in ind_ctx[tf].values()):
                return False
            for ind, s in pack_ctx[tf].items():
                if s["state"] == "error":
                    return False
                if not (s["expected_bases"] <= s["done_bases"]):
                    return False
            if any(s["state"] in ("pending", "error") for s in mw_ctx[tf].values()):
                return False
            return True

        for tf in REQUIRED_TFS:
            if not tf_done[tf] and all_done_tf(tf):
                tf_done[tf] = True
                elapsed_ms_tf = int((datetime.utcnow() - (tf_start_ts[tf] or start_ts_global)).total_seconds() * 1000)
                ok_inst_tf = sum(1 for s in ind_ctx[tf].values() if s["state"] == "ok")
                ok_packs_tf = sum(len(s["done_bases"]) for s in pack_ctx[tf].values())
                ok_mw_tf = sum(1 for s in mw_ctx[tf].values() if s["state"] == "ok")
                unique_rows_tf = await count_unique_rows(pg, position_uid, tf)
                log.debug(
                    f"IND_POS_STAT: position={position_uid} {symbol} {tf} snapshot complete: "
                    f"ok_instances={ok_inst_tf}, ok_packs={ok_packs_tf}, ok_mw={ok_mw_tf}, "
                    f"rows_upserted_tf={upserts_by_tf[tf]}, unique_rows_tf={unique_rows_tf}, elapsed_ms={elapsed_ms_tf}"
                )

        # –µ—Å–ª–∏ –≤—Å–µ TF –∑–∞–≤–µ—Ä—à–µ–Ω—ã ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ª–æ–≥ –∏ –≤—ã—Ö–æ–¥
        if all(tf_done.values()):
            elapsed_ms = int((datetime.utcnow() - start_ts_global).total_seconds() * 1000)
            unique_all = await count_unique_rows(pg, position_uid, None)
            total_ok_inst = sum(1 for tf in REQUIRED_TFS for s in ind_ctx[tf].values() if s["state"] == "ok")
            total_ok_packs = sum(len(s["done_bases"]) for tf in REQUIRED_TFS for s in pack_ctx[tf].values())
            total_ok_mw = sum(1 for tf in REQUIRED_TFS for s in mw_ctx[tf].values() if s["state"] == "ok")
            log.debug(
                f"IND_POS_STAT: position={position_uid} {symbol} ALL TF done: "
                f"ok_instances={total_ok_inst}, ok_packs={total_ok_packs}, ok_mw={total_ok_mw}, "
                f"rows_upserted_total={total_upserts}, unique_rows_total={unique_all}, elapsed_ms={elapsed_ms}"
            )
            break


# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä: –¥–∏—Å–ø–µ—Ç—á–µ—Ä –ø–æ–∑–∏—Ü–∏–π + –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤
async def run_indicator_position_stat(pg, redis, get_instances_by_tf, get_precision):
    group = "iv4_possnap_group"
    consumer = "iv4_possnap_1"

    # —Å–æ–∑–¥–∞—Ç—å consumer-group –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∞
    try:
        await redis.xgroup_create(POSITIONS_OPEN_STREAM, group, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create error: {e}")

    # —Ä–æ—É—Ç–∏–Ω–≥ req_id ‚Üí –æ—á–µ—Ä–µ–¥—å –ø–æ–∑–∏—Ü–∏–∏
    req_routes: dict[str, asyncio.Queue] = {}
    req_lock = asyncio.Lock()

    # –ª–∏–º–∏—Ç–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
    pos_sema = asyncio.Semaphore(POSITIONS_CONCURRENCY)

    # —Å–µ–º–∞—Ñ–æ—Ä—ã TF –¥–ª—è on-demand –∑–∞–ø—Ä–æ—Å–æ–≤
    tf_semaphores = {
        tf: asyncio.Semaphore(CONCURRENCY_PER_TF.get(tf, 30)) for tf in REQUIRED_TFS
    }

    # –æ—Å—Ç–∞–Ω–æ–≤ —Ä–æ—É—Ç–µ—Ä–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
    stop_event = asyncio.Event()
    router_task = asyncio.create_task(run_response_router(redis, req_routes, req_lock, stop_event))

    try:
        while True:
            try:
                resp = await redis.xreadgroup(
                    groupname=group,
                    consumername=consumer,
                    streams={POSITIONS_OPEN_STREAM: ">"},
                    count=BATCH_SIZE_POS_OPEN,
                    block=2000
                )
            except Exception as e:
                log.error(f"positions read error: {e}", exc_info=True)
                await asyncio.sleep(0.5)
                continue

            if not resp:
                continue

            to_ack = []
            pos_tasks = []

            for _, messages in resp:
                for msg_id, data in messages:
                    try:
                        if (data.get("event_type") or "").lower() != "opened":
                            to_ack.append(msg_id)
                            continue

                        position_uid = data["position_uid"]
                        strategy_id = int(data["strategy_id"])
                        symbol = data["symbol"]
                        created_at_iso = data.get("created_at") or data.get("received_at")
                        if not created_at_iso:
                            log.debug(f"[SKIP] position {position_uid}: no created_at/received_at")
                            to_ack.append(msg_id)
                            continue

                        async def run_one():
                            async with pos_sema:
                                await handle_position(pg, redis, get_instances_by_tf,
                                                      position_uid, strategy_id, symbol, created_at_iso,
                                                      req_routes, req_lock, tf_semaphores)

                        task = asyncio.create_task(run_one())
                        pos_tasks.append((msg_id, task))

                    except Exception as e:
                        log.error(f"position spawn error: {e}", exc_info=True)

            # –∂–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á —ç—Ç–æ–π –ø–∞—á–∫–∏ –∏ ACK-–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            for msg_id, task in pos_tasks:
                try:
                    await task
                    to_ack.append(msg_id)
                except Exception as e:
                    log.error(f"position task error: {e}", exc_info=True)

            if to_ack:
                await redis.xack(POSITIONS_OPEN_STREAM, group, *to_ack)

    finally:
        stop_event.set()
        try:
            await router_task
        except Exception:
            pass