# indicator_position_stat.py ‚Äî –≤–æ—Ä–∫–µ—Ä on-demand —Å–Ω–∏–º–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ (m5 only; consumer-groups, watchdog, –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä)

import asyncio
import json
import logging
from datetime import datetime, timedelta

# üî∏ –í—Ä–µ–º—è –±–∞—Ä–∞ (floor –∫ –Ω–∞—á–∞–ª—É)
from packs.pack_utils import floor_to_bar

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å—Ç—Ä–∏–º–æ–≤ –∏ —Ç–∞–±–ª–∏—Ü
POSITIONS_OPEN_STREAM   = "positions_open_stream"
INDICATOR_REQ_STREAM    = "indicator_request"
INDICATOR_RESP_STREAM   = "indicator_response"
GW_REQ_STREAM           = "indicator_gateway_request"
GW_RESP_STREAM          = "indicator_gateway_response"
TARGET_TABLE            = "indicator_position_stat"

# üî∏ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ—Ä–∫–µ—Ä–∞ / –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
REQUIRED_TFS            = ("m5",)         # —Ç–æ–ª—å–∫–æ m5
POLL_INTERVAL_SEC       = 1               # —á–∞—Å—Ç–æ—Ç–∞ —Ä–µ—Ç—Ä–∞–µ–≤
RESP_BLOCK_MS           = 300             # –∫–æ—Ä–æ—Ç–∫–∏–π –±–ª–æ–∫ –Ω–∞ —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
GLOBAL_TIMEOUT_SEC      = 600             # 10 –º–∏–Ω—É—Ç –Ω–∞ –ø–æ–∑–∏—Ü–∏—é
BATCH_SIZE_POS_OPEN     = 20              # —á—Ç–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
BATCH_SIZE_RESP         = 200             # —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ (indicator/gateway)
CONCURRENCY_PER_M5      = 150             # –ª–∏–º–∏—Ç on-demand –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ m5
POSITIONS_CONCURRENCY   = 16              # –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–æ–∑–∏—Ü–∏–π
LOST_REQ_SEC            = 12              # watchdog: —á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ —Å—á–∏—Ç–∞–µ–º req –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–º

# üî∏ –ü–∞–∫–µ—Ç—ã –∏ MW (m5)
PACK_INDS = ("ema", "rsi", "mfi", "bb", "lr", "atr", "adx_dmi", "macd")
MW_KINDS  = ("trend", "volatility", "momentum", "extremes")

# üî∏ –ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏ –ø–æ–ª–µ–π –ø–∞–∫–æ–≤ (—Å—Ç—Ä–æ–≥–æ –∫–∞–∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–æ)
PACK_FIELD_WHITELIST = {
    "rsi":     ["bucket_low", "trend"],
    "mfi":     ["bucket_low", "trend"],
    "bb":      ["bucket", "bucket_delta", "bw_trend_strict", "bw_trend_smooth"],
    "lr":      ["bucket", "bucket_delta", "angle_trend"],
    "atr":     ["bucket", "bucket_delta"],
    "adx_dmi": ["adx_bucket_low", "adx_dynamic_strict", "adx_dynamic_smooth",
                "gap_bucket_low", "gap_dynamic_strict", "gap_dynamic_smooth"],
    "ema":     ["side", "dynamic", "dynamic_strict", "dynamic_smooth"],
    "macd":    ["mode", "cross", "zero_side", "hist_bucket_low_pct",
                "hist_trend_strict", "hist_trend_smooth"],
}

# üî∏ Consumer-groups –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞)
IND_RESP_GROUP      = "iv4_possnap_indresp"
GW_RESP_GROUP       = "iv4_possnap_gwresp"
IND_RESP_CONSUMER   = "iv4_possnap_router_ind_1"
GW_RESP_CONSUMER    = "iv4_possnap_router_gw_1"

# üî∏ –õ–æ–≥–≥–µ—Ä
log = logging.getLogger("IND_POS_STAT")


# üî∏ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è created_at ‚Üí open_time –±–∞—Ä–∞ (ms)
def to_bar_open_ms(created_at_iso: str, tf: str) -> int:
    dt = datetime.fromisoformat(created_at_iso)
    return floor_to_bar(int(dt.timestamp() * 1000), tf)

# üî∏ –ü–∞—Ä—Å ISO ‚Üí datetime
def parse_iso(s: str) -> datetime:
    return datetime.fromisoformat(s)

# üî∏ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (param_type='indicator')
def build_rows_for_indicator_response(position_uid: str,
                                      strategy_id: int,
                                      symbol: str,
                                      tf: str,
                                      indicator_name: str,
                                      open_time_iso: str,
                                      results_json: str) -> list[tuple]:
    rows = []
    try:
        results = json.loads(results_json)
    except Exception:
        results = {}
    if not isinstance(results, dict) or not results:
        return rows
    open_time = parse_iso(open_time_iso)
    for param_name, str_val in results.items():
        try:
            value_num = float(str_val)
        except Exception:
            continue
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "indicator", indicator_name, param_name,
            value_num, None,
            open_time,
            "ok", None
        ))
    return rows

# üî∏ –í—Å—Ç–∞–≤–∫–∞ –ø–∞—á–∫–∏ —Å—Ç—Ä–æ–∫ –≤ PG (UPSERT); –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç upsert_count
async def insert_rows_pg(pg, rows: list[tuple]) -> int:
    if not rows:
        return 0
    async with pg.acquire() as conn:
        async with conn.transaction():
            await conn.executemany(f"""
                INSERT INTO {TARGET_TABLE}
                (position_uid, strategy_id, symbol, timeframe, param_type, param_base, param_name,
                 value_num, value_text, open_time, status, error_code, captured_at)
                VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12, NOW())
                ON CONFLICT (position_uid, timeframe, param_type, param_base, param_name)
                DO UPDATE SET
                    value_num = EXCLUDED.value_num,
                    value_text = EXCLUDED.value_text,
                    open_time = EXCLUDED.open_time,
                    status = EXCLUDED.status,
                    error_code = EXCLUDED.error_code,
                    captured_at = NOW()
            """, rows)
    return len(rows)

# üî∏ –ü–æ–¥—Å—á—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –∏ TF
async def count_unique_rows(pg, position_uid: str, tf: str) -> int:
    async with pg.acquire() as conn:
        rec = await conn.fetchrow(
            f"SELECT COUNT(*) AS cnt FROM {TARGET_TABLE} WHERE position_uid = $1 AND timeframe = $2",
            position_uid, tf
        )
        return int(rec["cnt"]) if rec else 0

# üî∏ –ê–Ω—Ç–∏–¥—É–±–ª–∏: –ª–æ–∫–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á—É —Ç–∞–±–ª–∏—Ü—ã
def dedup_rows(rows: list[tuple]) -> list[tuple]:
    seen = set()
    out = []
    for r in rows:
        key = (r[0], r[3], r[4], r[5], r[6])  # position_uid,timeframe,param_type,param_base,param_name
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out

# üî∏ –¢–∏–ø –±–∞–∑—ã –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É (ema50 ‚Üí ema, bb20_2_0 ‚Üí bb, ...)
def base_kind(base: str) -> str | None:
    for k in PACK_FIELD_WHITELIST.keys():
        if base.startswith(k):
            return k
    return None

# üî∏ –ü–ª–æ—Å–∫–∏–π –æ–±—Ö–æ–¥ pack['pack'] (—Ñ–∏–ª—å—Ç—Ä –º–µ—Ç–∞-–ø–æ–ª–µ–π)
def flatten_pack_dict(d: dict):
    for k, v in d.items():
        if k in ("open_time", "ref", "used_bases", "prev_state", "raw_state",
                 "streak_preview", "strong", "direction", "max_adx", "deltas"):
            continue
        yield (k, v)

# üî∏ –°—Ç—Ä–æ–∫–∏ –¥–ª—è –ø–∞–∫–æ–≤ (param_type='pack') –ø–æ whitelist
def build_rows_for_pack_response(position_uid: str,
                                 strategy_id: int,
                                 symbol: str,
                                 tf: str,
                                 base: str,
                                 open_time_iso: str,
                                 pack_payload: dict) -> list[tuple]:
    rows = []
    kind = base_kind(base)
    if not kind:
        return rows
    allowed = set(PACK_FIELD_WHITELIST.get(kind, []))
    if not allowed:
        return rows
    open_time = parse_iso(open_time_iso)
    for pname, pval in flatten_pack_dict(pack_payload):
        if pname not in allowed:
            continue
        val_num = None
        val_text = None
        if isinstance(pval, (int, float)):
            val_num = float(pval)
        else:
            try:
                val_num = float(pval)
            except Exception:
                if pval is None:
                    continue
                if isinstance(pval, bool):
                    val_text = "true" if pval else "false"
                else:
                    val_text = str(pval)
        rows.append((
            position_uid, strategy_id, symbol, tf,
            "pack", base, pname,
            val_num, val_text,
            open_time,
            "ok", None
        ))
    return rows

# üî∏ –û–∂–∏–¥–∞–µ–º—ã–µ –±–∞–∑—ã –ø–∞–∫–æ–≤ –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ (–ø–æ m5)
def build_expected_pack_bases(instances: list[dict]) -> dict[str, set[str]]:
    expected: dict[str, set[str]] = {ind: set() for ind in PACK_INDS}
    for inst in instances:
        ind = inst["indicator"]
        params = inst["params"]
        if ind not in expected:
            continue
        if ind in ("ema", "rsi", "mfi", "atr", "adx_dmi", "lr"):
            try:
                L = int(params["length"])
                expected[ind].add(f"{ind}{L}")
            except Exception:
                pass
        elif ind == "macd":
            try:
                F = int(params["fast"])
                expected[ind].add(f"macd{F}")
            except Exception:
                pass
        elif ind == "bb":
            try:
                L = int(params["length"])
                S = round(float(params["std"]), 2)
                std_str = str(S).replace(".", "_")
                expected[ind].add(f"bb{L}_{std_str}")
            except Exception:
                pass
    return expected

# üî∏ –†–æ—É—Ç–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤ (consumer-groups): –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –≤ –æ—á–µ—Ä–µ–¥—å –ø–æ–∑–∏—Ü–∏–∏
async def run_response_router(redis, req_routes: dict, req_lock: asyncio.Lock,
                              stop_event: asyncio.Event):

    # —Å–æ–∑–¥–∞—Ç—å consumer-groups –¥–ª—è –æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å—Ç—Ä–∏–º–æ–≤
    try:
        await redis.xgroup_create(INDICATOR_RESP_STREAM, IND_RESP_GROUP, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create ind_resp error: {e}")
    try:
        await redis.xgroup_create(GW_RESP_STREAM, GW_RESP_GROUP, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create gw_resp error: {e}")

    while not stop_event.is_set():
        try:
            tasks = [
                redis.xreadgroup(IND_RESP_GROUP, IND_RESP_CONSUMER, streams={INDICATOR_RESP_STREAM: ">"}, count=BATCH_SIZE_RESP, block=RESP_BLOCK_MS),
                redis.xreadgroup(GW_RESP_GROUP,  GW_RESP_CONSUMER,  streams={GW_RESP_STREAM: ">"},         count=BATCH_SIZE_RESP, block=RESP_BLOCK_MS),
            ]
            res_ind, res_gw = await asyncio.gather(*tasks, return_exceptions=True)

            # –æ–±—Ä–∞–±–æ—Ç–∫–∞ INDICATOR_RESP_STREAM
            if isinstance(res_ind, list) and res_ind:
                to_ack = []
                for _, msgs in res_ind:
                    for msg_id, payload in msgs:
                        req_id = payload.get("req_id")
                        if not req_id:
                            to_ack.append(msg_id)
                            continue
                        queue = None
                        async with req_lock:
                            queue = req_routes.get(req_id)
                        if queue:
                            try:
                                await queue.put(("indicator", payload))
                                to_ack.append(msg_id)
                            except Exception:
                                pass
                        else:
                            to_ack.append(msg_id)
                if to_ack:
                    await redis.xack(INDICATOR_RESP_STREAM, IND_RESP_GROUP, *to_ack)

            # –æ–±—Ä–∞–±–æ—Ç–∫–∞ GW_RESP_STREAM
            if isinstance(res_gw, list) and res_gw:
                to_ack = []
                for _, msgs in res_gw:
                    for msg_id, payload in msgs:
                        req_id = payload.get("req_id")
                        if not req_id:
                            to_ack.append(msg_id)
                            continue
                        queue = None
                        async with req_lock:
                            queue = req_routes.get(req_id)
                        if queue:
                            try:
                                await queue.put(("gateway", payload))
                                to_ack.append(msg_id)
                            except Exception:
                                pass
                        else:
                            to_ack.append(msg_id)
                if to_ack:
                    await redis.xack(GW_RESP_STREAM, GW_RESP_GROUP, *to_ack)

        except Exception as e:
            log.error(f"router loop error: {e}", exc_info=True)
            await asyncio.sleep(0.2)

# üî∏ –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ–¥–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ m5; watchdog —É—Ç–µ—Ä—è–Ω–Ω—ã—Ö req)
async def handle_position_m5(pg, redis, get_instances_by_tf,
                             position_uid: str, strategy_id: int, symbol: str, created_at_iso: str,
                             req_routes: dict, req_lock: asyncio.Lock,
                             m5_semaphore: asyncio.Semaphore):

    instances = [i for i in get_instances_by_tf("m5")]
    expected_bases = build_expected_pack_bases(instances)
    bar_open_ms = to_bar_open_ms(created_at_iso, "m5")

    start_ts = datetime.utcnow()
    deadline = start_ts + timedelta(seconds=GLOBAL_TIMEOUT_SEC)

    # –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã per instance_id
    ind_ctx = {
        inst["id"]: {"inflight": False, "state": "pending", "last_err": None,
                     "req_ids": set(), "indicator": inst["indicator"], "sent_at": None}
        for inst in instances
    }

    # –ø–∞–∫–∏ per indicator type
    pack_ctx = {
        ind: {"inflight": False, "state": "pending", "last_err": None,
              "req_ids": set(), "done_bases": set(),
              "expected_bases": set(expected_bases.get(ind, set())), "sent_at": None}
        for ind in PACK_INDS if expected_bases.get(ind)
    }

    # marketwatch
    mw_ctx = {
        kind: {"inflight": False, "state": "pending", "last_err": None,
               "req_ids": set(), "sent_at": None}
        for kind in MW_KINDS
    }

    # –æ—á–µ—Ä–µ–¥—å –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏ –∏ –Ω–∞–±–æ—Ä req_id –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—á–∏—Å—Ç–∫–∏
    resp_queue: asyncio.Queue = asyncio.Queue()
    all_req_ids: set[str] = set()

    # —Ä–µ—Ç—Ä–∞–∏–±–µ–ª—å–Ω–æ—Å—Ç—å
    def is_retriable(err: str) -> bool:
        return err not in ("instance_not_active", "exception")

    # –æ—Ç–ø—Ä–∞–≤–∫–∞ indicator_request
    async def send_indicator(inst_id: int):
        s = ind_ctx[inst_id]
        if s["inflight"] or s["state"] != "pending":
            return
        async with m5_semaphore:
            rid = await redis.xadd(INDICATOR_REQ_STREAM, {
                "symbol": symbol,
                "timeframe": "m5",
                "instance_id": str(inst_id),
                "timestamp_ms": str(bar_open_ms)
            })
        async with req_lock:
            req_routes[rid] = resp_queue
        s["inflight"] = True
        s["sent_at"] = datetime.utcnow()
        s["req_ids"].add(rid)
        all_req_ids.add(rid)

    # –æ—Ç–ø—Ä–∞–≤–∫–∞ gateway_request (pack –∏–ª–∏ mw)
    async def send_gateway(kind_or_pack: str, is_pack: bool):
        s = pack_ctx[kind_or_pack] if is_pack else mw_ctx[kind_or_pack]
        ok_states = ("pending", "ok_part") if is_pack else ("pending",)
        if s["inflight"] or s["state"] not in ok_states:
            return
        async with m5_semaphore:
            rid = await redis.xadd(GW_REQ_STREAM, {
                "symbol": symbol,
                "timeframe": "m5",
                "indicator": kind_or_pack,          # –∏–º—è –ø–∞–∫–∞ –∏–ª–∏ kind MW
                "timestamp_ms": str(bar_open_ms)
            })
        async with req_lock:
            req_routes[rid] = resp_queue
        s["inflight"] = True
        s["sent_at"] = datetime.utcnow()
        s["req_ids"].add(rid)
        all_req_ids.add(rid)

    # –ø–µ—Ä–≤–∞—è –≤–æ–ª–Ω–∞: –≤—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –≤—Å–µ –ø–∞–∫–∏, –≤—Å–µ MW
    await asyncio.gather(*[send_indicator(inst["id"]) for inst in instances])
    await asyncio.gather(*[send_gateway(ind, True) for ind in pack_ctx.keys()])
    await asyncio.gather(*[send_gateway(kind, False) for kind in MW_KINDS])

    total_upserts = 0

    # –≥–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª
    while True:
        now = datetime.utcnow()
        if now >= deadline:
            # –ª–æ–≥ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö
            for inst_id, s in ind_ctx.items():
                if s["state"] == "pending":
                    log.info(f"[TIMEOUT] IND {symbol}/m5 inst_id={inst_id} {s['indicator']} last_err={s['last_err']}")
            for ind, s in pack_ctx.items():
                if s["state"] in ("pending", "ok_part"):
                    missing = sorted(list(s["expected_bases"] - s["done_bases"]))
                    log.info(f"[TIMEOUT] PACK {symbol}/m5 {ind} missing_bases={missing} last_err={s['last_err']}")
            for kind, s in mw_ctx.items():
                if s["state"] == "pending":
                    log.info(f"[TIMEOUT] MW {symbol}/m5 {kind} last_err={s['last_err']}")
            break

        # –æ–∫–Ω–æ —Å–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤
        end_wait = now + timedelta(seconds=POLL_INTERVAL_SEC)
        collected_rows: list[tuple] = []

        while datetime.utcnow() < end_wait:
            try:
                src, payload = await asyncio.wait_for(resp_queue.get(), timeout=RESP_BLOCK_MS / 1000.0)
            except asyncio.TimeoutError:
                break

            status = payload.get("status")
            req_id = payload.get("req_id")
            r_symbol = payload.get("symbol")
            tf = payload.get("timeframe")
            if r_symbol != symbol or tf != "m5":
                continue

            # —É–±—Ä–∞—Ç—å req_id –∏–∑ –º–∞—Ä—à—Ä—É—Ç–æ–≤ (–±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω)
            if req_id:
                async with req_lock:
                    req_routes.pop(req_id, None)

            if src == "indicator":
                instance_id_raw = payload.get("instance_id")
                if not instance_id_raw:
                    continue
                iid = int(instance_id_raw)
                s = ind_ctx.get(iid)
                if not s or req_id not in s["req_ids"]:
                    continue
                s["req_ids"].discard(req_id)
                s["inflight"] = False

                if status == "ok":
                    rows = build_rows_for_indicator_response(
                        position_uid=position_uid,
                        strategy_id=strategy_id,
                        symbol=symbol,
                        tf="m5",
                        indicator_name=s["indicator"],
                        open_time_iso=payload.get("open_time"),
                        results_json=payload.get("results", "{}"),
                    )
                    collected_rows.extend(rows)
                    s["state"] = "ok"
                    s["last_err"] = None
                else:
                    err = payload.get("error") or "unknown"
                    s["last_err"] = err
                    if not is_retriable(err):
                        s["state"] = "error"

            else:  # gateway
                kind = payload.get("indicator")
                # —ç—Ç–æ pack –∏–ª–∏ MW?
                if kind in pack_ctx:
                    ctx_slot = pack_ctx[kind]
                    is_pack = True
                elif kind in mw_ctx:
                    ctx_slot = mw_ctx[kind]
                    is_pack = False
                else:
                    continue

                if req_id not in ctx_slot["req_ids"]:
                    continue
                ctx_slot["req_ids"].discard(req_id)
                ctx_slot["inflight"] = False

                if status == "ok":
                    try:
                        results = json.loads(payload.get("results", "[]"))
                    except Exception:
                        results = []

                    if is_pack:
                        if not isinstance(results, list):
                            ctx_slot["last_err"] = "bad_results"
                        else:
                            for item in results:
                                base = item.get("base")
                                p = item.get("pack", {})
                                if not base or not isinstance(p, dict):
                                    continue
                                open_time_iso = p.get("open_time") or datetime.utcfromtimestamp(bar_open_ms / 1000).isoformat()
                                rows = build_rows_for_pack_response(position_uid, strategy_id, symbol, "m5", base, open_time_iso, p)
                                if rows:
                                    collected_rows.extend(rows)
                                    ctx_slot["done_bases"].add(base)
                            ctx_slot["state"] = "ok" if (ctx_slot["expected_bases"] <= ctx_slot["done_bases"]) else "ok_part"
                            ctx_slot["last_err"] = None
                    else:
                        if not isinstance(results, list) or not results:
                            ctx_slot["last_err"] = "bad_results"
                        else:
                            item = results[0]
                            base = item.get("base", kind)  # trend|volatility|momentum|extremes
                            p = item.get("pack", {})
                            state_val = p.get("state")
                            if state_val is not None:
                                open_time_iso = p.get("open_time") or datetime.utcfromtimestamp(bar_open_ms / 1000).isoformat()
                                rows = [(
                                    position_uid, strategy_id, symbol, "m5",
                                    "marketwatch", base, "state",
                                    None, str(state_val),
                                    parse_iso(open_time_iso),
                                    "ok", None
                                )]
                                collected_rows.extend(rows)
                                ctx_slot["state"] = "ok"
                                ctx_slot["last_err"] = None
                            else:
                                ctx_slot["last_err"] = "no_state"
                else:
                    err = payload.get("error") or "unknown"
                    ctx_slot["last_err"] = err
                    if not is_retriable(err):
                        ctx_slot["state"] = "error"

        # –∑–∞–ø–∏—Å—å –≤ –ë–î
        if collected_rows:
            deduped = dedup_rows(collected_rows)
            n = await insert_rows_pg(pg, deduped)
            total_upserts += n

        # üî∏ watchdog —É—Ç–µ—Ä—è–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—á–µ—Ä–µ–∑ LOST_REQ_SEC —Å–Ω–∏–º–∞–µ–º inflight –∏ –ø–µ—Ä–µ–∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º)
        now = datetime.utcnow()
        for iid, s in ind_ctx.items():
            if s["state"] == "pending" and s["inflight"] and s["sent_at"] and (now - s["sent_at"]).total_seconds() > LOST_REQ_SEC:
                async with req_lock:
                    for rid in list(s["req_ids"]):
                        req_routes.pop(rid, None)
                s["req_ids"].clear()
                s["inflight"] = False
                s["sent_at"] = None
        for name, s in pack_ctx.items():
            if s["state"] in ("pending", "ok_part") and s["inflight"] and s["sent_at"] and (now - s["sent_at"]).total_seconds() > LOST_REQ_SEC:
                async with req_lock:
                    for rid in list(s["req_ids"]):
                        req_routes.pop(rid, None)
                s["req_ids"].clear()
                s["inflight"] = False
                s["sent_at"] = None
        for kind, s in mw_ctx.items():
            if s["state"] == "pending" and s["inflight"] and s["sent_at"] and (now - s["sent_at"]).total_seconds() > LOST_REQ_SEC:
                async with req_lock:
                    for rid in list(s["req_ids"]):
                        req_routes.pop(rid, None)
                s["req_ids"].clear()
                s["inflight"] = False
                s["sent_at"] = None

        # —Ä–µ—Ç—Ä–∞–∏ ¬´—Ä–∞–∑ –≤ —Å–µ–∫—É–Ω–¥—É¬ª
        for iid, s in ind_ctx.items():
            if s["state"] == "pending" and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                await send_indicator(iid)
        for ind, s in pack_ctx.items():
            if s["state"] in ("pending", "ok_part") and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                await send_gateway(ind, True)
        for kind, s in mw_ctx.items():
            if s["state"] == "pending" and not s["inflight"] and (s["last_err"] is None or is_retriable(s["last_err"])):
                await send_gateway(kind, False)

        # –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å m5?
        def all_done_m5() -> bool:
            if any(s["state"] in ("pending", "error") for s in ind_ctx.values()):
                return False
            for s in pack_ctx.values():
                if s["state"] == "error":
                    return False
                if not (s["expected_bases"] <= s["done_bases"]):
                    return False
            if any(s["state"] in ("pending", "error") for s in mw_ctx.values()):
                return False
            return True

        if all_done_m5():
            elapsed_ms = int((datetime.utcnow() - start_ts).total_seconds() * 1000)
            ok_inst = sum(1 for s in ind_ctx.values() if s["state"] == "ok")
            ok_packs = sum(len(s["done_bases"]) for s in pack_ctx.values())
            ok_mw = sum(1 for s in mw_ctx.values() if s["state"] == "ok")
            unique_rows_tf = await count_unique_rows(pg, position_uid, "m5")
            log.info(
                f"IND_POS_STAT: position={position_uid} {symbol} m5 snapshot complete: "
                f"ok_instances={ok_inst}, ok_packs={ok_packs}, ok_mw={ok_mw}, "
                f"rows_upserted_tf={total_upserts}, unique_rows_tf={unique_rows_tf}, elapsed_ms={elapsed_ms}"
            )
            break

    # –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö req_id —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –º–∞—Ä—à—Ä—É—Ç–æ–≤ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    async with req_lock:
        for rid in list(all_req_ids):
            req_routes.pop(rid, None)

# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä: –¥–∏—Å–ø–µ—Ç—á–µ—Ä –ø–æ–∑–∏—Ü–∏–π + –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤ (m5 only; –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π)
async def run_indicator_position_stat(pg, redis, get_instances_by_tf, get_precision):
    group = "iv4_possnap_group"
    consumer = "iv4_possnap_1"

    # —Å–æ–∑–¥–∞—Ç—å consumer-group –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∞ –ø–æ–∑–∏—Ü–∏–π
    try:
        await redis.xgroup_create(POSITIONS_OPEN_STREAM, group, id="$", mkstream=True)
    except Exception as e:
        if "BUSYGROUP" not in str(e):
            log.warning(f"xgroup_create pos_open error: {e}")

    # —Ä–æ—É—Ç–∏–Ω–≥ req_id ‚Üí –æ—á–µ—Ä–µ–¥—å –ø–æ–∑–∏—Ü–∏–∏
    req_routes: dict[str, asyncio.Queue] = {}
    req_lock = asyncio.Lock()

    # –ª–∏–º–∏—Ç–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤ m5
    pos_sema = asyncio.Semaphore(POSITIONS_CONCURRENCY)
    m5_semaphore = asyncio.Semaphore(CONCURRENCY_PER_M5)

    # –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤ (consumer-groups)
    stop_event = asyncio.Event()
    router_task = asyncio.create_task(run_response_router(redis, req_routes, req_lock, stop_event))

    # –ø—É–ª –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–∑–∏—Ü–∏–π (–¥–ª—è —É–±–æ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö)
    active_tasks: set[asyncio.Task] = set()

    try:
        while True:
            try:
                resp = await redis.xreadgroup(
                    groupname=group,
                    consumername=consumer,
                    streams={POSITIONS_OPEN_STREAM: ">"},
                    count=BATCH_SIZE_POS_OPEN,
                    block=2000
                )
            except Exception as e:
                log.error(f"positions read error: {e}", exc_info=True)
                await asyncio.sleep(0.5)
                continue

            if not resp:
                # —É–±–æ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ–±—ã –ø—É–ª –Ω–µ —Ä–æ—Å
                for t in list(active_tasks):
                    if t.done():
                        active_tasks.discard(t)
                continue

            to_ack = []

            for _, messages in resp:
                for msg_id, data in messages:
                    try:
                        if (data.get("event_type") or "").lower() != "opened":
                            to_ack.append(msg_id)
                            continue

                        position_uid = data["position_uid"]
                        strategy_id = int(data["strategy_id"])
                        symbol = data["symbol"]
                        created_at_iso = data.get("created_at") or data.get("received_at")
                        if not created_at_iso:
                            log.info(f"[SKIP] position {position_uid}: no created_at/received_at")
                            to_ack.append(msg_id)
                            continue

                        # –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–¥–∞—á—É (–Ω–µ –∂–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–µ)
                        async def run_one():
                            async with pos_sema:
                                await handle_position_m5(pg, redis, get_instances_by_tf,
                                                         position_uid, strategy_id, symbol, created_at_iso,
                                                         req_routes, req_lock, m5_semaphore)

                        task = asyncio.create_task(run_one())
                        active_tasks.add(task)
                        # –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä: ACK –°–†–ê–ó–£ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–ø–∞—É–Ω–∞ –∑–∞–¥–∞—á–∏
                        to_ack.append(msg_id)

                    except Exception as e:
                        log.error(f"position spawn error: {e}", exc_info=True)
                        # –Ω–µ ACK ‚Äî –ø–æ–≤—Ç–æ—Ä–∏–º –ø–æ–∑–∂–µ

            if to_ack:
                await redis.xack(POSITIONS_OPEN_STREAM, group, *to_ack)

            # —É–±–æ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
            for t in list(active_tasks):
                if t.done():
                    active_tasks.discard(t)

    finally:
        stop_event.set()
        try:
            await router_task
        except Exception:
            pass