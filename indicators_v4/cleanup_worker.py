# cleanup_worker.py ‚Äî —Ä–µ–≥—É–ª—è—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ TS/DB/Streams –¥–ª—è indicators_v4

import asyncio
import logging
from datetime import datetime, timedelta

log = logging.getLogger("IND_CLEANUP")

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ–ª–∏—Ç–∏–∫–∏
TS_RETENTION_MS = 14 * 24 * 60 * 60 * 1000             # 14 —Å—É—Ç–æ–∫
DB_KEEP_DAYS    = 30                                   # 30 —Å—É—Ç–æ–∫
STREAM_LIMITS = {
    "indicator_stream_core": 10000,
    "indicator_stream":      10000,
    "iv4_inserted":          20000,
    "indicator_request":     10000,
    "indicator_response":    10000,
}

# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—á–∏—Å—Ç–∫–∏ positions_indicators_stat
PIS_BATCH_SIZE        = 1_000   # —Å–∫–æ–ª—å–∫–æ ID –≤—ã–±–∏—Ä–∞–µ–º –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
PIS_DELETE_CHUNK_SIZE = 100    # —Å–∫–æ–ª—å–∫–æ ID —É–¥–∞–ª—è–µ–º –≤ –æ–¥–Ω–æ–º SQL-–∑–∞–ø—Ä–æ—Å–µ
PIS_CONCURRENCY       = 10       # –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á —É–¥–∞–ª–µ–Ω–∏—è
PIS_FIRST_RUN_DELAY   = timedelta(minutes=2)           # –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 2 –º–∏–Ω—É—Ç—ã
PIS_RUN_PERIOD        = timedelta(days=1)              # –∑–∞—Ç–µ–º —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏

# üîπ –¢–∞–π–º-–±—é–¥–∂–µ—Ç –Ω–∞ —Ä–µ—Ç–µ–Ω—Ü–∏—é TS –≤ –æ–¥–Ω–æ–º —Ü–∏–∫–ª–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –≤–æ—Ä–∫–µ—Ä –Ω–∞–¥–æ–ª–≥–æ
TS_RETENTION_TIME_BUDGET_SEC = 30

# üî∏ –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ ts_ind:* –∏ –≤—ã—Å—Ç–∞–≤–∏—Ç—å RETENTION=14 —Å—É—Ç–æ–∫ (—Å —Ç–∞–π–º-–±—é–¥–∂–µ—Ç–æ–º)
async def enforce_ts_retention(redis):
    """
    –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ RETENTION –Ω–∞ ts_ind:* —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–¥–∏–Ω —Ü–∏–∫–ª.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º SCAN —Å count=500; –µ—Å–ª–∏ –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ –≤—Ä–µ–º—è –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª–∏ ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.
    """
    try:
        start = datetime.utcnow()
        cursor = "0"
        pattern = "ts_ind:*"
        changed = 0

        while True:
            cursor, keys = await redis.scan(cursor=cursor, match=pattern, count=500)
            for k in keys:
                try:
                    await redis.execute_command("TS.ALTER", k, "RETENTION", TS_RETENTION_MS)
                    changed += 1
                except Exception as e:
                    log.warning(f"TS.ALTER {k} error: {e}")

            # –µ—Å–ª–∏ –ø—Ä–æ—à–ª–∏ –≤—Å–µ –∫–ª—é—á–∏ ‚Äî –≤—ã—Ö–æ–¥–∏–º
            if cursor == "0":
                break

            # –ø—Ä–æ–≤–µ—Ä—è–µ–º –±—é–¥–∂–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
            if (datetime.utcnow() - start).total_seconds() >= TS_RETENTION_TIME_BUDGET_SEC:
                log.debug(f"[TS] RETENTION pass time-budget reached, changed ~{changed}, will continue next loop")
                break

        if cursor == "0":
            log.debug(f"[TS] RETENTION=14d –ø—Ä–∏–º–µ–Ω—ë–Ω (–ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥), –∏–∑–º–µ–Ω–µ–Ω–æ ~{changed} –∫–ª—é—á–µ–π ts_ind:*")
    except Exception as e:
        log.error(f"[TS] enforce_ts_retention error: {e}", exc_info=True)

# üî∏ –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏–∑ –ë–î (open_time < NOW()-30d)
async def cleanup_db(pg):
    try:
        async with pg.acquire() as conn:
            cutoff = DB_KEEP_DAYS
            res1 = await conn.execute(
                f"""
                DELETE FROM indicator_values_v4
                WHERE open_time < NOW() - INTERVAL '{cutoff} days'
                """
            )
        log.debug(f"[DB] indicator_values_v4 —É–¥–∞–ª–µ–Ω–æ: {res1}")
    except Exception as e:
        log.error(f"[DB] cleanup_db error: {e}", exc_info=True)

# üîπ –í—ã–±–æ—Ä–∫–∞ –±–∞—Ç—á–∞ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ positions_indicators_stat
async def _fetch_pis_batch_ids(pg, limit: int) -> list[int]:
    sql = """
        SELECT pis.id
        FROM positions_indicators_stat pis
        JOIN strategies_v4 s ON s.id = pis.strategy_id
        WHERE COALESCE(s.market_watcher, false) = false
        LIMIT $1
    """
    async with pg.acquire() as conn:
        rows = await conn.fetch(sql, limit)
    return [r["id"] for r in rows]

# üîπ –£–¥–∞–ª–µ–Ω–∏–µ –ø–∞—á–∫–∏ ID (–æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º)
async def _delete_pis_ids_chunk(pg, ids: list[int]):
    if not ids:
        return
    sql = "DELETE FROM positions_indicators_stat WHERE id = ANY($1::bigint[])"
    async with pg.acquire() as conn:
        await conn.execute(sql, ids)

# üîπ –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ PIS –±–∞—Ç—á–∞–º–∏ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
async def cleanup_positions_indicators_stat(pg,
                                            batch_size: int = PIS_BATCH_SIZE,
                                            chunk_size: int = PIS_DELETE_CHUNK_SIZE,
                                            concurrency: int = PIS_CONCURRENCY):
    try:
        total_deleted = 0
        sem = asyncio.Semaphore(concurrency)

        while True:
            ids = await _fetch_pis_batch_ids(pg, batch_size)
            if not ids:
                break

            tasks = []
            for i in range(0, len(ids), chunk_size):
                chunk = ids[i:i+chunk_size]

                async def _task(c=chunk):
                    async with sem:
                        await _delete_pis_ids_chunk(pg, c)

                tasks.append(asyncio.create_task(_task()))

            await asyncio.gather(*tasks, return_exceptions=False)
            total_deleted += len(ids)
            log.info(f"[DB] PIS cleanup progress: deleted {len(ids)} this batch (total {total_deleted})")

        if total_deleted:
            log.info(f"[DB] PIS cleanup removed rows: {total_deleted}")
        else:
            log.info("[DB] PIS cleanup: nothing to delete")

    except Exception as e:
        log.error(f"[DB] cleanup_positions_indicators_stat error: {e}", exc_info=True)

# üî∏ –¢—Ä–∏–º –≤—Å–µ—Ö —Å—Ç—Ä–∏–º–æ–≤ –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ö–≤–æ—Å—Ç–∞
async def trim_streams(redis):
    for key, maxlen in STREAM_LIMITS.items():
        try:
            trimmed = await redis.execute_command("XTRIM", key, "MAXLEN", "~", maxlen)
            log.debug(f"[STREAM] {key} ‚Üí XTRIM ~{maxlen}, —É–¥–∞–ª–µ–Ω–æ ~{trimmed}")
        except Exception as e:
            log.warning(f"[STREAM] {key} XTRIM error: {e}")

# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä: –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏
async def run_indicators_cleanup(pg, redis):
    log.info("IND_CLEANUP started")
    last_db = datetime.min

    # —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ PIS ‚Äî –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 2 –º–∏–Ω—É—Ç—ã, –¥–∞–ª–µ–µ —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏
    now = datetime.utcnow()
    next_pis_run_at = now + PIS_FIRST_RUN_DELAY
    log.info(f"[DB] PIS cleanup scheduled at (UTC): {next_pis_run_at.isoformat()}")

    while True:
        try:
            # 1) —Å–Ω–∞—á–∞–ª–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–ø—É—Å–∫ PIS –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (—á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–æ—Å—å –¥–æ–ª–≥–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏)
            now = datetime.utcnow()
            if now >= next_pis_run_at:
                log.info("[DB] PIS cleanup: start")
                await cleanup_positions_indicators_stat(pg)
                log.info("[DB] PIS cleanup: done")
                next_pis_run_at = now + PIS_RUN_PERIOD
                log.info(f"[DB] PIS next run at (UTC): {next_pis_run_at.isoformat()}")

            # 2) –∑–∞—Ç–µ–º ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ Redis
            await enforce_ts_retention(redis)
            await trim_streams(redis)

            # 3) —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏ ‚Äî —á–∏—Å—Ç–∫–∞ indicator_values_v4
            now = datetime.utcnow()
            if (now - last_db) >= timedelta(days=1):
                await cleanup_db(pg)
                last_db = now

            # 4) –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Å–æ–Ω: –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ–±—ã—Ç–∏—è, –Ω–æ –Ω–µ –±–æ–ª—å—à–µ 300 —Å–µ–∫
            now = datetime.utcnow()
            sleep_sec = min(300, max(1, int((next_pis_run_at - now).total_seconds())))
            await asyncio.sleep(sleep_sec)

        except Exception as e:
            log.error(f"IND_CLEANUP loop error: {e}", exc_info=True)
            await asyncio.sleep(10)