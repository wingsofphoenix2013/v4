# ðŸ”¸ indicators_market_watcher.py â€” Ð­Ñ‚Ð°Ð¿ 2: Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ñ‡ Ð¸Ð· Redis TS Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ‚Ð½Ð¾ÑÑ‚Ð¸

import os
import asyncio
import json
import logging
from datetime import datetime

# ðŸ”¸ ÐšÐ¾Ð½ÑÑ‚Ð°Ð½Ñ‚Ñ‹ Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð³
READY_STREAM = "indicator_stream"
GROUP_NAME = os.getenv("MRW_GROUP", "mrw_v1_group")
CONSUMER_NAME = os.getenv("MRW_CONSUMER", "mrw_1")
REQUIRED_TFS = {"m5", "m15", "h1"}

DEBOUNCE_MS = int(os.getenv("MRW_DEBOUNCE_MS", "250"))
MAX_CONCURRENCY = int(os.getenv("MRW_MAX_CONCURRENCY", "64"))
MAX_PER_SYMBOL = int(os.getenv("MRW_MAX_PER_SYMBOL", "4"))
XREAD_BLOCK_MS = int(os.getenv("MRW_BLOCK_MS", "1000"))
XREAD_COUNT = int(os.getenv("MRW_COUNT", "50"))

N_PCT = int(os.getenv("MRW_N_PCT", "200"))   # Ð¾ÐºÐ½Ð¾ Ð´Ð»Ñ p30/p70
N_ACC = int(os.getenv("MRW_N_ACC", "50"))    # Ð¾ÐºÐ½Ð¾ Ð´Ð»Ñ z-score Î”hist

# ðŸ”¸ Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð°
task_gate = asyncio.Semaphore(MAX_CONCURRENCY)
symbol_semaphores: dict[str, asyncio.Semaphore] = {}
bucket_tasks: dict[tuple, asyncio.Task] = {}


# ðŸ”¸ Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹
def _iso_to_ms(iso_str: str) -> int:
    dt = datetime.fromisoformat(iso_str)
    return int(dt.timestamp() * 1000)

def _tf_step_ms(tf: str) -> int:
    return 300_000 if tf == "m5" else (900_000 if tf == "m15" else 3_600_000)


# ðŸ”¸ Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ñ‡ Ð½Ð° Ð±Ð°Ñ€: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ Ð³Ð¾Ñ‚Ð¾Ð²Ð¾ Ð½Ð° open_time, Ð¸ Ð¿Ð¾Ð´Ñ‚ÑÐ³Ð¸Ð²Ð°ÐµÐ¼ Ð¾ÐºÐ½Ð°
async def fetch_features_for_bar(redis, symbol: str, tf: str, open_time_ms: int) -> dict | None:
    log = logging.getLogger("MRW")

    adx_key = f"ts_ind:{symbol}:{tf}:adx_dmi14_adx" if tf in {"m5", "m15"} else f"ts_ind:{symbol}:{tf}:adx_dmi28_adx"
    ema_key = f"ts_ind:{symbol}:{tf}:ema21"
    macd_key = f"ts_ind:{symbol}:{tf}:macd12_macd_hist"
    bb_u = f"ts_ind:{symbol}:{tf}:bb20_2_0_upper"
    bb_l = f"ts_ind:{symbol}:{tf}:bb20_2_0_lower"
    bb_c = f"ts_ind:{symbol}:{tf}:bb20_2_0_center"
    atr_key = f"ts_ind:{symbol}:{tf}:atr14" if tf in {"m5", "m15"} else None

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°: Ð½Ð° open_time Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ñƒ Ð²ÑÐµÑ… ÐºÐ»ÑŽÑ‡ÐµÐ¹
    keys_now = [adx_key, ema_key, macd_key, bb_u, bb_l, bb_c] + ([atr_key] if atr_key else [])
    now_calls = [redis.execute_command("TS.RANGE", k, open_time_ms, open_time_ms) for k in keys_now]
    now_results = await asyncio.gather(*now_calls, return_exceptions=True)

    for k, r in zip(keys_now, now_results):
        if isinstance(r, Exception) or not r or int(r[0][0]) != open_time_ms:
            log.info(f"[INCOMPLETE] {symbol}/{tf} @ {open_time_ms} â†’ missing {k}")
            return None

    # ÐžÐºÐ½Ð° Ð´Ð»Ñ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐ¸Ñ… ÑÑ‚Ð°Ð¿Ð¾Ð² (Ð¿Ð¾ÐºÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð¸ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹)
    step_ms = _tf_step_ms(tf)
    t_prev = open_time_ms - step_ms
    t_start_pct = open_time_ms - (N_PCT - 1) * step_ms
    t_start_acc = open_time_ms - N_ACC * step_ms

    window_calls = [
        redis.execute_command("TS.RANGE", ema_key, t_prev, open_time_ms),        # 2 Ñ‚Ð¾Ñ‡ÐºÐ¸
        redis.execute_command("TS.RANGE", macd_key, t_start_acc, open_time_ms),  # N_ACC+1
        redis.execute_command("TS.RANGE", adx_key, t_start_pct, open_time_ms),   # N_PCT
        redis.execute_command("TS.RANGE", bb_u,   t_start_pct, open_time_ms),
        redis.execute_command("TS.RANGE", bb_l,   t_start_pct, open_time_ms),
        redis.execute_command("TS.RANGE", bb_c,   t_start_pct, open_time_ms),
    ]
    if atr_key:
        window_calls.append(redis.execute_command("TS.RANGE", atr_key, t_start_pct, open_time_ms))

    out = await asyncio.gather(*window_calls, return_exceptions=True)

    def _vals(series):
        return [float(v) for _, v in series] if series and not isinstance(series, Exception) else []

    ema_vals  = _vals(out[0])
    macd_vals = _vals(out[1])
    adx_vals  = _vals(out[2])
    bbu_vals  = _vals(out[3])
    bbl_vals  = _vals(out[4])
    bbc_vals  = _vals(out[5])
    atr_vals  = _vals(out[6]) if (atr_key and len(out) > 6) else None

    logging.getLogger("MRW").info(
        f"[FEATURES] {symbol}/{tf} @ {open_time_ms} â†’ "
        f"ema={len(ema_vals)} macd={len(macd_vals)} adx={len(adx_vals)} "
        f"bb(u/l/c)={[len(bbu_vals), len(bbl_vals), len(bbc_vals)]} "
        f"atr={'-' if atr_vals is None else len(atr_vals)}"
    )

    return {
        "ema_vals": ema_vals,
        "macd_vals": macd_vals,
        "adx_vals": adx_vals,
        "bb_u": bbu_vals,
        "bb_l": bbl_vals,
        "bb_c": bbc_vals,
        "atr_vals": atr_vals,
    }


# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð±Ð°ÐºÐµÑ‚Ð° (Ð­Ñ‚Ð°Ð¿ 2): debounce â†’ 2 Ñ€ÐµÑ‚Ñ€Ð°Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ñ„Ð¸Ñ‡ â†’ Ð»Ð¾Ð³
async def handle_bucket(symbol: str, tf: str, open_time_ms: int, redis):
    log = logging.getLogger("MRW")
    await asyncio.sleep(DEBOUNCE_MS / 1000)

    for attempt in range(2):
        feats = await fetch_features_for_bar(redis, symbol, tf, open_time_ms)
        if feats is not None:
            log.info(f"[OK] features ready: {symbol}/{tf} @ {open_time_ms}")
            return
        await asyncio.sleep(0.15)

    log.info(f"[SKIP] features incomplete: {symbol}/{tf} @ {open_time_ms}")


# ðŸ”¸ ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð° (Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐ°Ð½ Ð¿Ð¾Ð´ XREADGROUP) â€” Ð­Ñ‚Ð°Ð¿ 1 Ð»Ð¾Ð³Ð¸ Ð¿ÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ñ‹ Ð² debug
async def run_market_watcher(pg, redis):
    log = logging.getLogger("MRW")
    log.info(f"market_watcher starting: XGROUP init")

    try:
        await redis.xgroup_create(READY_STREAM, GROUP_NAME, id="$", mkstream=True)
        log.debug(f"consumer group '{GROUP_NAME}' created on '{READY_STREAM}'")
    except Exception as e:
        if "BUSYGROUP" in str(e):
            log.debug(f"consumer group '{GROUP_NAME}' already exists")
        else:
            log.error(f"XGROUP CREATE error: {e}", exc_info=True)

    while True:
        try:
            resp = await redis.xreadgroup(
                groupname=GROUP_NAME,
                consumername=CONSUMER_NAME,
                streams={READY_STREAM: ">"},
                count=XREAD_COUNT,
                block=XREAD_BLOCK_MS
            )
            if not resp:
                continue

            to_ack = []
            for _, messages in resp:
                for msg_id, data in messages:
                    try:
                        symbol = data.get("symbol")
                        tf = data.get("timeframe") or data.get("interval")
                        status = data.get("status")
                        open_time_iso = data.get("open_time")

                        if not symbol or tf not in REQUIRED_TFS or status != "ready" or not open_time_iso:
                            to_ack.append(msg_id)
                            continue

                        open_time_ms = _iso_to_ms(open_time_iso)
                        bucket = (symbol, tf, open_time_ms)

                        if bucket in bucket_tasks and not bucket_tasks[bucket].done():
                            to_ack.append(msg_id)
                            continue

                        if symbol not in symbol_semaphores:
                            symbol_semaphores[symbol] = asyncio.Semaphore(MAX_PER_SYMBOL)

                        log.debug(f"[READY] {symbol}/{tf} @ {open_time_iso} â†’ schedule bucket")

                        async def bucket_runner():
                            async with task_gate:
                                async with symbol_semaphores[symbol]:
                                    await handle_bucket(symbol, tf, open_time_ms, redis)

                        bucket_tasks[bucket] = asyncio.create_task(bucket_runner())
                        to_ack.append(msg_id)

                    except Exception as parse_err:
                        to_ack.append(msg_id)
                        log.error(f"message parse error: {parse_err}", exc_info=True)

            if to_ack:
                await redis.xack(READY_STREAM, GROUP_NAME, *to_ack)

        except Exception as e:
            log.error(f"XREADGROUP loop error: {e}", exc_info=True)
            await asyncio.sleep(1)