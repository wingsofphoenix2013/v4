# packs_config/db_loaders.py ‚Äî –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ind_pack –∏–∑ PostgreSQL (packs/meta/params/rules + winners from labels_v2)

from __future__ import annotations

# üî∏ Imports
import json
import logging
from typing import Any

from packs_config.models import BinRule, LabelsContext

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ë–î (—Ç–∞–±–ª–∏—Ü—ã)
PACK_INSTANCES_TABLE = "indicator_pack_instances_v4"
ANALYSIS_INSTANCES_TABLE = "bt_analysis_instances"
ANALYSIS_PARAMETERS_TABLE = "bt_analysis_parameters"
BINS_DICT_TABLE = "bt_analysis_bins_dict"
ADAPTIVE_BINS_TABLE = "bt_analysis_bin_dict_adaptive"
BINS_LABELS_TABLE = "bt_analysis_bins_labels_v2"
RUNS_TABLE = "bt_signal_backfill_runs"


# üî∏ DB loaders: packs / analyzers / params / rules
async def load_enabled_packs(pg: Any) -> list[dict[str, Any]]:
    log = logging.getLogger("PACK_INIT")
    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT id, analysis_id, bins_policy, enabled_at
            FROM {PACK_INSTANCES_TABLE}
            WHERE enabled = true
            """
        )

    packs: list[dict[str, Any]] = []
    parsed = 0

    for r in rows:
        policy = r["bins_policy"]

        # bins_policy –æ–±—ã—á–Ω–æ jsonb, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
        if isinstance(policy, str):
            try:
                policy = json.loads(policy)
                parsed += 1
            except Exception:
                policy = None

        packs.append(
            {
                "id": int(r["id"]),
                "analysis_id": int(r["analysis_id"]),
                "bins_policy": policy,
                "enabled_at": r["enabled_at"],
            }
        )

    log.debug(
        "PACK_INIT: –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö pack-–∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: %s (bins_policy parsed_from_str=%s)",
        len(packs),
        parsed,
    )
    return packs


async def load_analysis_instances(pg: Any, analysis_ids: list[int]) -> dict[int, dict[str, Any]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT id, family_key, "key", "name", enabled
            FROM {ANALYSIS_INSTANCES_TABLE}
            WHERE id = ANY($1::int[])
            """,
            analysis_ids,
        )

    out: dict[int, dict[str, Any]] = {}
    for r in rows:
        out[int(r["id"])] = {
            "family_key": str(r["family_key"]),
            "key": str(r["key"]),
            "name": str(r["name"]),
            "enabled": bool(r["enabled"]),
        }

    log.debug("PACK_INIT: bt_analysis_instances –∑–∞–≥—Ä—É–∂–µ–Ω–æ: %s", len(out))
    return out


async def load_analysis_parameters(pg: Any, analysis_ids: list[int]) -> dict[int, dict[str, str]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT analysis_id, param_name, param_value
            FROM {ANALYSIS_PARAMETERS_TABLE}
            WHERE analysis_id = ANY($1::int[])
            """,
            analysis_ids,
        )

    params: dict[int, dict[str, str]] = {}
    for r in rows:
        aid = int(r["analysis_id"])
        params.setdefault(aid, {})[str(r["param_name"])] = str(r["param_value"])

    ok = 0
    missing = 0
    for aid in analysis_ids:
        if (params.get(aid) or {}).get("param_name"):
            ok += 1
        else:
            missing += 1

    log.debug("PACK_INIT: bt_analysis_parameters (param_name) OK=%s, missing=%s", ok, missing)
    return params


async def load_static_bins_dict(pg: Any, analysis_ids: list[int]) -> dict[int, dict[str, dict[str, list[BinRule]]]]:
    log = logging.getLogger("PACK_INIT")
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT analysis_id, direction, timeframe, bin_type, bin_order, bin_name, val_from, val_to, to_inclusive
            FROM {BINS_DICT_TABLE}
            WHERE analysis_id = ANY($1::int[])
              AND bin_type = 'bins'
            """,
            analysis_ids,
        )

    out: dict[int, dict[str, dict[str, list[BinRule]]]] = {}
    total = 0

    for r in rows:
        aid = int(r["analysis_id"])
        direction = str(r["direction"])
        tf = str(r["timeframe"])

        rule = BinRule(
            direction=direction,
            timeframe=tf,
            bin_type=str(r["bin_type"]),
            bin_order=int(r["bin_order"]),
            bin_name=str(r["bin_name"]),
            val_from=str(r["val_from"]) if r["val_from"] is not None else None,
            val_to=str(r["val_to"]) if r["val_to"] is not None else None,
            to_inclusive=bool(r["to_inclusive"]),
        )

        out.setdefault(aid, {}).setdefault(tf, {}).setdefault(direction, []).append(rule)
        total += 1

    for aid in out:
        for tf in out[aid]:
            for direction in out[aid][tf]:
                out[aid][tf][direction].sort(key=lambda x: x.bin_order)

    log.debug("PACK_INIT: static bins –∑–∞–≥—Ä—É–∂–µ–Ω–æ: rules=%s", total)
    return out


# üî∏ DB loaders: winners from labels_v2 (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã -> run_id + winner analysis_id + indicator_param)
async def load_winners_from_labels_v2(
    pg: Any,
    pairs: list[tuple[int, int]],
) -> dict[tuple[int, int], dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç winner-–º–µ—Ç—É –ø–æ –ø–∞—Ä–∞–º (scenario_id, signal_id) –∏–∑ bt_analysis_bins_labels_v2.

    –¢–∞–±–ª–∏—Ü–∞ bt_analysis_bins_labels_v2 —Å—á–∏—Ç–∞–µ—Ç—Å—è "–∞–∫—Ç—É–∞–ª—å–Ω—ã–º —Å–Ω–∏–º–∫–æ–º" (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ü–µ–ª–∏–∫–æ–º).
    –ü–æ –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ –æ–∂–∏–¥–∞–µ—Ç—Å—è –æ–¥–∏–Ω run_id –∏ –æ–¥–∏–Ω analysis_id (winner), –Ω–æ –º–Ω–æ–≥–æ —Å—Ç—Ä–æ–∫ –ø–æ bin_name.
    """
    log = logging.getLogger("PACK_INIT")

    # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏
    if not pairs:
        return {}

    uniq_pairs = sorted({(int(sc), int(sig)) for (sc, sig) in pairs})
    scenario_ids = [int(sc) for sc, _ in uniq_pairs]
    signal_ids = [int(sig) for _, sig in uniq_pairs]

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            WITH wanted AS (
                SELECT *
                FROM unnest($1::int[], $2::int[]) AS t(scenario_id, signal_id)
            )
            SELECT DISTINCT ON (l.scenario_id, l.signal_id)
                   l.scenario_id,
                   l.signal_id,
                   l.run_id,
                   l.analysis_id,
                   l.indicator_param,
                   l.timeframe
            FROM {BINS_LABELS_TABLE} l
            JOIN wanted w
              ON w.scenario_id = l.scenario_id
             AND w.signal_id   = l.signal_id
            ORDER BY l.scenario_id, l.signal_id, l.created_at DESC, l.id DESC
            """,
            scenario_ids,
            signal_ids,
        )

    out: dict[tuple[int, int], dict[str, Any]] = {}
    for r in rows:
        try:
            sc = int(r["scenario_id"])
            sig = int(r["signal_id"])
            out[(sc, sig)] = {
                "run_id": int(r["run_id"]),
                "analysis_id": int(r["analysis_id"]),
                "indicator_param": str(r["indicator_param"]) if r["indicator_param"] is not None else None,
                "timeframe": str(r["timeframe"]) if r["timeframe"] is not None else None,
            }
        except Exception:
            continue

    missing = max(0, len(uniq_pairs) - len(out))
    log.debug(
        "PACK_INIT: winners loaded from labels_v2 ‚Äî requested_pairs=%s, found=%s, missing=%s",
        len(uniq_pairs),
        len(out),
        missing,
    )
    return out


# üî∏ DB loaders: run registry (legacy; –µ—Å–ª–∏ –Ω—É–∂–µ–Ω fallback)
async def load_latest_finished_run_ids(pg: Any, signal_ids: list[int]) -> dict[int, int]:
    """
    LEGACY: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç signal_id -> run_id (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–π –ø–æ finished_at).
    –û—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏/—Ñ–æ–ª–±—ç–∫–∞.
    """
    log = logging.getLogger("PACK_INIT")
    if not signal_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT DISTINCT ON (signal_id)
                   signal_id,
                   id AS run_id
            FROM {RUNS_TABLE}
            WHERE signal_id = ANY($1::int[])
              AND finished_at IS NOT NULL
              AND status <> 'running'
            ORDER BY signal_id, finished_at DESC, id DESC
            """,
            signal_ids,
        )

    out: dict[int, int] = {}
    for r in rows:
        try:
            out[int(r["signal_id"])] = int(r["run_id"])
        except Exception:
            continue

    log.debug("PACK_INIT: latest finished runs loaded ‚Äî signals=%s, runs=%s", len(signal_ids), len(out))
    return out


# üî∏ DB loaders: adaptive bins/quantiles (run-aware)
async def load_adaptive_bins_for_pair(
    pg: Any,
    run_id: int,
    analysis_ids: list[int],
    scenario_id: int,
    signal_id: int,
    bin_type: str,
) -> dict[tuple[int, str, str], list[BinRule]]:
    # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏
    if not analysis_ids:
        return {}

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT analysis_id, direction, timeframe, bin_type, bin_order, bin_name, val_from, val_to, to_inclusive
            FROM {ADAPTIVE_BINS_TABLE}
            WHERE run_id      = $1
              AND analysis_id = ANY($2::int[])
              AND scenario_id = $3
              AND signal_id   = $4
              AND bin_type    = $5
            ORDER BY analysis_id, timeframe, direction, bin_order
            """,
            int(run_id),
            analysis_ids,
            int(scenario_id),
            int(signal_id),
            str(bin_type),
        )

    out: dict[tuple[int, str, str], list[BinRule]] = {}
    for r in rows:
        aid = int(r["analysis_id"])
        tf = str(r["timeframe"])
        direction = str(r["direction"])

        rule = BinRule(
            direction=direction,
            timeframe=tf,
            bin_type=str(r["bin_type"]),
            bin_order=int(r["bin_order"]),
            bin_name=str(r["bin_name"]),
            val_from=str(r["val_from"]) if r["val_from"] is not None else None,
            val_to=str(r["val_to"]) if r["val_to"] is not None else None,
            to_inclusive=bool(r["to_inclusive"]),
        )

        out.setdefault((aid, tf, direction), []).append(rule)

    for k in out:
        out[k].sort(key=lambda x: x.bin_order)

    return out


# üî∏ DB loaders: labels bins (legacy; –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è "winner-only" –∫–µ—à–∞)
async def load_labels_bins_for_pair(
    pg: Any,
    run_id: int,
    scenario_id: int,
    signal_id: int,
    contexts: list[LabelsContext],
) -> dict[tuple[int, int, str, int, str, str], set[str]]:
    # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏
    if not contexts:
        return {}

    analysis_ids = sorted({c.analysis_id for c in contexts})
    indicator_params = sorted({c.indicator_param for c in contexts})
    timeframes = sorted({c.timeframe for c in contexts})

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT scenario_id, signal_id, direction, analysis_id, indicator_param, timeframe, bin_name
            FROM {BINS_LABELS_TABLE}
            WHERE run_id      = $1
              AND scenario_id = $2
              AND signal_id   = $3
              AND analysis_id = ANY($4::int[])
              AND indicator_param = ANY($5::text[])
              AND timeframe   = ANY($6::text[])
            """,
            int(run_id),
            int(scenario_id),
            int(signal_id),
            analysis_ids,
            indicator_params,
            timeframes,
        )

    out: dict[tuple[int, int, str, int, str, str], set[str]] = {}
    if not rows:
        return out

    ctx_set = {(c.analysis_id, c.indicator_param, c.timeframe) for c in contexts}
    for r in rows:
        try:
            aid = int(r["analysis_id"])
            ip = str(r["indicator_param"])
            tf = str(r["timeframe"])
            if (aid, ip, tf) not in ctx_set:
                continue

            direction = str(r["direction"] or "")
            bin_name = str(r["bin_name"] or "")
            if not direction or not bin_name:
                continue

            k = (
                int(scenario_id),
                int(signal_id),
                direction,
                aid,
                ip,
                tf,
            )
            out.setdefault(k, set()).add(bin_name)
        except Exception:
            continue

    return out