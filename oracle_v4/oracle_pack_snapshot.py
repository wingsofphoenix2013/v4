# ðŸ”¸ oracle_pack_snapshot.py â€” Ð²Ð¾Ñ€ÐºÐµÑ€ PACK-Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð¾Ð²: Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ Ð¿Ð¾ RSI (solo/combos), ÐºÐ°Ñ€ÐºÐ°Ñ Ð¿Ð¾Ð´ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ PACK-Ð¸

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import infra

log = logging.getLogger("ORACLE_PACK_SNAPSHOT")

# ðŸ”¸ ÐšÐ¾Ð½ÑÑ‚Ð°Ð½Ñ‚Ñ‹ Ð²Ð¾Ñ€ÐºÐµÑ€Ð° / Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
INITIAL_DELAY_SEC = 90                    # Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº Ñ‡ÐµÑ€ÐµÐ· 90 ÑÐµÐºÑƒÐ½Ð´
INTERVAL_SEC = 4 * 60 * 60                # Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ â€” ÐºÐ°Ð¶Ð´Ñ‹Ðµ 4 Ñ‡Ð°ÑÐ°
REDIS_TTL_SEC = 8 * 60 * 60               # TTL KV Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¹ â€” 8 Ñ‡Ð°ÑÐ¾Ð²
BATCH_SIZE = 500                          # Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð¿Ð¾ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑÐ¼
WINDOW_TAGS = ("7d", "14d", "28d")
WINDOW_SIZES = {
    "7d": timedelta(days=7),
    "14d": timedelta(days=14),
    "28d": timedelta(days=28),
}
TF_ORDER = ("m5", "m15", "h1")

# ðŸ”¸ Ð‘ÐµÐ»Ñ‹Ðµ ÑÐ¿Ð¸ÑÐºÐ¸ Ð¿Ð¾Ð»ÐµÐ¹ Ð¿Ð¾ PACK (Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€ Ð´Ð»Ñ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸)
PACK_FIELDS = {
    "rsi":     ["bucket_low", "trend"],
    "mfi":     ["bucket_low", "trend"],
    "bb":      ["bucket", "bucket_delta", "bw_trend_strict", "bw_trend_smooth"],
    "lr":      ["bucket", "bucket_delta", "angle_trend"],
    "atr":     ["bucket", "bucket_delta"],
    "adx_dmi": [
        "adx_bucket_low",
        "adx_dynamic_strict",
        "adx_dynamic_smooth",
        "gap_bucket_low",
        "gap_dynamic_strict",
        "gap_dynamic_smooth",
    ],
    "ema":     ["side", "dynamic", "dynamic_strict", "dynamic_smooth"],
    "macd":    ["mode", "cross", "zero_side", "hist_bucket_low_pct", "hist_trend_strict", "hist_trend_smooth"],
}

# ðŸ”¸ ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»ÐµÐ¹ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ PACK
PACK_COMBOS = {
    "rsi": [("bucket_low", "trend")],
    "mfi": [("bucket_low", "trend")],

    "bb": [
        # Ð¿Ð°Ñ€Ñ‹
        ("bucket", "bucket_delta"),
        ("bucket", "bw_trend_strict"),
        ("bucket", "bw_trend_smooth"),
        ("bucket_delta", "bw_trend_strict"),
        ("bucket_delta", "bw_trend_smooth"),
        ("bw_trend_strict", "bw_trend_smooth"),
        # Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸ (Ð±ÐµÐ· Ð·Ð°Ð¿Ñ€ÐµÑ‰Ñ‘Ð½Ð½Ð¾Ð¹ bucket_delta+bw_trend_strict+bw_trend_smooth)
        ("bucket", "bucket_delta", "bw_trend_strict"),
        ("bucket", "bucket_delta", "bw_trend_smooth"),
        ("bucket", "bw_trend_strict", "bw_trend_smooth"),
    ],

    "lr": [
        # Ð¿Ð°Ñ€Ñ‹
        ("bucket", "bucket_delta"),
        ("bucket", "angle_trend"),
        ("bucket_delta", "angle_trend"),
        # Ñ‚Ñ€Ð¾Ð¹ÐºÐ°
        ("bucket", "bucket_delta", "angle_trend"),
    ],

    "atr": [
        ("bucket", "bucket_delta"),
    ],

    "adx_dmi": [
        # Ð¿Ð°Ñ€Ñ‹
        ("adx_bucket_low", "gap_bucket_low"),
        ("adx_bucket_low", "adx_dynamic_strict"),
        ("adx_bucket_low", "adx_dynamic_smooth"),
        ("gap_bucket_low", "gap_dynamic_strict"),
        ("gap_bucket_low", "gap_dynamic_smooth"),
        ("adx_bucket_low", "gap_dynamic_strict"),
        ("adx_bucket_low", "gap_dynamic_smooth"),
        ("gap_bucket_low", "adx_dynamic_strict"),
        ("gap_bucket_low", "adx_dynamic_smooth"),
        # Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸
        ("adx_bucket_low", "gap_bucket_low", "adx_dynamic_strict"),
        ("adx_bucket_low", "gap_bucket_low", "adx_dynamic_smooth"),
        ("adx_bucket_low", "gap_bucket_low", "gap_dynamic_strict"),
        ("adx_bucket_low", "gap_bucket_low", "gap_dynamic_smooth"),
    ],

    "ema": [
        # Ð¿Ð°Ñ€Ñ‹ (Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº: side â†’ dynamic â†’ dynamic_strict â†’ dynamic_smooth)
        ("side", "dynamic"),
        ("side", "dynamic_strict"),
        ("side", "dynamic_smooth"),
        ("dynamic", "dynamic_strict"),
        ("dynamic", "dynamic_smooth"),
        # Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸
        ("side", "dynamic", "dynamic_strict"),
        ("side", "dynamic", "dynamic_smooth"),
        ("side", "dynamic_strict", "dynamic_smooth"),
        ("dynamic", "dynamic_strict", "dynamic_smooth"),
        # Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ°
        ("side", "dynamic", "dynamic_strict", "dynamic_smooth"),
    ],

    "macd": [
        # Ð¿Ð°Ñ€Ñ‹ (Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº: mode â†’ cross â†’ zero_side â†’ hist_bucket_low_pct â†’ hist_trend_strict â†’ hist_trend_smooth)
        ("mode", "cross"),
        ("mode", "zero_side"),
        ("mode", "hist_bucket_low_pct"),
        ("mode", "hist_trend_strict"),
        ("mode", "hist_trend_smooth"),
        ("cross", "zero_side"),
        ("cross", "hist_bucket_low_pct"),
        ("cross", "hist_trend_strict"),
        ("cross", "hist_trend_smooth"),
        ("zero_side", "hist_bucket_low_pct"),
        ("zero_side", "hist_trend_strict"),
        ("zero_side", "hist_trend_smooth"),
        ("hist_bucket_low_pct", "hist_trend_strict"),
        ("hist_bucket_low_pct", "hist_trend_smooth"),

        # Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸
        ("mode", "cross", "zero_side"),
        ("mode", "zero_side", "hist_trend_strict"),
        ("mode", "zero_side", "hist_trend_smooth"),
        ("cross", "zero_side", "hist_trend_strict"),
        ("cross", "zero_side", "hist_trend_smooth"),
        ("zero_side", "hist_bucket_low_pct", "hist_trend_strict"),
        ("zero_side", "hist_bucket_low_pct", "hist_trend_smooth"),
        ("mode", "hist_bucket_low_pct", "hist_trend_strict"),
        ("mode", "hist_bucket_low_pct", "hist_trend_smooth"),
        ("cross", "hist_bucket_low_pct", "hist_trend_strict"),
        ("cross", "hist_bucket_low_pct", "hist_trend_smooth"),
        ("mode", "cross", "hist_trend_strict"),
        ("mode", "cross", "hist_trend_smooth"),

        # Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ¸
        ("mode", "cross", "zero_side", "hist_trend_strict"),
        ("mode", "cross", "zero_side", "hist_trend_smooth"),
    ],
}

# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð²Ð¾Ñ€ÐºÐµÑ€Ð° (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¸Ð· oracle_v4_main.py â†’ run_periodic)
async def run_oracle_pack_snapshot():
    # ÑƒÑÐ»Ð¾Ð²Ð¸Ñ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
    if infra.pg_pool is None or infra.redis_client is None:
        log.debug("âŒ ÐŸÑ€Ð¾Ð¿ÑƒÑÐº: PG/Redis Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")
        return

    strategies = sorted(infra.market_watcher_strategies or [])
    if not strategies:
        log.debug("â„¹ï¸ Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹ Ñ market_watcher=true Ð½ÐµÑ‚ â€” Ð½ÐµÑ‡ÐµÐ³Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ")
        return

    t_ref = datetime.utcnow().replace(tzinfo=None)  # UTC-naive
    log.debug("ðŸš€ Ð¡Ñ‚Ð°Ñ€Ñ‚ PACK-Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð° t0=%s, ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹=%d", t_ref.isoformat(), len(strategies))

    async with infra.pg_pool.acquire() as conn:
        for sid in strategies:
            try:
                await _process_strategy(conn, sid, t_ref)
            except Exception:
                log.exception("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° PACK Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ strategy_id=%s", sid)

    log.debug("âœ… Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ PACK-Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð¾Ð² (ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹=%d)", len(strategies))


# ðŸ”¸ ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ Ð¿Ð¾ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸: Ð²ÑÐµ Ð¾ÐºÐ½Ð° â†’ Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð¾ÐºÐ½Ñƒ Ð²ÑÐµ TF Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾
async def _process_strategy(conn, strategy_id: int, t_ref: datetime):
    for tag in WINDOW_TAGS:
        win_start = t_ref - WINDOW_SIZES[tag]
        win_end = t_ref

        # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ (Ð¸Ð»Ð¸ Ð¿ÐµÑ€ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼) ÑˆÐ°Ð¿ÐºÑƒ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð° â€” Ñ‚Ð° Ð¶Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° oracle_report_stat
        report_id = await _create_report_header(conn, strategy_id, tag, win_start, win_end)

        # Ð¾Ð±Ñ‰Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ð¾ Ð¾ÐºÐ½Ñƒ â€” Ð¾Ð´Ð½Ð¸Ð¼ SQL
        closed_total, closed_wins, pnl_sum_total, pnl_sum_wins = await _calc_report_head_metrics(
            conn, strategy_id, win_start, win_end
        )
        days_in_window = WINDOW_SIZES[tag].total_seconds() / 86400.0

        winrate = round((closed_wins / closed_total) if closed_total else 0.0, 4)
        avg_pnl_per_trade = round((pnl_sum_total / closed_total) if closed_total else 0.0, 4)
        avg_trades_per_day = round(closed_total / days_in_window, 4)

        await _finalize_report_header(
            conn=conn,
            report_id=report_id,
            closed_total=closed_total,
            closed_wins=closed_wins,
            winrate=winrate,
            pnl_sum_total=pnl_sum_total,
            pnl_sum_wins=pnl_sum_wins,
            avg_pnl_per_trade=avg_pnl_per_trade,
            avg_trades_per_day=avg_trades_per_day,
        )

        if closed_total == 0:
            log.debug("[PACK REPORT] sid=%s win=%s total=0 â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐº TF/Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸", strategy_id, tag)
            continue

        # Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ Ð¿Ð¾ TF â€” ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐŸÐÐšÐ˜ (RSI + MFI)
        for tf in TF_ORDER:
            try:
                await _process_timeframe_rsi(conn, report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_mfi(conn,  report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_bb(conn,   report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_lr(conn,   report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_atr(conn,  report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_adx(conn,  report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_ema(conn,  report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
                await _process_timeframe_macd(conn, report_id, strategy_id, tag, tf, win_start, win_end, days_in_window)
            except Exception:
                log.exception("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° PACK Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸ sid=%s win=%s tf=%s", strategy_id, tag, tf)

        # Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ KV ÑÐ²Ð¾Ð´Ð¾Ðº Ð¿Ð¾ ÑÑ‚Ð¾Ð¼Ñƒ report (Ð½Ð° pack-ÐºÐ»ÑŽÑ‡Ð¸)
        try:
            await _publish_kv_bulk(conn, infra.redis_client, report_id, strategy_id, tag)
        except Exception:
            log.exception("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸ PACK KV sid=%s win=%s", strategy_id, tag)

        log.debug(
            "[PACK REPORT] sid=%s win=%s report_id=%s total=%d wins=%d wr=%.4f pnl_sum=%.4f avg_pnl=%.4f avg_tpd=%.4f",
            strategy_id, tag, report_id, closed_total, closed_wins, winrate, pnl_sum_total, avg_pnl_per_trade, avg_trades_per_day
        )

# ðŸ”¸ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ (Ð¸Ð»Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚ id) ÑˆÐ°Ð¿ÐºÐ¸ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð°
async def _create_report_header(conn, strategy_id: int, time_frame: str, win_start: datetime, win_end: datetime) -> int:
    row = await conn.fetchrow(
        """
        INSERT INTO oracle_report_stat (strategy_id, time_frame, window_start, window_end)
        VALUES ($1, $2, $3, $4)
        ON CONFLICT (strategy_id, time_frame, window_start, window_end)
        DO UPDATE SET created_at = oracle_report_stat.created_at
        RETURNING id
        """,
        strategy_id, time_frame, win_start, win_end
    )
    return int(row["id"])

# ðŸ”¸ Ð Ð°ÑÑ‡Ñ‘Ñ‚ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑˆÐ°Ð¿ÐºÐ¸ (Ð¾Ð´Ð½Ð¸Ð¼ SQL)
async def _calc_report_head_metrics(conn, strategy_id: int, win_start: datetime, win_end: datetime):
    r = await conn.fetchrow(
        """
        SELECT
            COUNT(*)::int                         AS closed_total,
            COALESCE(SUM( (pnl > 0)::int ), 0)::int AS closed_wins,
            COALESCE(SUM(pnl), 0)::numeric(24,4)    AS pnl_sum_total,
            COALESCE(SUM(CASE WHEN pnl > 0 THEN pnl ELSE 0 END), 0)::numeric(24,4) AS pnl_sum_wins
        FROM positions_v4
        WHERE strategy_id = $1
          AND status = 'closed'
          AND closed_at >= $2
          AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    return int(r["closed_total"]), int(r["closed_wins"]), float(r["pnl_sum_total"]), float(r["pnl_sum_wins"])


# ðŸ”¸ Ð¤Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑˆÐ°Ð¿ÐºÐ¸ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð° (update Ð¼ÐµÑ‚Ñ€Ð¸Ðº)
async def _finalize_report_header(
    conn,
    report_id: int,
    closed_total: int,
    closed_wins: int,
    winrate: float,
    pnl_sum_total: float,
    pnl_sum_wins: float,
    avg_pnl_per_trade: float,
    avg_trades_per_day: float,
):
    await conn.execute(
        """
        UPDATE oracle_report_stat
           SET closed_total       = $2,
               closed_wins        = $3,
               winrate            = $4,
               pnl_sum_total      = $5,
               pnl_sum_wins       = $6,
               avg_pnl_per_trade  = $7,
               avg_trades_per_day = $8
         WHERE id = $1
        """,
        report_id,
        int(closed_total),
        int(closed_wins),
        round(float(winrate), 4),
        round(float(pnl_sum_total), 4),
        round(float(pnl_sum_wins), 4),
        round(float(avg_pnl_per_trade), 4),
        round(float(avg_trades_per_day), 4),
    )


# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=RSI (solo + combo Ð²Ð½ÑƒÑ‚Ñ€Ð¸ RSI)
async def _process_timeframe_rsi(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-RSI] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    # Ð¿Ð¾Ð´ÑÐ¾Ð±Ð½Ñ‹Ðµ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð°/Ð¿Ð¾Ñ€ÑÐ´ÐºÐ¸
    rsi_fields = PACK_FIELDS["rsi"]
    rsi_combos = PACK_COMBOS["rsi"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ð·Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ PACK-Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ RSI (param_base LIKE 'rsi%'), Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'rsi%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, rsi_fields,
        )

        # Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼: uid â†’ pack_base (rsi14/21/...) â†’ { field: value(str) }
        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}  # uid -> set(pack_base) Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸

        for r in rows_pack:
            if r["status"] != "ok":
                # Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¼Ñƒ pack_base: Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÐµÐ³Ð¾ (Ð½Ð¾ Ð½Ðµ Ð²ÑÑŽ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑŽ)
                has_error.setdefault(r["position_uid"], set()).add(r["param_base"])
                continue

            uid = r["position_uid"]
            base = r["param_base"]           # rsi14, rsi21...
            name = r["param_name"]           # bucket_low | trend
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                # Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾Ðµ â€” Ñ…Ñ€Ð°Ð½Ð¸Ð¼ ÐºÐ°Ðº ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ (Ð±ÐµÐ· Ð·Ð°ÑÐ¾Ñ€ÐµÐ½Ð¸Ñ)
                num = float(r["value_num"] or 0.0)
                # RSI bucket_low â€” ÐºÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ñ†ÐµÐ»Ð¾Ðµ ÐºÑ€Ð°Ñ‚Ð½Ð¾ 5 â†’ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð½ÑƒÐ»ÐµÐ¹
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        # Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ batch (solo+combo)
        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                # Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ pack_base Ñ error
                if base in has_error.get(uid, set()):
                    continue

                # SOLO: Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¼Ñƒ Ð¿Ð¾Ð»ÑŽ
                for fname in rsi_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBO: Ð¿Ð¾ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð½Ð°Ð±Ð¾Ñ€Ð°Ð¼ Ð¿Ð¾Ð»ÐµÐ¹ (Ð² Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ Ð¸Ð· PACK_COMBOS['rsi'])
                for combo in rsi_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)  # 'bucket_low|trend'
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)  # 'bucket_low:50|trend:up'
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        # Ð±Ð°Ñ‚Ñ‡ÐµÐ²Ñ‹Ð¹ UPSERT
        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-RSI] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=MFI (solo + combo Ð²Ð½ÑƒÑ‚Ñ€Ð¸ MFI)
async def _process_timeframe_mfi(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-MFI] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    mfi_fields = PACK_FIELDS["mfi"]
    mfi_combos = PACK_COMBOS["mfi"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ MFI (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'mfi%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, mfi_fields,
        )

        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # mfi14, mfi21, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # bucket_low | trend

            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð¼Ñƒ Ð¿Ð¾Ð»ÑŽ
                for fname in mfi_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBO Ð²Ð½ÑƒÑ‚Ñ€Ð¸ MFI
                for combo in mfi_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)  # 'bucket_low|trend'
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-MFI] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=BB (solo + combos Ð²Ð½ÑƒÑ‚Ñ€Ð¸ BB)
async def _process_timeframe_bb(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-BB] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    bb_fields = PACK_FIELDS["bb"]
    bb_combos = PACK_COMBOS["bb"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ BB (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'bb%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, bb_fields,
        )

        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # bb20_2_0, bb50_2_0, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # bucket | bucket_delta | bw_trend_*
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð¼Ñƒ Ð¿Ð¾Ð»ÑŽ (bucket, bucket_delta, bw_trend_*)
                for fname in bb_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBOS Ð²Ð½ÑƒÑ‚Ñ€Ð¸ BB: Ð¿Ð°Ñ€Ñ‹ Ð¸ Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸ (ÐºÑ€Ð¾Ð¼Ðµ Ð·Ð°Ð¿Ñ€ÐµÑ‚Ð½Ð¾Ð¹ Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸)
                for combo in bb_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-BB] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=LR (solo + combos Ð²Ð½ÑƒÑ‚Ñ€Ð¸ LR)
async def _process_timeframe_lr(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-LR] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    lr_fields = PACK_FIELDS["lr"]
    lr_combos = PACK_COMBOS["lr"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ LR (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'lr%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, lr_fields,
        )

        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # lr50, lr100, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # bucket | bucket_delta | angle_trend
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð¼Ñƒ Ð¿Ð¾Ð»ÑŽ
                for fname in lr_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBOS Ð²Ð½ÑƒÑ‚Ñ€Ð¸ LR: Ð¿Ð°Ñ€Ñ‹ Ð¸ Ñ‚Ñ€Ð¾Ð¹ÐºÐ°
                for combo in lr_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-LR] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=EMA (solo + Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸/Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ°)
async def _process_timeframe_ema(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-EMA] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    ema_fields = PACK_FIELDS["ema"]          # ["side","dynamic","dynamic_strict","dynamic_smooth"]
    ema_combos = PACK_COMBOS["ema"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ EMA (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'ema%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, ema_fields,
        )

        # Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼: uid â†’ pack_base (ema21/ema50/...) â†’ { field: value(str) }
        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # ema21, ema50, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # side|dynamic|dynamic_strict|dynamic_smooth
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        # Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ batch (solo + Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸/Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ°)
        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO: Ð²ÑÐµ 4 Ð¿Ð¾Ð»Ñ Ð¿Ð¾ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
                for fname in ema_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBOS: Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸/Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ° â€” ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ ÑÐ¿Ð¸ÑÐºÑƒ
                for combo in ema_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-EMA] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=ATR (solo + combo bucket|bucket_delta)
async def _process_timeframe_atr(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-ATR] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    atr_fields = PACK_FIELDS["atr"]         # ["bucket", "bucket_delta"]
    atr_combos = PACK_COMBOS["atr"]         # [("bucket","bucket_delta")]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ATR (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'atr%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, atr_fields,
        )

        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # atr14, atr21, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # bucket | bucket_delta
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO
                for fname in atr_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBO bucket|bucket_delta
                for combo in atr_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-ATR] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=ADX_DMI (solo Ñ‚Ð¾Ð»ÑŒÐºÐ¾ bucket-Ð¿Ð¾Ð»Ñ; Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸ â€” Ð¿Ð¾ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ ÑÐ¿Ð¸ÑÐºÑƒ)
async def _process_timeframe_adx(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-ADX_DMI] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    adx_fields = PACK_FIELDS["adx_dmi"]
    adx_combos = PACK_COMBOS["adx_dmi"]
    # solo â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ bucket-Ð¿Ð¾Ð»ÑÐ¼ (ÐºÐ°Ðº Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð»Ð¸ÑÑŒ)
    adx_solo_fields = ("adx_bucket_low", "gap_bucket_low")

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ADX_DMI (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'adx_dmi%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, adx_fields,
        )

        # Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼: uid â†’ pack_base (adx_dmi14/21/...) â†’ { field: value(str) }
        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}  # uid -> set(pack_base) Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # adx_dmi14, adx_dmi21, ...
            status = r["status"]

            if status != "ok":
                # Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¼Ñƒ pack_base: Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÐµÐ³Ð¾ (Ð½Ð¾ Ð½Ðµ Ð²ÑÑŽ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑŽ)
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]      # Ð¸Ð· adx_fields
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                # bucket_low Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹, Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ â€” Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ð¾
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        # Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ batch (solo + Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸)
        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                # Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ pack_base Ñ error
                if base in has_error.get(uid, set()):
                    continue

                # SOLO: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ bucket-Ð¿Ð¾Ð»Ñ
                for fname in adx_solo_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBOS: Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ ÑÐ¿Ð¸ÑÐºÑƒ
                for combo in adx_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        # Ð±Ð°Ñ‚Ñ‡ÐµÐ²Ñ‹Ð¹ UPSERT
        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-ADX_DMI] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)

# ðŸ”¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° TF: PACK=MACD (solo + Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸/Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ¸)
async def _process_timeframe_macd(
    conn,
    report_id: int,
    strategy_id: int,
    time_frame: str,
    timeframe: str,
    win_start: datetime,
    win_end: datetime,
    days_in_window: float,
):
    # Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð° (direction, pnl)
    rows = await conn.fetch(
        """
        SELECT position_uid, direction, pnl
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end
    )
    positions = [dict(r) for r in rows]
    if not positions:
        log.debug("[PACK-MACD] sid=%s win=%s tf=%s total=0", strategy_id, time_frame, timeframe)
        return

    total = len(positions)
    ok_rows = 0
    batch_count = (total + BATCH_SIZE - 1) // BATCH_SIZE

    macd_fields = PACK_FIELDS["macd"]        # ["mode","cross","zero_side","hist_bucket_low_pct","hist_trend_strict","hist_trend_smooth"]
    macd_combos = PACK_COMBOS["macd"]

    for bi in range(batch_count):
        batch = positions[bi * BATCH_SIZE : (bi + 1) * BATCH_SIZE]
        uid_list = [p["position_uid"] for p in batch]
        uid_meta = {p["position_uid"]: (p["direction"], float(p["pnl"] or 0.0)) for p in batch}

        # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ PACK Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ MACD (Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ TF), Ð¿Ð¾ whitelisted Ð¿Ð¾Ð»ÑÐ¼
        rows_pack = await conn.fetch(
            """
            SELECT position_uid, timeframe, param_base, param_name, value_num, value_text, status
              FROM indicator_position_stat
             WHERE position_uid = ANY($1::text[])
               AND param_type = 'pack'
               AND timeframe = $2
               AND param_base LIKE 'macd%'
               AND param_name = ANY($3::text[])
            """,
            uid_list, timeframe, macd_fields,
        )

        # Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼: uid â†’ pack_base (macd12, macd5, ...) â†’ { field: value(str) }
        by_uid: Dict[str, Dict[str, Dict[str, str]]] = {}
        has_error: Dict[str, set] = {}

        for r in rows_pack:
            uid = r["position_uid"]
            base = r["param_base"]      # macd12, macd5, ...
            status = r["status"]

            if status != "ok":
                has_error.setdefault(uid, set()).add(base)
                continue

            name = r["param_name"]
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ
            if r["value_text"] is not None:
                val = str(r["value_text"])
            else:
                num = float(r["value_num"] or 0.0)
                val = f"{num:.8f}".rstrip('0').rstrip('.') if '.' in f"{num:.8f}" else f"{int(num)}"

            by_uid.setdefault(uid, {}).setdefault(base, {})[name] = val

        # Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ batch
        inc_map: Dict[Tuple, Dict[str, float]] = {}

        for uid, base_map in by_uid.items():
            direction, pnl = uid_meta.get(uid, ("long", 0.0))
            is_win = pnl > 0.0

            for base, fields in base_map.items():
                if base in has_error.get(uid, set()):
                    continue

                # SOLO: Ð²ÑÐµ 6 Ð¿Ð¾Ð»ÐµÐ¹ Ð¿Ð¾ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
                for fname in macd_fields:
                    if fname not in fields:
                        continue
                    fval = fields[fname]
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "solo", fname, fval)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

                # COMBOS: Ð¿Ð°Ñ€Ñ‹/Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸/Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€ÐºÐ¸ â€” ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ ÑÐ¿Ð¸ÑÐºÑƒ
                for combo in macd_combos:
                    if not all(f in fields for f in combo):
                        continue
                    agg_key = "|".join(combo)
                    agg_value = "|".join(f"{f}:{fields[f]}" for f in combo)
                    k = (report_id, strategy_id, time_frame, direction, timeframe, base, "combo", agg_key, agg_value)
                    inc = inc_map.setdefault(k, {"t": 0, "w": 0, "pt": 0.0, "pw": 0.0})
                    inc["t"] += 1
                    if is_win:
                        inc["w"] += 1
                        inc["pw"] = round(inc["pw"] + pnl, 4)
                    inc["pt"] = round(inc["pt"] + pnl, 4)

        if inc_map:
            await _upsert_aggregates_batch(conn, inc_map, days_in_window)
            ok_rows += sum(v["t"] for v in inc_map.values())

    log.debug("[PACK-MACD] sid=%s win=%s tf=%s positions=%d agg_rows=%d", strategy_id, time_frame, timeframe, total, ok_rows)
    
# ðŸ”¸ Ð‘Ð°Ñ‚Ñ‡ÐµÐ²Ñ‹Ð¹ UPSERT (UNNEST + ON CONFLICT) Ñ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
async def _upsert_aggregates_batch(conn, inc_map: Dict[Tuple, Dict[str, float]], days_in_window: float):
    # ÐºÐ»ÑŽÑ‡: (report_id, strategy_id, time_frame, direction, timeframe, pack_base, agg_type, agg_key, agg_value)
    report_ids, strategy_ids, time_frames, directions = [], [], [], []
    timeframes, pack_bases, agg_types, agg_keys, agg_values = [], [], [], [], []
    trades_inc, wins_inc, pnl_total_inc, pnl_wins_inc = [], [], [], []

    for (report_id, strategy_id, time_frame, direction, timeframe, pack_base, agg_type, agg_key, agg_value), v in inc_map.items():
        report_ids.append(report_id)
        strategy_ids.append(strategy_id)
        time_frames.append(time_frame)
        directions.append(direction)
        timeframes.append(timeframe)
        pack_bases.append(pack_base)
        agg_types.append(agg_type)
        agg_keys.append(agg_key)
        agg_values.append(agg_value)
        trades_inc.append(int(v["t"]))
        wins_inc.append(int(v["w"]))
        pnl_total_inc.append(round(float(v["pt"]), 4))
        pnl_wins_inc.append(round(float(v["pw"]), 4))

    await conn.execute(
        """
        WITH data AS (
          SELECT
            unnest($1::bigint[])   AS report_id,
            unnest($2::int[])      AS strategy_id,
            unnest($3::text[])     AS time_frame,
            unnest($4::text[])     AS direction,
            unnest($5::text[])     AS timeframe,
            unnest($6::text[])     AS pack_base,
            unnest($7::text[])     AS agg_type,
            unnest($8::text[])     AS agg_key,
            unnest($9::text[])     AS agg_value,
            unnest($10::int[])     AS t_inc,
            unnest($11::int[])     AS w_inc,
            unnest($12::numeric[]) AS pt_inc,
            unnest($13::numeric[]) AS pw_inc
        )
        INSERT INTO oracle_pack_aggregated_stat (
            report_id, strategy_id, time_frame, direction, timeframe, pack_base, agg_type, agg_key, agg_value,
            trades_total, trades_wins, winrate,
            pnl_sum_total, pnl_sum_wins,
            avg_pnl_per_trade, avg_trades_per_day
        )
        SELECT
            report_id, strategy_id, time_frame, direction, timeframe, pack_base, agg_type, agg_key, agg_value,
            t_inc, w_inc,
            ROUND(CASE WHEN t_inc > 0 THEN w_inc::numeric / t_inc::numeric ELSE 0 END, 4),
            pt_inc, pw_inc,
            ROUND(CASE WHEN t_inc > 0 THEN pt_inc::numeric / t_inc::numeric ELSE 0 END, 4),
            ROUND(t_inc::numeric / $14::numeric, 4)
        FROM data
        ON CONFLICT (report_id, strategy_id, time_frame, direction, timeframe, pack_base, agg_type, agg_key, agg_value)
        DO UPDATE SET
            trades_total       = oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total,
            trades_wins        = oracle_pack_aggregated_stat.trades_wins  + EXCLUDED.trades_wins,
            pnl_sum_total      = ROUND(oracle_pack_aggregated_stat.pnl_sum_total + EXCLUDED.pnl_sum_total, 4),
            pnl_sum_wins       = ROUND(oracle_pack_aggregated_stat.pnl_sum_wins  + EXCLUDED.pnl_sum_wins,  4),
            winrate            = ROUND(
                                   CASE
                                     WHEN (oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total) > 0
                                       THEN (oracle_pack_aggregated_stat.trades_wins + EXCLUDED.trades_wins)::numeric
                                            / (oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total)::numeric
                                     ELSE 0
                                   END, 4),
            avg_pnl_per_trade  = ROUND(
                                   CASE
                                     WHEN (oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total) > 0
                                       THEN (oracle_pack_aggregated_stat.pnl_sum_total + EXCLUDED.pnl_sum_total)::numeric
                                            / (oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total)::numeric
                                     ELSE 0
                                   END, 4),
            avg_trades_per_day = ROUND(
                                   ( (oracle_pack_aggregated_stat.trades_total + EXCLUDED.trades_total)::numeric / $14::numeric ),
                                   4),
            updated_at         = now()
        """,
        report_ids, strategy_ids, time_frames, directions, timeframes, pack_bases, agg_types, agg_keys, agg_values,
        trades_inc, wins_inc, pnl_total_inc, pnl_wins_inc,
        days_in_window,
    )

# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ KV ÑÐ²Ð¾Ð´Ð¾Ðº (Ð¿ÐµÑ€-TF)
async def _publish_kv_bulk(conn, redis, report_id: int, strategy_id: int, time_frame: str):
    row_rep = await conn.fetchrow("SELECT closed_total FROM oracle_report_stat WHERE id = $1", report_id)
    if not row_rep:
        return
    closed_total = int(row_rep["closed_total"] or 0)

    rows = await conn.fetch(
        """
        SELECT DISTINCT ON (direction, timeframe, pack_base, agg_key, agg_value)
               direction, timeframe, pack_base, agg_key, agg_value, trades_total, winrate
          FROM oracle_pack_aggregated_stat
         WHERE report_id = $1
         ORDER BY direction, timeframe, pack_base, agg_key, agg_value, updated_at DESC
        """,
        report_id,
    )
    if not rows:
        return

    pipe = redis.pipeline()
    for r in rows:
        direction = r["direction"]
        timeframe = r["timeframe"]
        pack_base = r["pack_base"]
        agg_key = r["agg_key"]
        agg_value = r["agg_value"]
        trades_total = int(r["trades_total"] or 0)
        winrate = float(r["winrate"] or 0.0)

        # ÐºÐ»ÑŽÑ‡ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ TF (m5/m15/h1)
        key = f"oracle:pack:{strategy_id}:{direction}:{timeframe}:{pack_base}:{agg_key}:{agg_value}:{time_frame}"
        payload = {
            "strategy_id": strategy_id,
            "direction": direction,
            "timeframe": timeframe,
            "pack_base": pack_base,
            "agg_key": agg_key,
            "agg_value": agg_value,
            "time_frame": time_frame,
            "report_id": report_id,
            "closed_total": closed_total,
            "agg_trades_total": trades_total,
            "winrate": f"{winrate:.4f}",
        }
        pipe.set(key, json.dumps(payload), ex=REDIS_TTL_SEC)

    await pipe.execute()