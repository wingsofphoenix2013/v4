# üî∏ oracle_mw_confidence.py ‚Äî –≤–æ—Ä–∫–µ—Ä: —á–∞—Å—Ç–æ—Ç—ã MW (solo+combo), —Ä–∞—Å—á—ë—Ç confidence_score, —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è KV (TTL 8h)

import asyncio
import logging
import math
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import json
from collections import defaultdict

import infra

log = logging.getLogger("ORACLE_MW_CONFIDENCE")

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –≤–æ—Ä–∫–µ—Ä–∞ / –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è
REPORT_READY_STREAM = "oracle:mw:reports_ready"   # –≤—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–∏–º —Å–æ–±—ã—Ç–∏–π ¬´–æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤¬ª
CONF_GROUP = "mwconf_group"                       # consumer-group –≤–æ—Ä–∫–µ—Ä–∞
CONF_CONSUMER = "mwconf_1"                        # –∏–º—è consumer-–∞ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –∏–∑ ENV)
FETCH_BLOCK_MS = 5000                             # –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ XREADGROUP (–º—Å)
BATCH_SIZE_UPDATE = 1000                          # —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–ø–¥–µ–π—Ç–æ–≤ –≤ –ë–î
FINAL_KV_TTL_SEC = 8 * 60 * 60                    # TTL —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö KV ‚Äî 8 —á–∞—Å–æ–≤

# üî∏ –î–æ–º–µ–Ω—ã –∏ –±–∞–∑–æ–≤—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞
TF_ORDER = ("m5", "m15", "h1")
DIRECTIONS = ("long", "short")
MW_COMPONENTS = ("trend", "volatility", "momentum", "extremes")

# üî∏ –ö–æ–º–±–æ-–Ω–∞–±–æ—Ä—ã (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ ‚Äî –∫–∞–∫ –≤ oracle_mw_snapshot)
COMBOS_2 = (
    ("trend", "volatility"),
    ("trend", "extremes"),
    ("trend", "momentum"),
    ("volatility", "extremes"),
    ("volatility", "momentum"),
    ("extremes", "momentum"),
)
COMBOS_3 = (
    ("trend", "volatility", "extremes"),
    ("trend", "volatility", "momentum"),
    ("trend", "extremes", "momentum"),
    ("volatility", "extremes", "momentum"),
)
COMBOS_4 = (tuple(MW_COMPONENTS),)


# üî∏ –¢–∏–ø—ã –∏ –∫–ª—é—á–∏
@dataclass
class MwasRow:
    id: int
    report_id: int
    strategy_id: int
    direction: str
    timeframe: str
    time_frame: str
    agg_type: str
    agg_base: str
    agg_state: str
    trades_total: int
    trades_wins: int
    winrate: float


# üî∏ –ü—É–±–ª–∏—á–Ω–∞—è —Ç–æ—á–∫–∞ –∑–∞–ø—É—Å–∫–∞ –≤–æ—Ä–∫–µ—Ä–∞ (–¥–æ–ª–≥–æ–∏–≥—Ä–∞—é—â–∏–π consumer Redis Stream)
async def run_oracle_mw_confidence():
    # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if infra.pg_pool is None or infra.redis_client is None:
        log.info("‚ùå –ü—Ä–æ–ø—É—Å–∫ –∑–∞–ø—É—Å–∫–∞: PG/Redis –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        return

    # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ consumer-group (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)
    try:
        await infra.redis_client.xgroup_create(
            name=REPORT_READY_STREAM,
            groupname=CONF_GROUP,
            id="$",
            mkstream=True,
        )
        log.info("üì° –°–æ–∑–¥–∞–Ω consumer-group '%s' –¥–ª—è —Å—Ç—Ä–∏–º–∞ '%s'", CONF_GROUP, REPORT_READY_STREAM)
    except Exception as e:
        if "BUSYGROUP" in str(e):
            log.debug("‚ÑπÔ∏è Consumer-group '%s' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç", CONF_GROUP)
        else:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è consumer-group: %s", e)

    log.info("üöÄ –°—Ç–∞—Ä—Ç –≤–æ—Ä–∫–µ—Ä–∞ ORACLE_MW_CONFIDENCE (consumer=%s)", CONF_CONSUMER)

    # –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π
    while True:
        try:
            resp = await infra.redis_client.xreadgroup(
                groupname=CONF_GROUP,
                consumername=CONF_CONSUMER,
                streams={REPORT_READY_STREAM: ">"},
                count=32,
                block=FETCH_BLOCK_MS,
            )
            if not resp:
                continue

            for _, messages in resp:
                for msg_id, fields in messages:
                    # –ø–∞—Ä—Å–∏–º payload (–æ–¥–Ω–æ –ø–æ–ª–µ 'data' —Å–æ —Å—Ç—Ä–æ–∫–æ–π JSON)
                    try:
                        payload = json.loads(fields.get("data") or "{}")
                    except Exception:
                        payload = {}

                    ok = await _handle_report_ready(msg_id, payload)

                    if ok:
                        try:
                            await infra.redis_client.xack(REPORT_READY_STREAM, CONF_GROUP, msg_id)
                            await infra.redis_client.xdel(REPORT_READY_STREAM, msg_id)
                        except Exception:
                            log.exception("‚ùå –û—à–∏–±–∫–∞ ACK/DEL msg_id=%s", msg_id)

        except asyncio.CancelledError:
            log.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤ –≤–æ—Ä–∫–µ—Ä–∞ ORACLE_MW_CONFIDENCE –ø–æ —Å–∏–≥–Ω–∞–ª—É")
            raise
        except Exception:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞ —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞; –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")


# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–±—ã—Ç–∏—è ¬´–æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤¬ª
async def _handle_report_ready(msg_id: str, payload: dict) -> bool:
    # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å
    if not payload or "report_id" not in payload:
        log.info("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å–æ–æ–±—â–µ–Ω–∏—è msg_id=%s: –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π payload", msg_id)
        return True  # ack

    report_id = int(payload["report_id"])
    strategy_id = int(payload.get("strategy_id") or 0)
    time_frame = str(payload.get("time_frame") or "")
    window_start_iso = payload.get("window_start")
    window_end_iso = payload.get("window_end")

    try:
        async with infra.pg_pool.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT strategy_id, time_frame, window_start, window_end FROM oracle_report_stat WHERE id = $1",
                report_id,
            )
            if not row:
                log.info("‚ö†Ô∏è report_id=%s –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ oracle_report_stat ‚Äî –ø—Ä–æ–ø—É—Å–∫", report_id)
                return True

            if not strategy_id:
                strategy_id = int(row["strategy_id"])
            if not time_frame:
                time_frame = str(row["time_frame"])
            win_start = row["window_start"]
            win_end = row["window_end"]
            if window_start_iso:
                try:
                    win_start = datetime.fromisoformat(window_start_iso)
                except Exception:
                    pass
            if window_end_iso:
                try:
                    win_end = datetime.fromisoformat(window_end_iso)
                except Exception:
                    pass

            t0 = datetime.utcnow().replace(tzinfo=None)

            # —à–∞–≥ 1: –∑–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä–∫–∞—Å–∞ –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ (–ø–æ report_id)
            mwas_rows = await _fetch_mwas_rows(conn, report_id)
            if not mwas_rows:
                log.info("[CONF] report_id=%s: –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ –Ω–µ—Ç ‚Äî –Ω–µ—á–µ–≥–æ —Å—á–∏—Ç–∞—Ç—å", report_id)
                return True

            # —à–∞–≥ 2: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç (solo+combo) –∏ –º–∞—Ä–≥–∏–Ω–∞–ª–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            occ_solo, occ_combo, comp_map, denom_map = await _build_occurrence_and_components(
                conn=conn,
                strategy_id=strategy_id,
                win_start=win_start,
                win_end=win_end,
            )

            # —à–∞–≥ 3: UPSERT —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü (occurrence solo+combo, component marginals)
            await _upsert_occurrence(conn, report_id, time_frame, occ_solo, occ_combo, denom_map)
            await _upsert_component_counts(conn, report_id, time_frame, comp_map, denom_map)

            # —à–∞–≥ 4: –∫—ç—à p7/p14/p28 ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–º –æ–∫–Ω–æ–º
            await _update_window_winrates_cache(conn, mwas_rows, time_frame)

            # —à–∞–≥ 5: —á–∏—Ç–∞–µ–º –∫—ç—à –æ–∫–æ–Ω –¥–ª—è –∫–ª—é—á–µ–π –æ—Ç—á—ë—Ç–∞ + —Å—Ç—Ä–æ–∏–º sd-—Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã –ø–æ –∫–æ–≥–æ—Ä—Ç–∞–º (data-driven)
            cache_map = await _fetch_window_cache_map(conn, mwas_rows)
            sd_refs = _build_sd_references_from_cache(mwas_rows, cache_map)

            # —à–∞–≥ 6: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è q_scale (–≤–∫–ª—é—á–∞—è combo)
            dist_map = _build_cohort_distributions(mwas_rows, occ_solo, occ_combo, denom_map)

            # —à–∞–≥ 7: —Ä–∞—Å—á—ë—Ç confidence –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ –∞–ø–¥–µ–π—Ç oracle_mw_aggregated_stat
            upd_items = _compute_confidence_items(
                mwas_rows=mwas_rows,
                occ_solo=occ_solo,
                occ_combo=occ_combo,
                denom_map=denom_map,
                comp_map=comp_map,
                cache_map=cache_map,
                sd_refs=sd_refs,
            )
            if upd_items:
                await _persist_confidence_items(conn, upd_items)

            # —à–∞–≥ 8: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è KV (TTL=8h)
            published = await _publish_final_kv(conn, infra.redis_client, report_id, strategy_id, time_frame)

            # –ª–æ–≥ –∏—Ç–æ–≥–∞
            t1 = datetime.utcnow().replace(tzinfo=None)
            zeros = sum(1 for it in upd_items if (it["confidence_score"] or 0) == 0)
            log.info(
                "[CONF_DONE] report_id=%s sid=%s win=%s rows=%d zeros=%d kv=%d elapsed_ms=%d",
                report_id, strategy_id, time_frame, len(upd_items), zeros, published, int((t1 - t0).total_seconds() * 1000),
            )

            return True

    except Exception:
        log.exception("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ report_id=%s (msg_id=%s)", report_id, msg_id)
        return False


# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–æ–∫ oracle_mw_aggregated_stat –¥–ª—è report_id
async def _fetch_mwas_rows(conn, report_id: int) -> List[MwasRow]:
    rows = await conn.fetch(
        """
        SELECT
            id, report_id, strategy_id, direction, timeframe, time_frame,
            agg_type, agg_base, agg_state,
            trades_total, trades_wins, winrate
        FROM oracle_mw_aggregated_stat
        WHERE report_id = $1
        """,
        report_id,
    )
    out: List[MwasRow] = []
    for r in rows:
        out.append(
            MwasRow(
                id=int(r["id"]),
                report_id=int(r["report_id"]),
                strategy_id=int(r["strategy_id"]),
                direction=str(r["direction"]),
                timeframe=str(r["timeframe"]),
                time_frame=str(r["time_frame"]),
                agg_type=str(r["agg_type"]),
                agg_base=str(r["agg_base"]),
                agg_state=str(r["agg_state"]),
                trades_total=int(r["trades_total"] or 0),
                trades_wins=int(r["trades_wins"] or 0),
                winrate=float(r["winrate"] or 0.0),
            )
        )
    return out


# üî∏ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –ø–æ –æ–∫–Ω—É: SOLO, COMBO –∏ –º–∞—Ä–≥–∏–Ω–∞–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
async def _build_occurrence_and_components(
    conn,
    strategy_id: int,
    win_start: datetime,
    win_end: datetime,
):
    # –≤—ã–±–∏—Ä–∞–µ–º –∑–∞–∫—Ä—ã—Ç—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –æ–∫–Ω–∞ (uid, direction)
    rows_pos = await conn.fetch(
        """
        SELECT position_uid, direction
          FROM positions_v4
         WHERE strategy_id = $1
           AND status = 'closed'
           AND closed_at >= $2
           AND closed_at <  $3
        """,
        strategy_id, win_start, win_end,
    )
    if not rows_pos:
        return {}, {}, {}, {}

    # –º–∞–ø—ã –ø–æ uid
    uid_direction: Dict[str, str] = {str(r["position_uid"]): str(r["direction"]) for r in rows_pos}
    uids: List[str] = list(uid_direction.keys())

    # —á–∏—Ç–∞–µ–º –≤—Å–µ MW-—Å—Ç—Ä–æ–∫–∏ IPS –¥–ª—è —ç—Ç–∏—Ö –ø–æ–∑–∏—Ü–∏–π
    rows_ips = await conn.fetch(
        """
        SELECT position_uid, timeframe, param_base, value_text, status
          FROM indicator_position_stat
         WHERE position_uid = ANY($1::text[])
           AND param_type = 'marketwatch'
        """,
        uids,
    )

    # –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å: –ø–æ–∑–∏—Ü–∏–∏, –≥–¥–µ –Ω–∞ TF –µ—Å—Ç—å –ª—é–±—ã–µ MW-—Å—Ç—Ä–æ–∫–∏ (ok|error)
    denom_map: Dict[Tuple[str, str], set] = defaultdict(set)

    # states_ok[(direction, timeframe)][uid] = {base -> state}
    states_ok: Dict[Tuple[str, str], Dict[str, Dict[str, str]]] = defaultdict(lambda: defaultdict(dict))

    # –ø—Ä–æ—Ö–æ–¥ –ø–æ IPS
    for r in rows_ips:
        uid = str(r["position_uid"])
        tf = str(r["timeframe"])
        base = str(r["param_base"])
        status = str(r["status"])
        val = r["value_text"]
        if uid not in uid_direction:
            continue
        direction = uid_direction[uid]
        key = (direction, tf)
        denom_map[key].add(uid)
        if status == "ok" and base in MW_COMPONENTS and isinstance(val, str) and val:
            states_ok[key][uid][base] = val

    # SOLO occurrence: (direction, timeframe, base, state) -> count
    occ_solo: Dict[Tuple[str, str, str, str], int] = defaultdict(int)
    # COMBO occurrence: (direction, timeframe, agg_base, agg_state) -> count
    occ_combo: Dict[Tuple[str, str, str, str], int] = defaultdict(int)
    # Component marginals: (direction, timeframe, component, comp_state) -> count
    comp_map: Dict[Tuple[str, str, str, str], int] = defaultdict(int)

    for key in states_ok.keys():
        dir_tf_uids = denom_map.get(key, set())
        if not dir_tf_uids:
            continue
        uid_states = states_ok[key]  # {uid -> {base: state}}

        # SOLO: –∫–∞–∂–¥—É—é –±–∞–∑—É/—Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—á–∏—Ç–∞–µ–º 1 —Ä–∞–∑ –Ω–∞ uid
        for uid, bases in uid_states.items():
            for base, state in bases.items():
                occ_solo[(key[0], key[1], base, state)] += 1
                comp_map[(key[0], key[1], base, state)] += 1

        # COMBO: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ uid –ø–æ –∫–∞–∂–¥–æ–π –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        # —Å–æ–±–∏—Ä–∞–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø–æ –±–∞–∑–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ uid, –≥–¥–µ –±–∞–∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        inv_index: Dict[str, Dict[str, set]] = defaultdict(lambda: defaultdict(set))  # base -> state -> {uid}
        for uid, bases in uid_states.items():
            for base, state in bases.items():
                inv_index[base][state].add(uid)

        # –ø–∞—Ä—ã
        for combo in COMBOS_2:
            # —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ ‚Äî –≤—Å–µ –±–∞–∑—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ö–æ—Ç—è –±—ã –≥–¥–µ-—Ç–æ
            if not all(b in inv_index for b in combo):
                continue
            # —Å—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—ã–µ –ø–∞—Ä—ã (state –Ω–∞ –∫–∞–∂–¥—É—é –±–∞–∑—É), –∑–∞—Ç–µ–º –∏—Ö –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            # –ø–µ—Ä–µ—á–∏—Å–ª—è–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π, —Ä–µ–∞–ª—å–Ω–æ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏—Ö—Å—è –≤ uid_states
            states_lists: List[List[Tuple[str, str]]] = []
            for b in combo:
                states_lists.append([(b, s) for s in inv_index[b].keys()])
            for states_tuple in _cartesian_product(states_lists):
                # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ uid
                uids_sets = [inv_index[b][s] for b, s in states_tuple]
                inter = set.intersection(*uids_sets) if uids_sets else set()
                if not inter:
                    continue
                agg_base = "_".join(combo)
                agg_state = "|".join(f"{b}:{s}" for b, s in states_tuple)
                occ_combo[(key[0], key[1], agg_base, agg_state)] += len(inter)

        # —Ç—Ä–æ–π–∫–∏
        for combo in COMBOS_3:
            if not all(b in inv_index for b in combo):
                continue
            states_lists = [[(b, s) for s in inv_index[b].keys()] for b in combo]
            for states_tuple in _cartesian_product(states_lists):
                uids_sets = [inv_index[b][s] for b, s in states_tuple]
                inter = set.intersection(*uids_sets) if uids_sets else set()
                if not inter:
                    continue
                agg_base = "_".join(combo)
                agg_state = "|".join(f"{b}:{s}" for b, s in states_tuple)
                occ_combo[(key[0], key[1], agg_base, agg_state)] += len(inter)

        # —á–µ—Ç–≤—ë—Ä–∫–∏
        for combo in COMBOS_4:
            if not all(b in inv_index for b in combo):
                continue
            states_lists = [[(b, s) for s in inv_index[b].keys()] for b in combo]
            for states_tuple in _cartesian_product(states_lists):
                uids_sets = [inv_index[b][s] for b, s in states_tuple]
                inter = set.intersection(*uids_sets) if uids_sets else set()
                if not inter:
                    continue
                agg_base = "_".join(combo)
                agg_state = "|".join(f"{b}:{s}" for b, s in states_tuple)
                occ_combo[(key[0], key[1], agg_base, agg_state)] += len(inter)

    return occ_solo, occ_combo, comp_map, denom_map


# —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏: –¥–µ–∫–∞—Ä—Ç–æ–≤–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –ø–∞—Ä
def _cartesian_product(lists: List[List[Tuple[str, str]]]) -> List[List[Tuple[str, str]]]:
    if not lists:
        return []
    res: List[List[Tuple[str, str]]] = [[]]
    for lst in lists:
        new_res: List[List[Tuple[str, str]]] = []
        for prefix in res:
            for item in lst:
                new_res.append(prefix + [item])
        res = new_res
    return res


# üî∏ UPSERT occurrence (solo + combo) –≤ oracle_mw_occurrence_stat
async def _upsert_occurrence(
    conn,
    report_id: int,
    time_frame: str,
    occ_solo: Dict[Tuple[str, str, str, str], int],
    occ_combo: Dict[Tuple[str, str, str, str], int],
    denom_map: Dict[Tuple[str, str], set],
):
    # —Å–æ–±–µ—Ä—ë–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ (solo + combo)
    records = []

    # solo
    for (direction, timeframe), uids in denom_map.items():
        total_all = len(uids)
        # –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ (dir, tf) –≤—ã–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏
        for (d, tf, base, state), count in occ_solo.items():
            if d == direction and tf == timeframe:
                records.append((
                    report_id, direction, timeframe, time_frame,
                    "solo", base, state,
                    int(count), int(total_all),
                ))

    # combo
    for (direction, timeframe), uids in denom_map.items():
        total_all = len(uids)
        for (d, tf, agg_base, agg_state), count in occ_combo.items():
            if d == direction and tf == timeframe:
                records.append((
                    report_id, direction, timeframe, time_frame,
                    "combo", agg_base, agg_state,
                    int(count), int(total_all),
                ))

    if not records:
        return

    # —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
    cols = list(zip(*records))
    await conn.execute(
        """
        WITH data AS (
          SELECT
            unnest($1::bigint[]) AS report_id,
            unnest($2::text[])   AS direction,
            unnest($3::text[])   AS timeframe,
            unnest($4::text[])   AS time_frame,
            unnest($5::text[])   AS agg_type,
            unnest($6::text[])   AS agg_base,
            unnest($7::text[])   AS agg_state,
            unnest($8::int[])    AS positions_state,
            unnest($9::int[])    AS positions_all
        )
        INSERT INTO oracle_mw_occurrence_stat (
          report_id, strategy_id, direction, timeframe, time_frame,
          agg_type, agg_base, agg_state, positions_state, positions_all
        )
        SELECT
          d.report_id, ors.strategy_id, d.direction, d.timeframe, d.time_frame,
          d.agg_type, d.agg_base, d.agg_state, d.positions_state, d.positions_all
        FROM data d
        JOIN oracle_report_stat ors ON ors.id = d.report_id
        ON CONFLICT (report_id, strategy_id, direction, timeframe, time_frame, agg_type, agg_base, agg_state)
        DO UPDATE SET
          positions_state = EXCLUDED.positions_state,
          positions_all   = EXCLUDED.positions_all,
          updated_at      = now()
        """,
        cols[0], cols[1], cols[2], cols[3], cols[4], cols[5], cols[6], cols[7], cols[8],
    )
    log.info("[CONF_OCC] report_id=%s rows=%d (solo+combo)", report_id, len(records))


# üî∏ UPSERT component marginals –≤ oracle_mw_component_counts (–ø–æ report_id/time_frame)
async def _upsert_component_counts(
    conn,
    report_id: int,
    time_frame: str,
    comp_map: Dict[Tuple[str, str, str, str], int],
    denom_map: Dict[Tuple[str, str], set],
):
    records = []
    for (direction, timeframe), uids in denom_map.items():
        total_all = len(uids)
        for (d, tf, comp, cstate), cnt in comp_map.items():
            if d == direction and tf == timeframe:
                records.append((
                    report_id, direction, timeframe, time_frame,
                    comp, cstate, int(cnt), int(total_all),
                ))

    if not records:
        return

    cols = list(zip(*records))
    await conn.execute(
        """
        WITH data AS (
          SELECT
            unnest($1::bigint[]) AS report_id,
            unnest($2::text[])   AS direction,
            unnest($3::text[])   AS timeframe,
            unnest($4::text[])   AS time_frame,
            unnest($5::text[])   AS component,
            unnest($6::text[])   AS comp_state,
            unnest($7::int[])    AS positions_comp,
            unnest($8::int[])    AS positions_all
        )
        INSERT INTO oracle_mw_component_counts (
          report_id, strategy_id, direction, timeframe, time_frame,
          component, comp_state, positions_comp, positions_all
        )
        SELECT
          d.report_id, ors.strategy_id, d.direction, d.timeframe, d.time_frame,
          d.component, d.comp_state, d.positions_comp, d.positions_all
        FROM data d
        JOIN oracle_report_stat ors ON ors.id = d.report_id
        ON CONFLICT (report_id, strategy_id, direction, timeframe, time_frame, component, comp_state)
        DO UPDATE SET
          positions_comp = EXCLUDED.positions_comp,
          positions_all  = EXCLUDED.positions_all,
          updated_at     = now()
        """,
        cols[0], cols[1], cols[2], cols[3], cols[4], cols[5], cols[6], cols[7],
    )
    log.info("[CONF_COMP] report_id=%s rows=%d", report_id, len(records))


# üî∏ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ p7/p14/p28 –ø–æ –∫–ª—é—á–∞–º —Ç–µ–∫—É—â–µ–≥–æ —Ä–µ–ø–æ—Ä—Ç–∞
async def _update_window_winrates_cache(conn, mwas_rows: List[MwasRow], time_frame: str):
    if not mwas_rows:
        return

    strategy_ids, directions, timeframes, agg_types, agg_bases, agg_states = [], [], [], [], [], []
    p7, p14, p28 = [], [], []

    for r in mwas_rows:
        strategy_ids.append(r.strategy_id)
        directions.append(r.direction)
        timeframes.append(r.timeframe)
        agg_types.append(r.agg_type)
        agg_bases.append(r.agg_base)
        agg_states.append(r.agg_state)
        if time_frame == "7d":
            p7.append(r.winrate); p14.append(None); p28.append(None)
        elif time_frame == "14d":
            p7.append(None); p14.append(r.winrate); p28.append(None)
        else:
            p7.append(None); p14.append(None); p28.append(r.winrate)

    await conn.execute(
        """
        WITH data AS (
          SELECT
            unnest($1::int[])  AS strategy_id,
            unnest($2::text[]) AS direction,
            unnest($3::text[]) AS timeframe,
            unnest($4::text[]) AS agg_type,
            unnest($5::text[]) AS agg_base,
            unnest($6::text[]) AS agg_state,
            unnest($7::numeric[]) AS p7_in,
            unnest($8::numeric[]) AS p14_in,
            unnest($9::numeric[]) AS p28_in
        )
        INSERT INTO oracle_mw_window_winrates_cache (
          strategy_id, direction, timeframe, agg_type, agg_base, agg_state,
          p7, p14, p28, windows_available
        )
        SELECT
          d.strategy_id, d.direction, d.timeframe, d.agg_type, d.agg_base, d.agg_state,
          d.p7_in, d.p14_in, d.p28_in,
          ((d.p7_in IS NOT NULL)::int + (d.p14_in IS NOT NULL)::int + (d.p28_in IS NOT NULL)::int)
        FROM data d
        ON CONFLICT (strategy_id, direction, timeframe, agg_type, agg_base, agg_state)
        DO UPDATE SET
          p7 = COALESCE(EXCLUDED.p7, oracle_mw_window_winrates_cache.p7),
          p14 = COALESCE(EXCLUDED.p14, oracle_mw_window_winrates_cache.p14),
          p28 = COALESCE(EXCLUDED.p28, oracle_mw_window_winrates_cache.p28),
          windows_available =
            ((COALESCE(EXCLUDED.p7, oracle_mw_window_winrates_cache.p7)  IS NOT NULL)::int +
             (COALESCE(EXCLUDED.p14, oracle_mw_window_winrates_cache.p14) IS NOT NULL)::int +
             (COALESCE(EXCLUDED.p28, oracle_mw_window_winrates_cache.p28) IS NOT NULL)::int),
          updated_at = now()
        """,
        strategy_ids, directions, timeframes, agg_types, agg_bases, agg_states, p7, p14, p28,
    )
    log.info("[CONF_WINCACHE] rows=%d updated (win=%s)", len(mwas_rows), time_frame)


# üî∏ –ß—Ç–µ–Ω–∏–µ –∫—ç—à–∞ p7/p14/p28 –¥–ª—è –≤—Å–µ—Ö –∫–ª—é—á–µ–π —Ç–µ–∫—É—â–µ–≥–æ –æ—Ç—á—ë—Ç–∞
async def _fetch_window_cache_map(conn, mwas_rows: List[MwasRow]) -> Dict[Tuple[int, str, str, str, str, str], Tuple[Optional[float], Optional[float], Optional[float], int]]:
    if not mwas_rows:
        return {}

    # —Å–æ–±–µ—Ä—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏
    keys = {(r.strategy_id, r.direction, r.timeframe, r.agg_type, r.agg_base, r.agg_state) for r in mwas_rows}
    if not keys:
        return {}

    # —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –ø–æ –º–∞—Å—Å–∏–≤–∞–º –¥–ª—è UNNEST JOIN
    sid, d, tf, at, ab, as_ = zip(*keys)

    rows = await conn.fetch(
        """
        WITH k AS (
          SELECT
            unnest($1::int[])  AS strategy_id,
            unnest($2::text[]) AS direction,
            unnest($3::text[]) AS timeframe,
            unnest($4::text[]) AS agg_type,
            unnest($5::text[]) AS agg_base,
            unnest($6::text[]) AS agg_state
        )
        SELECT k.strategy_id, k.direction, k.timeframe, k.agg_type, k.agg_base, k.agg_state,
               w.p7, w.p14, w.p28, w.windows_available
          FROM k
          LEFT JOIN oracle_mw_window_winrates_cache w
            ON (w.strategy_id = k.strategy_id AND w.direction = k.direction AND w.timeframe = k.timeframe
                AND w.agg_type = k.agg_type AND w.agg_base = k.agg_base AND w.agg_state = k.agg_state)
        """,
        list(sid), list(d), list(tf), list(at), list(ab), list(as_),
    )

    out: Dict[Tuple[int, str, str, str, str, str], Tuple[Optional[float], Optional[float], Optional[float], int]] = {}
    for r in rows:
        key = (int(r["strategy_id"]), str(r["direction"]), str(r["timeframe"]),
               str(r["agg_type"]), str(r["agg_base"]), str(r["agg_state"]))
        out[key] = (
            (float(r["p7"]) if r["p7"] is not None else None),
            (float(r["p14"]) if r["p14"] is not None else None),
            (float(r["p28"]) if r["p28"] is not None else None),
            int(r["windows_available"] or 0),
        )
    return out


# üî∏ SD-—Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã –¥–ª—è q_window (–ø–æ –∫–æ–≥–æ—Ä—Ç–∞–º) –∏–∑ –∫—ç—à–∞ –æ–∫–æ–Ω
def _build_sd_references_from_cache(
    mwas_rows: List[MwasRow],
    cache_map: Dict[Tuple[int, str, str, str, str, str], Tuple[Optional[float], Optional[float], Optional[float], int]],
) -> Dict[Tuple[int, str, str], List[float]]:
    # cohort_key = (strategy_id, direction, timeframe)
    sd_refs: Dict[Tuple[int, str, str], List[float]] = defaultdict(list)

    # —Å–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–æ–≥–æ—Ä—Ç–µ –∏ –ø–æ—Å—á–∏—Ç–∞–µ–º sd –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º p7/p14/p28 –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞
    for r in mwas_rows:
        k = (r.strategy_id, r.direction, r.timeframe)
        t = cache_map.get((r.strategy_id, r.direction, r.timeframe, r.agg_type, r.agg_base, r.agg_state))
        if not t:
            continue
        p7, p14, p28, _ = t
        vals = [x for x in (p7, p14, p28) if isinstance(x, (int, float))]
        if len(vals) < 2:
            continue
        mean = sum(vals) / len(vals)
        sd = math.sqrt(sum((v - mean) ** 2 for v in vals) / len(vals))
        sd_refs[k].append(sd)

    return sd_refs


# üî∏ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–æ–ª–µ–π –¥–ª—è q_scale (–ø–æ –∫–æ–≥–æ—Ä—Ç–∞–º) ‚Äî –≤–∫–ª—é—á–∞–µ—Ç SOLO –∏ COMBO
def _build_cohort_distributions(
    mwas_rows: List[MwasRow],
    occ_solo: Dict[Tuple[str, str, str, str], int],
    occ_combo: Dict[Tuple[str, str, str, str], int],
    denom_map: Dict[Tuple[str, str], set],
) -> Dict[Tuple[int, str, str, str], List[float]]:
    dist_map: Dict[Tuple[int, str, str, str], List[float]] = defaultdict(list)
    for r in mwas_rows:
        kpos = (r.strategy_id, r.direction, r.timeframe, r.time_frame)
        N_all = len(denom_map.get((r.direction, r.timeframe), set()))
        if N_all <= 0:
            continue
        if r.agg_type == "solo":
            N_s = int(occ_solo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
        else:
            N_s = int(occ_combo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
        s = (N_s / N_all) if N_all > 0 else 0.0
        dist_map[kpos].append(s)
    return dist_map


# üî∏ –†–∞—Å—á—ë—Ç confidence –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ –æ—Ç—á—ë—Ç–∞
def _compute_confidence_items(
    mwas_rows: List[MwasRow],
    occ_solo: Dict[Tuple[str, str, str, str], int],
    occ_combo: Dict[Tuple[str, str, str, str], int],
    denom_map: Dict[Tuple[str, str], set],
    comp_map: Dict[Tuple[str, str, str, str], int],
    cache_map: Dict[Tuple[int, str, str, str, str, str], Tuple[Optional[float], Optional[float], Optional[float], int]],
    sd_refs: Dict[Tuple[int, str, str], List[float]],
) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []

    for r in mwas_rows:
        N_all = len(denom_map.get((r.direction, r.timeframe), set()))
        if r.agg_type == "solo":
            N_s = int(occ_solo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
            p_joint = (N_s / N_all) if N_all > 0 else 0.0
            p_marginals: List[float] = []  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è solo
        else:
            N_s = int(occ_combo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
            p_joint = (N_s / N_all) if N_all > 0 else 0.0
            # –º–∞—Ä–≥–∏–Ω–∞–ª–∏ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º –∏–∑ comp_map
            pairs = parse_combo(r.agg_state)
            p_marginals = []
            for c, s in pairs:
                positions_comp = int(comp_map.get((r.direction, r.timeframe, c, s), 0))
                p_marginals.append((positions_comp / N_all) if N_all > 0 else 0.0)

        # q-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        q1 = compute_q_ci_result(r.trades_wins, r.trades_total)
        q2 = compute_q_ci_occurrence(N_s, N_all)

        p7, p14, p28, _wa = cache_map.get((r.strategy_id, r.direction, r.timeframe, r.agg_type, r.agg_base, r.agg_state), (None, None, None, 0))
        q3 = compute_q_window(
            p7, p14, p28,
            cohort_key=(r.strategy_id, r.direction, r.timeframe),
            sd_reference=sd_refs.get((r.strategy_id, r.direction, r.timeframe)),
        )

        # –¥–ª—è q_scale –≤–æ–∑—å–º—ë–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–æ–≥–æ—Ä—Ç–µ ‚Äî –æ–Ω–æ —Å—Ç—Ä–æ–∏—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –∏ –Ω–µ –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —Å—é–¥–∞ –ø–æ –ø–∞–º—è—Ç–∏,
        # –ø–æ—Å–∫–æ–ª—å–∫—É –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–≥–æ s –º—ã —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ N_s/N_all –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å –ø–æ—Å—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ:
        # –æ–¥–Ω–∞–∫–æ, —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å dist_map –µ—â—ë —Ä–∞–∑, –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π: percentile –ø–æ (solo+combo) —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –Ω–∏–∂–µ
        # –∑–¥–µ—Å—å –æ—Å—Ç–∞–≤–∏–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ; —Ä–µ–∞–ª—å–Ω—ã–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –ø–æ—Å—á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ compute_q_scale_percentile(...)
        # –Ω–æ —á—Ç–æ–±—ã —Å–æ–±–ª—é—Å—Ç–∏ –Ω–∞—à –¥–æ–≥–æ–≤–æ—Ä, –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ–º percentile –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ s –Ω–∞ –ª–µ—Ç—É –∏–∑ –≤—Å–µ–π –∫–æ–≥–æ—Ä—Ç—ã:
        # ‚Üí —Å–æ–±–µ—Ä—ë–º s_list –∫–æ–≥–æ—Ä—Ç—ã –æ–¥–∏–Ω —Ä–∞–∑ –≤–Ω–µ —Ü–∏–∫–ª–∞ (—ç—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ _build_cohort_distributions) –∏ –ø–µ—Ä–µ–¥–∞–¥–∏–º —Å—é–¥–∞! (—Å–º. –≤—ã—à–µ)
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: q_scale –≤—ã—á–∏—Å–ª–∏–º –∑–¥–µ—Å—å —á–µ—Ä–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–∏–µ—Ü:
        #   ‚Äî —á—Ç–æ–±—ã –Ω–µ –≥–æ—Ä–æ–¥–∏—Ç—å –µ—â—ë –æ–¥–∏–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä, –ø–µ—Ä–µ—Å—á—ë—Ç –¥–µ–ª–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∏–∂–µ, –≤—ã–∑—ã–≤–∞—é—â–∏–π –∫–æ–¥ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª dist_map.
        # (—Å–º. –ø—Ä–∞–≤–∫—É –Ω–∏–∂–µ)
        pass

    # —ç—Ç–æ—Ç –±–ª–æ–∫ –∑–∞–º–µ–Ω—ë–Ω –±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π –Ω–∏–∂–µ; –æ—Å—Ç–∞–≤–ª–µ–Ω –∫–∞–∫ –ø–æ—è—Å–Ω–µ–Ω–∏–µ
    return _compute_confidence_items_with_scale(
        mwas_rows, occ_solo, occ_combo, denom_map, comp_map, cache_map, sd_refs
    )


# –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ä–∞—Å—á—ë—Ç–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º q_scale –ø–æ –ø—Ä–µ–¥—Å–æ–±—Ä–∞–Ω–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
def _compute_confidence_items_with_scale(
    mwas_rows: List[MwasRow],
    occ_solo: Dict[Tuple[str, str, str, str], int],
    occ_combo: Dict[Tuple[str, str, str, str], int],
    denom_map: Dict[Tuple[str, str], set],
    comp_map: Dict[Tuple[str, str, str, str], int],
    cache_map: Dict[Tuple[int, str, str, str, str, str], Tuple[Optional[float], Optional[float], Optional[float], int]],
    sd_refs: Dict[Tuple[int, str, str], List[float]],
) -> List[Dict[str, Any]]:
    # —Å–æ–±–µ—Ä—ë–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è s –ø–æ –∫–æ–≥–æ—Ä—Ç–∞–º –¥–ª—è q_scale
    dist_map = _build_cohort_distributions(mwas_rows, occ_solo, occ_combo, denom_map)

    items: List[Dict[str, Any]] = []
    for r in mwas_rows:
        N_all = len(denom_map.get((r.direction, r.timeframe), set()))
        if r.agg_type == "solo":
            N_s = int(occ_solo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
            p_joint = (N_s / N_all) if N_all > 0 else 0.0
            p_marginals: List[float] = []
        else:
            N_s = int(occ_combo.get((r.direction, r.timeframe, r.agg_base, r.agg_state), 0))
            p_joint = (N_s / N_all) if N_all > 0 else 0.0
            pairs = parse_combo(r.agg_state)
            p_marginals = []
            for c, s in pairs:
                positions_comp = int(comp_map.get((r.direction, r.timeframe, c, s), 0))
                p_marginals.append((positions_comp / N_all) if N_all > 0 else 0.0)

        q1 = compute_q_ci_result(r.trades_wins, r.trades_total)
        q2 = compute_q_ci_occurrence(N_s, N_all)

        p7, p14, p28, _wa = cache_map.get((r.strategy_id, r.direction, r.timeframe, r.agg_type, r.agg_base, r.agg_state), (None, None, None, 0))
        q3 = compute_q_window(
            p7, p14, p28,
            cohort_key=(r.strategy_id, r.direction, r.timeframe),
            sd_reference=sd_refs.get((r.strategy_id, r.direction, r.timeframe)),
        )

        q4 = compute_q_scale(
            positions_state=N_s,
            positions_all=N_all,
            distribution_in_cohort=dist_map.get((r.strategy_id, r.direction, r.timeframe, r.time_frame), []),
        )

        q_list: List[Optional[float]] = [q1, q2, q3, q4]
        q5 = None
        if r.agg_type == "combo":
            q5 = compute_q_npmi(p_joint, p_marginals)
            q_list.append(q5)

        score = compute_confidence(q_list)

        meta = {
            "q_CI_result": q1,
            "q_CI_occurrence": q2,
            "q_window": q3,
            "q_scale": q4,
            "wins": r.trades_wins,
            "trades": r.trades_total,
            "positions_state": N_s,
            "positions_all": N_all,
            "windows_available": int((p7 is not None) + (p14 is not None) + (p28 is not None)),
        }
        if r.agg_type == "combo":
            meta["q_npmi"] = q5

        items.append({
            "id": r.id,
            "confidence_score": round(float(score), 2),
            "confidence_meta": json.dumps(meta, separators=(",", ":")),
            "complexity_level": compute_complexity_level(r.agg_type, r.agg_state),
        })

    return items


# üî∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π confidence_* –≤ oracle_mw_aggregated_stat
async def _persist_confidence_items(conn, items: List[Dict[str, Any]]):
    if not items:
        return
    for i in range(0, len(items), BATCH_SIZE_UPDATE):
        batch = items[i:i + BATCH_SIZE_UPDATE]
        ids = [it["id"] for it in batch]
        scores = [it["confidence_score"] for it in batch]
        metas = [it["confidence_meta"] for it in batch]
        levels = [it["complexity_level"] for it in batch]

        await conn.execute(
            """
            WITH data AS (
              SELECT
                unnest($1::bigint[]) AS id,
                unnest($2::numeric[]) AS confidence_score,
                unnest($3::jsonb[])   AS confidence_meta,
                unnest($4::int[])     AS complexity_level
            )
            UPDATE oracle_mw_aggregated_stat m
               SET confidence_score = d.confidence_score,
                   confidence_meta  = d.confidence_meta,
                   complexity_level = d.complexity_level,
                   updated_at       = now()
              FROM data d
             WHERE m.id = d.id
            """,
            ids, scores, metas, levels,
        )
    log.info("[CONF_PERSIST] updated rows=%d", len(items))


# üî∏ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è KV —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º confidence_score/complexity_level (TTL 8h)
async def _publish_final_kv(conn, redis, report_id: int, strategy_id: int, time_frame: str) -> int:
    row_rep = await conn.fetchrow("SELECT closed_total FROM oracle_report_stat WHERE id = $1", report_id)
    if not row_rep:
        return 0
    closed_total = int(row_rep["closed_total"] or 0)

    rows = await conn.fetch(
        """
        SELECT DISTINCT ON (direction, timeframe, agg_base, agg_state)
               direction, timeframe, agg_base, agg_state, trades_total, winrate,
               COALESCE(confidence_score, 0)::numeric(5,2) AS confidence_score,
               COALESCE(complexity_level, 1) AS complexity_level
          FROM oracle_mw_aggregated_stat
         WHERE report_id = $1
         ORDER BY direction, timeframe, agg_base, agg_state, updated_at DESC
        """,
        report_id,
    )
    if not rows:
        return 0

    pipe = redis.pipeline()
    published = 0
    for r in rows:
        direction = r["direction"]
        timeframe = r["timeframe"]
        agg_base = r["agg_base"]
        agg_state = r["agg_state"]
        trades_total = int(r["trades_total"] or 0)
        winrate = float(r["winrate"] or 0.0)
        confidence_score = float(r["confidence_score"] or 0.0)
        complexity_level = int(r["complexity_level"] or 1)

        key = f"oracle:mw:{strategy_id}:{direction}:{timeframe}:{agg_base}:{agg_state}:{time_frame}"
        payload = {
            "strategy_id": strategy_id,
            "direction": direction,
            "timeframe": timeframe,
            "agg_base": agg_base,
            "agg_state": agg_state,
            "time_frame": time_frame,
            "report_id": report_id,
            "closed_total": closed_total,
            "agg_trades_total": trades_total,
            "winrate": f"{winrate:.4f}",
            "confidence_score": round(confidence_score, 2),
            "complexity_level": complexity_level,
        }
        pipe.set(key, str(payload), ex=FINAL_KV_TTL_SEC)
        published += 1

    await pipe.execute()
    log.info("[CONF_KV] report_id=%s published=%d ttl=%ds", report_id, published, FINAL_KV_TTL_SEC)
    return published


# üî∏ –ü–∞—Ä—Å–∏–Ω–≥ combo-—Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
def parse_combo(agg_state: str) -> List[Tuple[str, str]]:
    if not agg_state:
        return []
    pairs: List[Tuple[str, str]] = []
    parts = agg_state.split("|")
    for part in parts:
        if ":" not in part:
            continue
        comp, state = part.split(":", 1)
        comp = comp.strip()
        state = state.strip()
        if comp in MW_COMPONENTS and state:
            pairs.append((comp, state))
    return pairs


# üî∏ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ complexity_level
def compute_complexity_level(agg_type: str, agg_state: str) -> int:
    if agg_type == "solo":
        return 1
    return max(1, len(parse_combo(agg_state)))


# üî∏ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ q_CI_result ‚Äî —à–∏—Ä–∏–Ω–∞ Wilson CI –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–æ–∫
def compute_q_ci_result(wins: int, total: int, conf_level: float = 0.95) -> float:
    if total <= 0:
        return 0.0
    p = wins / total
    z = 1.959963984540054 if conf_level == 0.95 else 1.96
    denom = 1 + z**2 / total
    half_width = (z * math.sqrt(p * (1 - p) / total + z**2 / (4 * total**2))) / denom
    width = min(1.0, max(0.0, 2 * half_width))  # –ø–æ–ª–Ω–∞—è —à–∏—Ä–∏–Ω–∞ CI
    return max(0.0, 1.0 - width)


# üî∏ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ q_CI_occurrence ‚Äî —à–∏—Ä–∏–Ω–∞ Wilson CI –ø–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
def compute_q_ci_occurrence(positions_state: int, positions_all: int, conf_level: float = 0.95) -> float:
    if positions_all <= 0:
        return 0.0
    return compute_q_ci_result(positions_state, positions_all, conf_level)


# üî∏ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ q_window ‚Äî —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å winrate –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏ (data-driven —á–µ—Ä–µ–∑ CUME_DIST)
def compute_q_window(
    p7: Optional[float],
    p14: Optional[float],
    p28: Optional[float],
    cohort_key: Tuple[int, str, str],
    sd_reference: Optional[List[float]] = None,
) -> float:
    vals = [x for x in (p7, p14, p28) if isinstance(x, (int, float))]
    if len(vals) < 2:
        return 0.5  # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
    mean = sum(vals) / len(vals)
    sd = math.sqrt(sum((v - mean) ** 2 for v in vals) / len(vals))
    if not sd_reference:
        # –±–µ–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞ ‚Äî –≤–µ—Ä–Ω—ë–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –Ω–µ –º–∞–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥
        return 0.5
    # CUME_DIST –ø–æ sd —Å—Ä–µ–¥–∏ –∫–æ–≥–æ—Ä—Ç—ã
    sorted_ref = sorted(sd_reference)
    rank = 0
    for x in sorted_ref:
        if x <= sd:
            rank += 1
        else:
            break
    perc = rank / max(1, len(sorted_ref))
    return max(0.0, min(1.0, 1.0 - perc))


# üî∏ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ q_npmi ‚Äî –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å combo
def compute_q_npmi(p_joint: float, p_marginals: List[float], eps: float = 1e-12) -> float:
    p_joint = max(eps, min(1.0, p_joint))
    prod_marg = 1.0
    for x in p_marginals or [eps]:
        prod_marg *= max(eps, min(1.0, x))
    pmi = math.log(p_joint) - math.log(prod_marg)
    npmi = pmi / (-math.log(p_joint))
    return max(0.0, min(1.0, (npmi + 1.0) / 2.0))


# üî∏ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ q_scale ‚Äî –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –¥–æ–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –∫–æ–≥–æ—Ä—Ç–µ (CUME_DIST)
def compute_q_scale(positions_state: int, positions_all: int, distribution_in_cohort: List[float]) -> float:
    if positions_all <= 0:
        return 0.0
    s = positions_state / positions_all
    if not distribution_in_cohort:
        return 0.5
    cnt = sum(1 for x in distribution_in_cohort if x <= s)
    return max(0.0, min(1.0, cnt / len(distribution_in_cohort)))


# üî∏ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤ –∏—Ç–æ–≥–æ–≤—ã–π confidence (–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ, ¬´–∂—ë—Å—Ç–∫–∞—è –Ω—É–ª—ë–≤–∫–∞¬ª)
def compute_confidence(qs: List[Optional[float]]) -> float:
    vals = [q for q in qs if q is not None]
    if not vals:
        return 0.0
    if any(q <= 0.0 for q in vals):
        return 0.0
    prod = 1.0
    for q in vals:
        prod *= max(1e-12, min(1.0, float(q)))
    K = len(vals)
    return 100.0 * (prod ** (1.0 / K))