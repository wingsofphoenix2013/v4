# strategy_confidence_worker.py

import asyncio
import logging
import json
import math
from statistics import median

import infra

log = logging.getLogger("STRATEGY_CONFIDENCE_WORKER")

STREAM_NAME = "emasnapshot:ratings:commands"

# üî∏ –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã
def extract_tf_from_table_name(table: str) -> str:
    parts = table.split("_")
    if len(parts) >= 3:
        return parts[-2]
    raise ValueError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã: {table}")
    
# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä
async def run_strategy_confidence_worker():
    redis = infra.redis_client

    try:
        stream_info = await redis.xinfo_stream(STREAM_NAME)
        last_id = stream_info["last-generated-id"]
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å last ID –∏–∑ stream: {e}")
        last_id = "$"

    log.info(f"üì° –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ Redis Stream: {STREAM_NAME}")

    while True:
        try:
            response = await redis.xread(
                streams={STREAM_NAME: last_id},
                count=50,
                block=1000
            )
            for stream, messages in response:
                for msg_id, msg_data in messages:
                    parsed = {k: v for k, v in msg_data.items()}
                    asyncio.create_task(handle_message(parsed))
                    last_id = msg_id
        except Exception:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ Redis Stream")
            await asyncio.sleep(1)

# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
async def handle_message(msg: dict):
    table = msg.get("table")
    strategies_raw = msg.get("strategies")

    if not table or not strategies_raw:
        log.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg}")
        return

    try:
        strategy_ids = json.loads(strategies_raw)
        assert isinstance(strategy_ids, list)
    except Exception:
        log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π: {strategies_raw}")
        return

    log.info(f"üì© –ü—Ä–∏–Ω—è—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: table = {table}, strategies = {strategy_ids}")

    async with infra.pg_pool.acquire() as conn:
        for strategy_id in strategy_ids:
            if "emasnapshot" in table and "pattern" not in table:
                log.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ snapshot-—Ç–∞–±–ª–∏—Ü—ã: {table} | strategy_id={strategy_id}")
                await process_snapshot_confidence(conn, table, strategy_id)

            elif "emapattern" in table:
                log.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ pattern-—Ç–∞–±–ª–∏—Ü—ã: {table} | strategy_id={strategy_id}")
                await process_pattern_confidence(conn, table, strategy_id)

            else:
                log.info(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫: —Ç–∏–ø —Ç–∞–±–ª–∏—Ü—ã {table} –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

# üî∏ –†–∞—Å—á—ë—Ç –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ confidence_score V3 –¥–ª—è snapshot-—Ç–∞–±–ª–∏—Ü—ã
async def process_snapshot_confidence(conn, table: str, strategy_id: int):
    tf = extract_tf_from_table_name(table)

    rows = await conn.fetch(f"""
        SELECT strategy_id, direction, emasnapshot_dict_id, num_trades, num_wins
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    if not rows:
        log.info(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫: –Ω–µ—Ç —Å—Ç—Ä–æ–∫ –≤ {table} –¥–ª—è strategy_id={strategy_id}")
        return

    # üîπ –ì–ª–æ–±–∞–ª—å–Ω—ã–π winrate –∏ total_trades
    global_data = await conn.fetchrow(f"""
        SELECT SUM(num_wins)::float / NULLIF(SUM(num_trades), 0) AS global_winrate,
               SUM(num_trades)::int AS total_trades
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    gw = global_data["global_winrate"] or 0.0
    total = global_data["total_trades"] or 0

    # üîπ –ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (emasnapshot_dict_id)
    unique_count = await conn.fetchval(f"""
        SELECT COUNT(DISTINCT emasnapshot_dict_id)
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    vweight = min(
        total / 10,
        10 + math.log2(unique_count or 1)
    )
    alpha = gw * vweight
    beta = (1 - gw) * vweight

    trade_counts = [r["num_trades"] for r in rows]
    trade_counts.sort()

    mean = sum(trade_counts) / len(trade_counts)
    median_val = trade_counts[len(trade_counts) // 2]
    p25 = trade_counts[int(len(trade_counts) * 0.25)]

    if abs(mean - median_val) / mean < 0.1:
        threshold_n = round(0.1 * mean)
        method = "mean*0.1"
    elif median_val < mean * 0.6:
        threshold_n = max(5, round(median_val / 2))
        method = "median/2"
    else:
        threshold_n = round(p25)
        method = "percentile_25"

    fragmented = sum(1 for n in trade_counts if n < threshold_n)
    fragmentation = fragmented / len(trade_counts)
    frag_modifier = max(0.3, (1 - fragmentation) ** 0.7)

    log.info(f"üìä strategy={strategy_id} tf={tf} ‚Üí T={threshold_n} by {method}, frag={fragmentation:.3f}")

    # üîπ –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ ‚Äî —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º confidence_raw
    raw_scores = []
    objects = []

    for row in rows:
        w = row["num_wins"]
        n = row["num_trades"]
        sid = row["emasnapshot_dict_id"]
        direction = row["direction"]

        bayes_wr = (w + alpha) / (n + alpha + beta)
        score = bayes_wr * math.log(1 + n) * frag_modifier

        raw_scores.append(score)
        objects.append((sid, direction, n, w))  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞

    # üîπ –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    median_score = median(raw_scores) or 1e-6

    # üîπ –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ ‚Äî –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    for i, (score_raw) in enumerate(raw_scores):
        sid, direction, n, w = objects[i]
        score_norm = score_raw / median_score

        await conn.execute("""
            INSERT INTO strategy_confidence_log (
                strategy_id, direction, tf, object_type, object_id,
                num_trades, num_wins, global_winrate, alpha, beta,
                mean, median, percentile_25, threshold_n, threshold_method,
                fragmentation, density,
                confidence_score, confidence_raw, confidence_normalized
            ) VALUES (
                $1, $2, $3, 'snapshot', $4,
                $5, $6, $7, $8, $9,
                $10, $11, $12, $13, $14,
                $15, NULL,
                $16, $17, $18
            )
        """, strategy_id, direction, tf, sid,
             n, w, gw, alpha, beta,
             mean, median_val, p25, threshold_n, method,
             fragmentation,
             score_raw, score_raw, score_norm)

        log.debug(f"[OK] strategy={strategy_id} snapshot_id={sid} raw={score_raw:.4f} norm={score_norm:.4f}")   
# üî∏ –†–∞—Å—á—ë—Ç –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ confidence_score V3 –¥–ª—è pattern-—Ç–∞–±–ª–∏—Ü—ã
async def process_pattern_confidence(conn, table: str, strategy_id: int):
    tf = extract_tf_from_table_name(table)

    rows = await conn.fetch(f"""
        SELECT strategy_id, direction, pattern_id, num_trades, num_wins
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    if not rows:
        log.info(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫: –Ω–µ—Ç —Å—Ç—Ä–æ–∫ –≤ {table} –¥–ª—è strategy_id={strategy_id}")
        return

    global_data = await conn.fetchrow(f"""
        SELECT SUM(num_wins)::float / NULLIF(SUM(num_trades), 0) AS global_winrate,
               SUM(num_trades)::int AS total_trades
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    gw = global_data["global_winrate"] or 0.0
    total = global_data["total_trades"] or 0

    unique_count = await conn.fetchval(f"""
        SELECT COUNT(DISTINCT pattern_id)
        FROM {table}
        WHERE strategy_id = $1
    """, strategy_id)

    vweight = min(
        total / 10,
        10 + math.log2(unique_count or 1)
    )
    alpha = gw * vweight
    beta = (1 - gw) * vweight

    trade_counts = [r["num_trades"] for r in rows]
    trade_counts.sort()

    mean = sum(trade_counts) / len(trade_counts)
    median_val = trade_counts[len(trade_counts) // 2]
    p25 = trade_counts[int(len(trade_counts) * 0.25)]

    if abs(mean - median_val) / mean < 0.1:
        threshold_n = round(0.1 * mean)
        method = "mean*0.1"
    elif median_val < mean * 0.6:
        threshold_n = max(5, round(median_val / 2))
        method = "median/2"
    else:
        threshold_n = round(p25)
        method = "percentile_25"

    fragmented = sum(1 for n in trade_counts if n < threshold_n)
    fragmentation = fragmented / len(trade_counts)
    frag_modifier = max(0.3, (1 - fragmentation) ** 0.7)

    log.info(f"üìä strategy={strategy_id} tf={tf} (pattern) ‚Üí T={threshold_n} by {method}, frag={fragmentation:.3f}")

    # üîπ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    snap_table = table.replace("pattern", "snapshot")

    density_rows = await conn.fetch(f"""
        WITH dict AS (
            SELECT id AS snapshot_id, pattern_id
            FROM oracle_emasnapshot_dict
            WHERE pattern_id IS NOT NULL
        ),
        joined AS (
            SELECT d.pattern_id, s.num_trades
            FROM dict d
            JOIN {snap_table} s ON s.emasnapshot_dict_id = d.snapshot_id
            WHERE s.strategy_id = $1
        ),
        agg AS (
            SELECT pattern_id,
                   COUNT(*) AS num_snaps,
                   SUM(num_trades)::float AS total_trades
            FROM joined
            GROUP BY pattern_id
        ),
        max_avg AS (
            SELECT MAX(total_trades / NULLIF(num_snaps, 0)) AS max_density
            FROM agg
        )
        SELECT
            a.pattern_id,
            a.total_trades / NULLIF(a.num_snaps, 0) AS avg_density,
            a.total_trades / NULLIF(a.num_snaps * m.max_density, 0) AS norm_density
        FROM agg a, max_avg m
    """, strategy_id)

    density_lookup = {r["pattern_id"]: r["norm_density"] for r in density_rows}

    # üîπ –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ ‚Äî confidence_raw
    raw_scores = []
    objects = []

    for row in rows:
        w = row["num_wins"]
        n = row["num_trades"]
        pid = row["pattern_id"]
        direction = row["direction"]

        if pid not in density_lookup:
            continue

        bayes_wr = (w + alpha) / (n + alpha + beta)
        score = bayes_wr * math.log(1 + n) * frag_modifier * density_lookup[pid]

        raw_scores.append(score)
        objects.append((pid, direction, n, w, density_lookup[pid]))

    if not raw_scores:
        log.warning(f"‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç—Ä–æ–∫ —Å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é –¥–ª—è strategy_id={strategy_id}")
        return

    median_score = median(raw_scores) or 1e-6

    # üîπ –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ ‚Äî –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    for i, score_raw in enumerate(raw_scores):
        pid, direction, n, w, density = objects[i]
        score_norm = score_raw / median_score

        await conn.execute("""
            INSERT INTO strategy_confidence_log (
                strategy_id, direction, tf, object_type, object_id,
                num_trades, num_wins, global_winrate, alpha, beta,
                mean, median, percentile_25, threshold_n, threshold_method,
                fragmentation, density,
                confidence_score, confidence_raw, confidence_normalized
            ) VALUES (
                $1, $2, $3, 'pattern', $4,
                $5, $6, $7, $8, $9,
                $10, $11, $12, $13, $14,
                $15, $16,
                $17, $18, $19
            )
        """, strategy_id, direction, tf, pid,
             n, w, gw, alpha, beta,
             mean, median_val, p25, threshold_n, method,
             fragmentation, density,
             score_raw, score_raw, score_norm)

        log.debug(f"[OK] strategy={strategy_id} pattern_id={pid} raw={score_raw:.4f} norm={score_norm:.4f}")
# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä
async def run_strategy_confidence_worker():
    redis = infra.redis_client

    try:
        stream_info = await redis.xinfo_stream(STREAM_NAME)
        last_id = stream_info["last-generated-id"]
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å last ID –∏–∑ stream: {e}")
        last_id = "$"

    log.info(f"üì° –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ Redis Stream: {STREAM_NAME}")

    while True:
        try:
            response = await redis.xread(
                streams={STREAM_NAME: last_id},
                count=50,
                block=1000
            )
            for stream, messages in response:
                for msg_id, msg_data in messages:
                    parsed = {k: v for k, v in msg_data.items()}
                    asyncio.create_task(handle_message(parsed))
                    last_id = msg_id
        except Exception:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ Redis Stream")
            await asyncio.sleep(1)