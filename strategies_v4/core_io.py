# core_io.py

import asyncio
import logging
from datetime import datetime
from infra import infra

# üî∏ –õ–æ–≥–≥–µ—Ä –¥–ª—è I/O-–æ–ø–µ—Ä–∞—Ü–∏–π
log = logging.getLogger("CORE_IO")

# üî∏ –í–æ—Ä–∫–µ—Ä: –∑–∞–ø–∏—Å—å –ª–æ–≥–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Consumer Group
async def run_signal_log_writer():
    stream_name = "signal_log_queue"
    group_name = "core_io_group"
    consumer_name = "core_io_1"
    buffer_limit = 100
    flush_interval_sec = 1.0
    redis = infra.redis_client
    pg = infra.pg_pool

    # üîπ –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –≥—Ä—É–ø–ø—É (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ)
    try:
        await redis.xgroup_create(stream_name, group_name, id="$", mkstream=True)
        log.info(f"üîß –ì—Ä—É–ø–ø–∞ {group_name} —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è {stream_name}")
    except Exception as e:
        if "BUSYGROUP" in str(e):
            log.info(f"‚ÑπÔ∏è –ì—Ä—É–ø–ø–∞ {group_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        else:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Consumer Group")
            return

    log.info(f"üì° –ü–æ–¥–ø–∏—Å–∫–∞ —á–µ—Ä–µ–∑ Consumer Group: {stream_name} ‚Üí {group_name}")

    while True:
        try:
            entries = await redis.xreadgroup(
                groupname=group_name,
                consumername=consumer_name,
                streams={stream_name: ">"},
                count=buffer_limit,
                block=int(flush_interval_sec * 1000)
            )

            if not entries:
                continue

            for stream_key, records in entries:
                buffer = []
                ack_ids = []

                for record_id, data in records:
                    try:
                        buffer.append(_parse_signal_log_data(data))
                        ack_ids.append(record_id)
                    except Exception:
                        log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ (id={record_id})")

                if buffer:
                    await write_log_entry_batch(buffer)
                    for rid in ack_ids:
                        await redis.xack(stream_name, group_name, rid)

        except Exception:
            log.exception("‚ùå –û—à–∏–±–∫–∞ –≤ loop Consumer Group")
            await asyncio.sleep(5)  
# üî∏ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Redis Stream
def _parse_signal_log_data(data: dict) -> tuple:
    return (
        data["log_uid"],
        int(data["strategy_id"]),
        data["status"],
        data.get("note"),
        data.get("position_uid"),
        datetime.fromisoformat(data["logged_at"])
    )

# üî∏ –ë–∞—Ç—á-–∑–∞–ø–∏—Å—å –ª–æ–≥–æ–≤ –≤ PostgreSQL
async def write_log_entry_batch(batch: list[tuple]):
    if not batch:
        return

    try:
        await infra.pg_pool.executemany(
            '''
            INSERT INTO signal_log_entries_v4 (
                log_uid, strategy_id, status, note, position_uid, logged_at
            )
            VALUES ($1, $2, $3, $4, $5, $6)
            ''',
            batch
        )
        log.info(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ –ª–æ–≥–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(batch)}")
    except Exception:
        log.exception("‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –ë–î")