# feed_and_aggregate.py ‚Äî –ø—Ä–∏—ë–º –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

import logging
import asyncio
import websockets
import json
import aiohttp
from infra import info_log
from decimal import Decimal, ROUND_DOWN
import time
from datetime import datetime, timedelta

# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤, —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç—É—Å–∞ –∏–∑ PostgreSQL
async def load_all_tickers(pg_pool):
    async with pg_pool.acquire() as conn:
        rows = await conn.fetch("""
            SELECT symbol, precision_price, status FROM tickers_v4
        """)
        tickers = {}
        active = set()
        for row in rows:
            tickers[row['symbol']] = row['precision_price']
            if row['status'] == 'enabled':
                active.add(row['symbol'].lower())
        return tickers, active
# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π –≤–∫–ª—é—á–µ–Ω–∏—è/–æ—Ç–∫–ª—é—á–µ–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ Redis Stream
async def handle_ticker_events(redis, state, pg, refresh_queue):
    group = "aggregator_group"
    stream = "tickers_status_stream"
    logger = logging.getLogger("TICKER_STREAM")

    try:
        await redis.xgroup_create(stream, group, id="0", mkstream=True)
    except Exception:
        pass  # –≥—Ä—É–ø–ø–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

    while True:
        resp = await redis.xreadgroup(group, "aggregator", streams={stream: ">"}, count=50, block=1000)
        for _, messages in resp:
            for msg_id, data in messages:
                symbol = data.get("symbol")
                action = data.get("action")
                if not symbol or not action:
                    continue

                if symbol not in state["tickers"]:
                    async with pg.acquire() as conn:
                        row = await conn.fetchrow("""
                            SELECT precision_price FROM tickers_v4 WHERE symbol = $1
                        """, symbol)
                        if row:
                            state["tickers"][symbol] = row["precision_price"]
                            info_log("TICKER_STREAM", f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–∏–∫–µ—Ä –∏–∑ –ë–î: {symbol}")

                if action == "enabled" and symbol in state["tickers"]:
                    logger.info(f"–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —Ç–∏–∫–µ—Ä: {symbol}")
                    state["active"].add(symbol.lower())
                    await refresh_queue.put("refresh")

                    # –∑–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ markPrice –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
                    precision = state["tickers"][symbol]
                    task = asyncio.create_task(watch_mark_price(symbol, redis, precision))
                    state["markprice_tasks"][symbol] = task

                elif action == "disabled" and symbol in state["tickers"]:
                    logger.info(f"–û—Ç–∫–ª—é—á—ë–Ω —Ç–∏–∫–µ—Ä: {symbol}")
                    state["active"].discard(symbol.lower())
                    await refresh_queue.put("refresh")

                    # –æ—Ç–º–µ–Ω–∞ –ø–æ—Ç–æ–∫–∞ markPrice, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    task = state["markprice_tasks"].pop(symbol, None)
                    if task:
                        task.cancel()

                await redis.xack(stream, group, msg_id)
# üî∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Å–≤–µ—á–∏ M1 –≤ RedisJSON
async def store_and_publish_m1(redis, symbol, open_time, kline, precision):

    logger = logging.getLogger("KLINE")
    timestamp = int(open_time.timestamp() * 1000)
    json_key = f"ohlcv:{symbol.lower()}:m1:{timestamp}"

    def r(val):
        return float(Decimal(val).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))

    candle = {
        "o": r(kline["o"]),
        "h": r(kline["h"]),
        "l": r(kline["l"]),
        "c": r(kline["c"]),
        "v": r(kline["v"]),
        "ts": timestamp
    }

    await redis.execute_command("JSON.SET", json_key, "$", str(candle).replace("'", '"'))
    logger.info(f"[{symbol}] M1 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞: {open_time} ‚Üí C={candle['c']}")
    # –≤—ã–∑–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ M5
    await try_aggregate_m5(redis, symbol, open_time)
    # –≤—ã–∑–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ M15
    await try_aggregate_m15(redis, symbol, open_time)
# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è M5 –Ω–∞ –æ—Å–Ω–æ–≤–µ RedisJSON M1-—Å–≤–µ—á–µ–π
async def try_aggregate_m5(redis, symbol, open_time):

    logger = logging.getLogger("KLINE")

    if open_time.minute % 5 != 4:
        return

    end_ts = int(open_time.timestamp() * 1000)
    ts_list = [end_ts - 60_000 * i for i in reversed(range(5))]
    candles = []

    for ts in ts_list:
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        try:
            data = await redis.execute_command("JSON.GET", key, "$")
            if not data:
                logger.warning(f"[{symbol}] M5: –ø—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞ {ts}")
                return
            parsed = json.loads(data)[0]
            candles.append(parsed)
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –¥–ª—è M5: {e}")
            return

    o = candles[0]["o"]
    h = max(c["h"] for c in candles)
    l = min(c["l"] for c in candles)
    c = candles[-1]["c"]
    v = sum(c["v"] for c in candles)
    m5_ts = ts_list[0]

    key = f"ohlcv:{symbol.lower()}:m5:{m5_ts}"
    candle = { "o": o, "h": h, "l": l, "c": c, "v": v, "ts": m5_ts }
    await redis.execute_command("JSON.SET", key, "$", str(candle).replace("'", '"'))

    logger.info(f"[{symbol}] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ M5: {open_time.replace(second=0)} ‚Üí O:{o} H:{h} L:{l} C:{c}")
# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è M15 –Ω–∞ –æ—Å–Ω–æ–≤–µ RedisJSON M1-—Å–≤–µ—á–µ–π
async def try_aggregate_m15(redis, symbol, open_time):

    logger = logging.getLogger("KLINE")

    if open_time.minute % 15 != 14:
        return

    end_ts = int(open_time.timestamp() * 1000)
    ts_list = [end_ts - 60_000 * i for i in reversed(range(15))]
    candles = []

    for ts in ts_list:
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        try:
            data = await redis.execute_command("JSON.GET", key, "$")
            if not data:
                logger.warning(f"[{symbol}] M15: –ø—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞ {ts}")
                return
            parsed = json.loads(data)[0]
            candles.append(parsed)
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –¥–ª—è M15: {e}")
            return

    o = candles[0]["o"]
    h = max(c["h"] for c in candles)
    l = min(c["l"] for c in candles)
    c = candles[-1]["c"]
    v = sum(c["v"] for c in candles)
    m15_ts = ts_list[0]

    key = f"ohlcv:{symbol.lower()}:m15:{m15_ts}"
    candle = { "o": o, "h": h, "l": l, "c": c, "v": v, "ts": m15_ts }
    await redis.execute_command("JSON.SET", key, "$", str(candle).replace("'", '"'))

    logger.info(f"[{symbol}] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ M15: {open_time.replace(second=0)} ‚Üí O:{o} H:{h} L:{l} C:{c}")
# üî∏ –ü–æ–∏—Å–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö M1 –∏ –∑–∞–ø–∏—Å—å –≤ missing_m1_log_v4 + system_log_v4
async def detect_missing_m1(redis, pg, symbol, now_ts):
    logger = logging.getLogger("RECOVERY")
    missing = []

    for i in range(1, 5):  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 –º–∏–Ω—É—Ç—ã
        ts = now_ts - i * 60 * 1000
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        exists = await redis.exists(key)
        if not exists:
            missing.append(ts)

    async with pg.acquire() as conn:
        for ts in missing:
            open_time = datetime.utcfromtimestamp(ts / 1000)
            try:
                await conn.execute(
                    "INSERT INTO missing_m1_log_v4 (symbol, open_time) VALUES ($1, $2) ON CONFLICT DO NOTHING",
                    symbol, open_time
                )
                await conn.execute(
                    "INSERT INTO system_log_v4 (module, level, message, details) VALUES ($1, $2, $3, $4)",
                    "AGGREGATOR", "WARNING", "M1 missing", 
                    json.dumps({"symbol": symbol, "open_time": str(open_time)})
                )
                logger.warning(f"[{symbol}] –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞: {open_time}")
            except Exception as e:
                logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –ø—Ä–æ–ø—É—Å–∫–∞: {e}")
# üî∏ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–π M1 —Å–≤–µ—á–∏ —á–µ—Ä–µ–∑ Binance API
async def restore_missing_m1(symbol, open_time, redis, pg, precision):

    logger = logging.getLogger("RECOVERY")
    ts = int(open_time.timestamp() * 1000)
    end_ts = ts + 60_000

    url = f"https://fapi.binance.com/fapi/v1/klines"
    params = {
        "symbol": symbol.upper(),
        "interval": "1m",
        "startTime": ts,
        "endTime": end_ts,
        "limit": 1
    }

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as resp:
                if resp.status != 200:
                    logger.error(f"[{symbol}] Binance API error: {resp.status}")
                    return False
                raw = await resp.json()
                if not raw:
                    logger.warning(f"[{symbol}] Binance API –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                    return False

                data = raw[0]
                o, h, l, c, v = data[1:6]
                timestamp = int(data[0])

                def r(val):
                    return float(Decimal(val).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))

                candle = {
                    "o": r(o),
                    "h": r(h),
                    "l": r(l),
                    "c": r(c),
                    "v": r(v),
                    "ts": timestamp,
                    "fixed": True
                }

                key = f"ohlcv:{symbol.lower()}:m1:{timestamp}"
                await redis.execute_command("JSON.SET", key, "$", str(candle).replace("'", '"'))

                async with pg.acquire() as conn:
                    await conn.execute(
                        "UPDATE missing_m1_log_v4 SET fixed = true, fixed_at = NOW() WHERE symbol = $1 AND open_time = $2",
                        symbol, open_time
                    )
                    await conn.execute(
                        "INSERT INTO system_log_v4 (module, level, message, details) VALUES ($1, $2, $3, $4)",
                        "AGGREGATOR", "WARNING", "M1 missing", 
                        json.dumps({"symbol": symbol, "open_time": str(open_time)})
                    )
                logger.info(f"[{symbol}] –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ M1: {open_time}")
                return True

    except Exception as e:
        logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ API: {e}", exc_info=True)
        return False
# üî∏ –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –∏–∑ missing_m1_log_v4
async def restore_missing_m1_loop(redis, pg, state):
    logger = logging.getLogger("RECOVERY")

    while True:
        async with pg.acquire() as conn:
            rows = await conn.fetch(
                "SELECT symbol, open_time FROM missing_m1_log_v4 WHERE fixed IS NOT true ORDER BY open_time ASC"
            )

        for row in rows:
            symbol = row["symbol"]
            open_time = row["open_time"]
            precision = state["tickers"].get(symbol)
            if not precision:
                continue

            success = await restore_missing_m1(symbol, open_time, redis, pg, precision)

            if success:
                minute = open_time.minute
                if minute % 5 == 4:
                    await try_aggregate_m5(redis, symbol, int(open_time.timestamp() * 1000))

        await asyncio.sleep(60)
# üî∏ –°–ª—É—à–∞–µ—Ç WebSocket Binance –∏ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–∏–∫–µ—Ä–æ–≤
async def listen_kline_stream(redis, state, refresh_queue):
    logger = logging.getLogger("KLINE")

    while True:
        if not state["active"]:
            info_log("KLINE", "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏")
            await asyncio.sleep(10)
            continue

        symbols = sorted(state["active"])
        streams = [f"{s}@kline_1m" for s in symbols]
        stream_url = f"wss://fstream.binance.com/stream?streams={'/'.join(streams)}"

        try:
            async with websockets.connect(stream_url) as ws:
                logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ WebSocket Binance: {len(symbols)} —Ç–∏–∫–µ—Ä–æ–≤")

                async def reader():
                    try:
                        async for msg in ws:
                            data = json.loads(msg)
                            if "data" not in data or "k" not in data["data"]:
                                continue
                            kline = data["data"]["k"]
                            if not kline["x"]:
                                continue
                            symbol = kline["s"]
                            open_time = datetime.utcfromtimestamp(kline["t"] / 1000)

                            await store_and_publish_m1(
                                redis,
                                symbol,
                                open_time,
                                kline,
                                state["tickers"][symbol]
                            )

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è WebSocket: {e}", exc_info=True)

                async def watcher():
                    await refresh_queue.get()
                    logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è WebSocket")
                    await ws.close()

                reader_task = asyncio.create_task(reader())
                watcher_task = asyncio.create_task(watcher())
                await asyncio.wait([reader_task, watcher_task], return_when=asyncio.FIRST_COMPLETED)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ WebSocket: {e}", exc_info=True)
            await asyncio.sleep(5)
# üî∏ –ü–æ—Ç–æ–∫ markPrice –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ —Å fstream.binance.com
async def watch_mark_price(symbol, redis, precision):

    logger = logging.getLogger("KLINE")

    url = f"wss://fstream.binance.com/ws/{symbol.lower()}@markPrice@1s"
    last_update = 0

    while True:
        try:
            async with websockets.connect(url) as ws:
                logger.info(f"[{symbol}] –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ø–æ—Ç–æ–∫—É markPrice (futures)")
                async for msg in ws:
                    try:
                        data = json.loads(msg)
                        price = data.get("p")
                        if not price:
                            continue

                        now = time.time()
                        if now - last_update < 1:
                            continue

                        last_update = now
                        rounded = str(Decimal(price).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))
                        await redis.set(f"price:{symbol}", rounded)
                        info_log("KLINE", f"[{symbol}] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markPrice (futures): {rounded}")
                    except Exception as e:
                        logger.warning(f"[{symbol}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ markPrice: {e}")
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ WebSocket markPrice (futures): {e}", exc_info=True)
            await asyncio.sleep(5)
# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
async def run_feed_and_aggregator(pg, redis):

    log = logging.getLogger("FEED+AGGREGATOR")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ (enabled + disabled)
    tickers, active = await load_all_tickers(pg)
    log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–∏–∫–µ—Ä–æ–≤: {len(tickers)} ‚Üí {list(tickers.keys())}")

    for s in tickers:
        if s.lower() in active:
            log.info(f"–ê–∫—Ç–∏–≤–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {s}")
        else:
            log.info(f"–í—ã–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {s}")

    # –û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    state = {
        "tickers": tickers,            # symbol -> precision_price
        "active": active,              # set of lowercase symbols
        "markprice_tasks": {}          # symbol -> asyncio.Task
    }

    # –û—á–µ—Ä–µ–¥—å —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ WebSocket
    refresh_queue = asyncio.Queue()

    # –ó–∞–ø—É—Å–∫ –ø–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ Redis Stream
    asyncio.create_task(handle_ticker_events(redis, state, pg, refresh_queue))

    # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤ markPrice –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ (—Ñ—å—é—á–µ—Ä—Å–Ω—ã–π —Ä—ã–Ω–æ–∫)
    for symbol in state["active"]:
        upper_symbol = symbol.upper()
        precision = state["tickers"].get(upper_symbol)
        if precision is not None:
            task = asyncio.create_task(watch_mark_price(upper_symbol, redis, precision))
            state["markprice_tasks"][upper_symbol] = task

    # üî∏ –§–æ–Ω–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö M1
    async def recovery_loop():
        while True:
            now_ts = int(datetime.utcnow().timestamp() * 1000)
            for symbol in state["active"]:
                await detect_missing_m1(redis, pg, symbol.upper(), now_ts)
            await asyncio.sleep(60)

    asyncio.create_task(recovery_loop())

    # üî∏ –§–æ–Ω–æ–≤–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π
    asyncio.create_task(restore_missing_m1_loop(redis, pg, state))

    # –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–ª—É—à–∞—Ç–µ–ª—è WebSocket
    async def loop_listen():
        while True:
            await listen_kline_stream(redis, state, refresh_queue)

    asyncio.create_task(loop_listen())

    # –¶–∏–∫–ª –æ–∂–∏–¥–∞–Ω–∏—è (–º–æ–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ watchdog)
    while True:
        await asyncio.sleep(5)