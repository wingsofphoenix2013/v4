# feed_and_aggregate.py ‚Äî –ø—Ä–∏—ë–º –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - 20/05/2025

import logging
import asyncio
import websockets
import json
import aiohttp
from infra import info_log
from decimal import Decimal, ROUND_DOWN
import time
from datetime import datetime, timedelta

# üî∏ –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤, —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç—É—Å–∞ –∏–∑ PostgreSQL
async def load_all_tickers(pg_pool):
    async with pg_pool.acquire() as conn:
        rows = await conn.fetch("""
            SELECT symbol, precision_price, status FROM tickers_v4
        """)
        tickers = {}
        active = set()
        for row in rows:
            tickers[row['symbol']] = row['precision_price']
            if row['status'] == 'enabled':
                active.add(row['symbol'].lower())
        return tickers, active
# üî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π –≤–∫–ª—é—á–µ–Ω–∏—è/–æ—Ç–∫–ª—é—á–µ–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ Redis Stream
async def handle_ticker_events(redis, state, pg, refresh_queue):
    group = "aggregator_group"
    stream = "tickers_status_stream"
    logger = logging.getLogger("TICKER_STREAM")

    try:
        await redis.xgroup_create(stream, group, id="0", mkstream=True)
    except Exception:
        pass  # –≥—Ä—É–ø–ø–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

    while True:
        resp = await redis.xreadgroup(group, "aggregator", streams={stream: ">"}, count=50, block=1000)
        for _, messages in resp:
            for msg_id, data in messages:
                symbol = data.get("symbol")
                action = data.get("action")
                if not symbol or not action:
                    continue

                if symbol not in state["tickers"]:
                    async with pg.acquire() as conn:
                        row = await conn.fetchrow("""
                            SELECT precision_price FROM tickers_v4 WHERE symbol = $1
                        """, symbol)
                        if row:
                            state["tickers"][symbol] = row["precision_price"]
                            info_log("TICKER_STREAM", f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–∏–∫–µ—Ä –∏–∑ –ë–î: {symbol}")

                if action == "enabled" and symbol in state["tickers"]:
                    logger.info(f"–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —Ç–∏–∫–µ—Ä: {symbol}")
                    state["active"].add(symbol.lower())
                    await refresh_queue.put("refresh")

                    # –∑–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ markPrice –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
                    precision = state["tickers"][symbol]
                    task = asyncio.create_task(watch_mark_price(symbol, redis, precision))
                    state["markprice_tasks"][symbol] = task

                elif action == "disabled" and symbol in state["tickers"]:
                    logger.info(f"–û—Ç–∫–ª—é—á—ë–Ω —Ç–∏–∫–µ—Ä: {symbol}")
                    state["active"].discard(symbol.lower())
                    await refresh_queue.put("refresh")

                    # –æ—Ç–º–µ–Ω–∞ –ø–æ—Ç–æ–∫–∞ markPrice, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    task = state["markprice_tasks"].pop(symbol, None)
                    if task:
                        task.cancel()

                await redis.xack(stream, group, msg_id)

# üî∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Å–≤–µ—á–∏ M1 –≤ RedisJSON + Stream + Pub/Sub
async def store_and_publish_m1(redis, symbol, open_time, kline, precision):

    logger = logging.getLogger("KLINE")
    timestamp = int(open_time.timestamp() * 1000)
    json_key = f"ohlcv:{symbol.lower()}:m1:{timestamp}"

    def r(val):
        return float(Decimal(val).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))

    candle = {
        "o": r(kline["o"]),
        "h": r(kline["h"]),
        "l": r(kline["l"]),
        "c": r(kline["c"]),
        "v": r(kline["v"]),
        "ts": timestamp
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–µ—á—É –≤ Redis JSON
    await redis.execute_command("JSON.SET", json_key, "$", json.dumps(candle))

    info_log("KLINE", f"[{symbol}] M1 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞: {open_time} ‚Üí C={candle['c']}")

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏–µ
    event = {
        "symbol": symbol,
        "interval": "m1",
        "timestamp": str(timestamp)
    }

    # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ Redis Stream (–¥–ª—è core_io.py)
    await redis.xadd("ohlcv_stream", event)

    # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ Redis Pub/Sub (–¥–ª—è realtime-–ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)
    await redis.publish("ohlcv_channel", json.dumps(event))

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è M5 –∏ M15
    await try_aggregate_m5(redis, symbol, open_time)
    await try_aggregate_m15(redis, symbol, open_time)
# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è M5 –Ω–∞ –æ—Å–Ω–æ–≤–µ RedisJSON M1-—Å–≤–µ—á–µ–π (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ minute % 5 == 4)
async def try_aggregate_m5(redis, symbol, open_time):
    import logging
    import json

    logger = logging.getLogger("KLINE")

    if open_time.minute % 5 != 4:
        return

    end_ts = int(open_time.timestamp() * 1000)
    ts_list = [end_ts - 60_000 * i for i in reversed(range(5))]
    candles = []

    for ts in ts_list:
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        try:
            data = await redis.execute_command("JSON.GET", key, "$")
            if not data:
                logger.warning(f"[{symbol}] M5: –ø—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞ {ts}")
                return
            parsed = json.loads(data)[0]
            candles.append(parsed)
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –¥–ª—è M5: {e}")
            return

    o = candles[0]["o"]
    h = max(c["h"] for c in candles)
    l = min(c["l"] for c in candles)
    c = candles[-1]["c"]
    v = sum(c["v"] for c in candles)
    m5_ts = ts_list[0]

    key = f"ohlcv:{symbol.lower()}:m5:{m5_ts}"
    candle = {
        "o": o,
        "h": h,
        "l": l,
        "c": c,
        "v": v,
        "ts": m5_ts
    }

    exists = await redis.exists(key)
    if exists:
        return

    await redis.execute_command("JSON.SET", key, "$", json.dumps(candle))
    info_log("KLINE", f"[{symbol}] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ M5: {open_time.replace(second=0)} ‚Üí O:{o} H:{h} L:{l} C:{c}")

    # üì§ Redis Stream (–¥–ª—è core_io.py)
    await redis.xadd("ohlcv_stream", {
        "symbol": symbol,
        "interval": "m5",
        "timestamp": str(m5_ts)
    })

    # üì¢ Redis Pub/Sub (–¥–ª—è realtime –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)
    await redis.publish("ohlcv_channel", json.dumps({
        "symbol": symbol,
        "interval": "m5",
        "timestamp": m5_ts
    }))
# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è M15 –Ω–∞ –æ—Å–Ω–æ–≤–µ RedisJSON M1-—Å–≤–µ—á–µ–π (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ minute % 15 == 14)
async def try_aggregate_m15(redis, symbol, open_time):
    import logging
    import json

    logger = logging.getLogger("KLINE")

    if open_time.minute % 15 != 14:
        return

    end_ts = int(open_time.timestamp() * 1000)
    ts_list = [end_ts - 60_000 * i for i in reversed(range(15))]
    candles = []

    for ts in ts_list:
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        try:
            data = await redis.execute_command("JSON.GET", key, "$")
            if not data:
                logger.warning(f"[{symbol}] M15: –ø—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞ {ts}")
                return
            parsed = json.loads(data)[0]
            candles.append(parsed)
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –¥–ª—è M15: {e}")
            return

    o = candles[0]["o"]
    h = max(c["h"] for c in candles)
    l = min(c["l"] for c in candles)
    c = candles[-1]["c"]
    v = sum(c["v"] for c in candles)
    m15_ts = ts_list[0]

    key = f"ohlcv:{symbol.lower()}:m15:{m15_ts}"
    candle = {
        "o": o,
        "h": h,
        "l": l,
        "c": c,
        "v": v,
        "ts": m15_ts
    }

    exists = await redis.exists(key)
    if exists:
        return

    await redis.execute_command("JSON.SET", key, "$", json.dumps(candle))
    info_log("KLINE", f"[{symbol}] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ M15: {open_time.replace(second=0)} ‚Üí O:{o} H:{h} L:{l} C:{c}")

    # üì§ Redis Stream (–¥–ª—è core_io.py)
    await redis.xadd("ohlcv_stream", {
        "symbol": symbol,
        "interval": "m15",
        "timestamp": str(m15_ts)
    })

    # üì¢ Redis Pub/Sub (–¥–ª—è realtime –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)
    await redis.publish("ohlcv_channel", json.dumps({
        "symbol": symbol,
        "interval": "m15",
        "timestamp": m15_ts
    }))
# üî∏ –ü–æ–∏—Å–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö M1 –∏ –∑–∞–ø–∏—Å—å –≤ missing_m1_log_v4 + system_log_v4
async def detect_missing_m1(redis, pg, symbol, now_ts):
    logger = logging.getLogger("RECOVERY")
    missing = []

    for i in range(1, 5):  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 –º–∏–Ω—É—Ç—ã
        ts = now_ts - i * 60 * 1000
        key = f"ohlcv:{symbol.lower()}:m1:{ts}"
        exists = await redis.exists(key)
        if not exists:
            missing.append(ts)

    async with pg.acquire() as conn:
        for ts in missing:
            open_time = datetime.utcfromtimestamp(ts / 1000)
            try:
                await conn.execute(
                    "INSERT INTO missing_m1_log_v4 (symbol, open_time) VALUES ($1, $2) ON CONFLICT DO NOTHING",
                    symbol, open_time
                )
                await conn.execute(
                    "INSERT INTO system_log_v4 (module, level, message, details) VALUES ($1, $2, $3, $4)",
                    "AGGREGATOR", "WARNING", "M1 missing", 
                    json.dumps({"symbol": symbol, "open_time": str(open_time)})
                )
                logger.warning(f"[{symbol}] –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å–≤–µ—á–∞: {open_time}")
            except Exception as e:
                logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –ø—Ä–æ–ø—É—Å–∫–∞: {e}")
# üî∏ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–π M1 —Å–≤–µ—á–∏ —á–µ—Ä–µ–∑ Binance API
async def restore_missing_m1(symbol, open_time, redis, pg, precision):
    import logging
    import aiohttp
    import json
    from decimal import Decimal, ROUND_DOWN

    logger = logging.getLogger("RECOVERY")
    ts = int(open_time.timestamp() * 1000)
    end_ts = ts + 60_000

    url = "https://fapi.binance.com/fapi/v1/klines"
    params = {
        "symbol": symbol.upper(),
        "interval": "1m",
        "startTime": ts,
        "endTime": end_ts,
        "limit": 1
    }

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as resp:
                if resp.status != 200:
                    logger.error(f"[{symbol}] Binance API error: {resp.status}")
                    return False

                raw = await resp.json()
                if not raw:
                    logger.warning(f"[{symbol}] Binance API –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                    return False

                data = raw[0]
                o, h, l, c, v = data[1:6]
                timestamp = int(data[0])

                def r(val):
                    return float(Decimal(val).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))

                candle = {
                    "o": r(o),
                    "h": r(h),
                    "l": r(l),
                    "c": r(c),
                    "v": r(v),
                    "ts": timestamp,
                    "fixed": True
                }

                key = f"ohlcv:{symbol.lower()}:m1:{timestamp}"
                await redis.execute_command("JSON.SET", key, "$", json.dumps(candle))

                async with pg.acquire() as conn:
                    await conn.execute(
                        "UPDATE missing_m1_log_v4 SET fixed = true, fixed_at = NOW() WHERE symbol = $1 AND open_time = $2",
                        symbol, open_time
                    )
                    await conn.execute(
                        "INSERT INTO system_log_v4 (module, level, message, details) VALUES ($1, $2, $3, $4)",
                        "AGGREGATOR", "INFO", "M1 restored", 
                        json.dumps({"symbol": symbol, "open_time": str(open_time)})
                    )

                logger.info(f"[{symbol}] –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ M1: {open_time}")

                # üî∏ –ü–æ–ø—ã—Ç–∫–∞ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å M5 –∏ M15 –ø–æ –Ω—É–∂–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º
                m5_minute = (open_time.minute // 5) * 5
                m5_time = open_time.replace(minute=m5_minute, second=0, microsecond=0)
                await try_aggregate_m5(redis, symbol, m5_time)

                m15_minute = (open_time.minute // 15) * 15
                m15_time = open_time.replace(minute=m15_minute, second=0, microsecond=0)
                await try_aggregate_m15(redis, symbol, m15_time)

                return True

    except Exception as e:
        logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ API: {e}", exc_info=True)
        return False
# üî∏ –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –∏–∑ missing_m1_log_v4
async def restore_missing_m1_loop(redis, pg, state):
    logger = logging.getLogger("RECOVERY")

    while True:
        async with pg.acquire() as conn:
            rows = await conn.fetch(
                "SELECT symbol, open_time FROM missing_m1_log_v4 WHERE fixed IS NOT true ORDER BY open_time ASC"
            )

        for row in rows:
            symbol = row["symbol"]
            open_time = row["open_time"]
            precision = state["tickers"].get(symbol)
            if not precision:
                continue

            success = await restore_missing_m1(symbol, open_time, redis, pg, precision)

            if success:
                minute = open_time.minute
                if minute % 5 == 4:
                    await try_aggregate_m5(redis, symbol, open_time)
        await asyncio.sleep(60)
# üî∏ –ü—Ä–æ–≤–µ—Ä–∫–∞: 4 –∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä—è–¥ –Ω–µ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π ‚Üí –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞
async def check_consecutive_m1_failures(pg, symbol):
    logger = logging.getLogger("RECOVERY")

    async with pg.acquire() as conn:
        rows = await conn.fetch("""
            SELECT open_time FROM missing_m1_log_v4
            WHERE symbol = $1 AND fixed IS NOT true
            ORDER BY open_time ASC
            LIMIT 10
        """, symbol)

        if len(rows) < 4:
            return

        timestamps = [row["open_time"] for row in rows]
        deltas = [(timestamps[i+1] - timestamps[i]).total_seconds() for i in range(len(timestamps)-1)]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥—Ä—è–¥ 4 –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –ø–æ 60 —Å–µ–∫—É–Ω–¥ (—Ä–∞–∑–±—Ä–æ—Å ¬±1—Å)
        count = 1
        for i in range(len(deltas)):
            if 59 <= deltas[i] <= 61:
                count += 1
                if count >= 4:
                    break
            else:
                count = 1

        if count >= 4:
            await conn.execute("""
                UPDATE tickers_v4 SET tradepermission = 'disabled'
                WHERE symbol = $1 AND tradepermission = 'enabled'
            """, symbol)

            await conn.execute("""
                INSERT INTO system_log_v4 (module, level, message, details)
                VALUES ($1, $2, $3, $4)
            """, "AGGREGATOR", "CRITICAL", "M1 permanently degraded",
                json.dumps({"symbol": symbol, "reason": "4+ consecutive missing M1"}))

            logger.critical(f"[{symbol}] –û—Ç–∫–ª—é—á—ë–Ω ‚Äî 4+ –ø–æ–¥—Ä—è–¥ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π")
# üî∏ –°–ª—É—à–∞–µ—Ç WebSocket Binance –∏ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–∏–∫–µ—Ä–æ–≤
async def listen_kline_stream(redis, state, refresh_queue):
    logger = logging.getLogger("KLINE")

    while True:
        if not state["active"]:
            info_log("KLINE", "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏")
            await asyncio.sleep(10)
            continue

        symbols = sorted(state["active"])
        streams = [f"{s}@kline_1m" for s in symbols]
        stream_url = f"wss://fstream.binance.com/stream?streams={'/'.join(streams)}"

        try:
            async with websockets.connect(stream_url) as ws:
                logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ WebSocket Binance: {len(symbols)} —Ç–∏–∫–µ—Ä–æ–≤")

                async def reader():
                    try:
                        async for msg in ws:
                            data = json.loads(msg)
                            if "data" not in data or "k" not in data["data"]:
                                continue
                            kline = data["data"]["k"]
                            if not kline["x"]:
                                continue
                            symbol = kline["s"]
                            open_time = datetime.utcfromtimestamp(kline["t"] / 1000)

                            await store_and_publish_m1(
                                redis,
                                symbol,
                                open_time,
                                kline,
                                state["tickers"][symbol]
                            )

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è WebSocket: {e}", exc_info=True)

                async def watcher():
                    await refresh_queue.get()
                    logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è WebSocket")
                    await ws.close()

                reader_task = asyncio.create_task(reader())
                watcher_task = asyncio.create_task(watcher())
                await asyncio.wait([reader_task, watcher_task], return_when=asyncio.FIRST_COMPLETED)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ WebSocket: {e}", exc_info=True)
            await asyncio.sleep(5)
# üî∏ –ü–æ—Ç–æ–∫ markPrice –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ —Å fstream.binance.com
async def watch_mark_price(symbol, redis, precision):

    logger = logging.getLogger("KLINE")

    url = f"wss://fstream.binance.com/ws/{symbol.lower()}@markPrice@1s"
    last_update = 0

    while True:
        try:
            async with websockets.connect(url) as ws:
                logger.info(f"[{symbol}] –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ø–æ—Ç–æ–∫—É markPrice (futures)")
                async for msg in ws:
                    try:
                        data = json.loads(msg)
                        price = data.get("p")
                        if not price:
                            continue

                        now = time.time()
                        if now - last_update < 1:
                            continue

                        last_update = now
                        rounded = str(Decimal(price).quantize(Decimal(f"1e-{precision}"), rounding=ROUND_DOWN))
                        await redis.set(f"price:{symbol}", rounded)
                        info_log("KLINE", f"[{symbol}] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markPrice (futures): {rounded}")
                    except Exception as e:
                        logger.warning(f"[{symbol}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ markPrice: {e}")
        except Exception as e:
            logger.error(f"[{symbol}] –û—à–∏–±–∫–∞ WebSocket markPrice (futures): {e}", exc_info=True)
            await asyncio.sleep(5)
# üî∏ –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ç–∞—Å–∫–∞–º–∏
async def run_feed_and_aggregator(pg, redis):

    log = logging.getLogger("FEED+AGGREGATOR")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ (enabled + disabled)
    tickers, active = await load_all_tickers(pg)
    log.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–∏–∫–µ—Ä–æ–≤: {len(tickers)} ‚Üí {list(tickers.keys())}")

    for s in tickers:
        if s.lower() in active:
            log.info(f"–ê–∫—Ç–∏–≤–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {s}")
        else:
            log.info(f"–í—ã–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {s}")

    # –û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    state = {
        "tickers": tickers,            # symbol -> precision_price
        "active": active,              # set of lowercase symbols
        "markprice_tasks": {},         # symbol -> asyncio.Task
        "tasks": set()                 # –≤—Å–µ —Ñ–æ–Ω–æ–≤—ã–µ —Ç–∞—Å–∫–∏
    }

    # –û—á–µ—Ä–µ–¥—å —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ WebSocket
    refresh_queue = asyncio.Queue()

    # –•–µ–ª–ø–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Ç–∞—Å–æ–∫
    def create_tracked_task(coro, name):
        task = asyncio.create_task(coro)
        state["tasks"].add(task)
        def on_done(t):
            try:
                t.result()
            except Exception as e:
                log.exception(f"[{name}] Task crashed: {e}")
            finally:
                state["tasks"].discard(t)
        task.add_done_callback(on_done)
        return task

    # –ó–∞–ø—É—Å–∫ –ø–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ Redis Stream
    create_tracked_task(handle_ticker_events(redis, state, pg, refresh_queue), "ticker_events")

    # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤ markPrice –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ (—Ñ—å—é—á–µ—Ä—Å–Ω—ã–π —Ä—ã–Ω–æ–∫)
    for symbol in state["active"]:
        upper_symbol = symbol.upper()
        precision = state["tickers"].get(upper_symbol)
        if precision is not None:
            task = create_tracked_task(watch_mark_price(upper_symbol, redis, precision), f"markprice_{upper_symbol}")
            state["markprice_tasks"][upper_symbol] = task

    # üî∏ –§–æ–Ω–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö M1
    async def recovery_loop():
        while True:
            now_ts = int(datetime.utcnow().replace(second=0, microsecond=0).timestamp() * 1000)
            for symbol in state["active"]:
                await detect_missing_m1(redis, pg, symbol.upper(), now_ts)
            await asyncio.sleep(60)

    create_tracked_task(recovery_loop(), "recovery_loop")

    # üî∏ –§–æ–Ω–æ–≤–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π
    async def restore_loop():
        while True:
            async with pg.acquire() as conn:
                rows = await conn.fetch(
                    "SELECT symbol, open_time FROM missing_m1_log_v4 WHERE fixed IS NOT true ORDER BY open_time ASC"
                )

            for row in rows:
                symbol = row["symbol"]
                open_time = row["open_time"]
                precision = state["tickers"].get(symbol)
                if not precision:
                    continue

                await check_consecutive_m1_failures(pg, symbol)

                success = await restore_missing_m1(symbol, open_time, redis, pg, precision)

                if success and open_time.minute % 5 == 4:
                    await try_aggregate_m5(redis, symbol, open_time)

            await asyncio.sleep(60)

    create_tracked_task(restore_loop(), "restore_loop")

    # –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–ª—É—à–∞—Ç–µ–ª—è WebSocket
    async def loop_listen():
        while True:
            await listen_kline_stream(redis, state, refresh_queue)

    create_tracked_task(loop_listen(), "listen_kline")

    # –¶–∏–∫–ª –æ–∂–∏–¥–∞–Ω–∏—è (–º–æ–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ watchdog –∏–ª–∏ —Ç–æ—á–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)
    try:
        while True:
            await asyncio.sleep(5)
    except asyncio.CancelledError:
        log.info("Aggregator shutdown requested, cancelling tasks...")
        for t in list(state["tasks"]):
            t.cancel()
        await asyncio.gather(*state["tasks"], return_exceptions=True)
        log.info("All tasks shut down cleanly.")