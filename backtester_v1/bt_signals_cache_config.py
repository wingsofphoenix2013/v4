# bt_signals_cache_config.py â€” ĞºĞµÑˆ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² live-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² (bad bins) Ğ¿Ğ¾ mirror (scenario_id/signal_id) Ğ´Ğ»Ñ backtester_v1

import asyncio
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple

# ğŸ”¸ ĞšĞµÑˆĞ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ backtester_v1
from backtester_config import get_enabled_signals

log = logging.getLogger("BT_SIG_CACHE")

# ğŸ”¸ Ğ¡Ñ‚Ñ€Ğ¸Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ postproc Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²)
ANALYSIS_POSTPROC_STREAM_KEY = "bt:analysis:postproc_ready"
CACHE_CONSUMER_GROUP = "bt_signals_cache"
CACHE_CONSUMER_NAME = "bt_signals_cache_main"

CACHE_STREAM_BATCH_SIZE = 50
CACHE_STREAM_BLOCK_MS = 5000

# ğŸ”¸ ĞšĞµÑˆ: ĞºĞ»ÑÑ‡ (scenario_id, signal_id, direction) -> bad bins
# required_pairs: set[(analysis_id, timeframe)]
# bad_bins_map: (analysis_id, timeframe) -> set[bin_name]
bt_mirror_required_pairs: Dict[Tuple[int, int, str], Set[Tuple[int, str]]] = {}
bt_mirror_bad_bins_map: Dict[Tuple[int, int, str], Dict[Tuple[int, str], Set[str]]] = {}

# ğŸ”¸ Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… mirror-Ğ¿Ğ°Ñ€ (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ»Ğ¸ÑˆĞ½ĞµĞµ)
_active_mirrors: Set[Tuple[int, int, str]] = set()


# ğŸ”¸ ĞŸÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ³ĞµÑ‚Ñ‚ĞµÑ€: Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞµÑˆ bad bins Ğ´Ğ»Ñ mirror (scenario_id, signal_id, direction)
def get_mirror_bad_cache(
    mirror_scenario_id: int,
    mirror_signal_id: int,
    direction: str,
) -> Tuple[Optional[Set[Tuple[int, str]]], Optional[Dict[Tuple[int, str], Set[str]]]]:
    key = (int(mirror_scenario_id), int(mirror_signal_id), str(direction).strip().lower())
    return bt_mirror_required_pairs.get(key), bt_mirror_bad_bins_map.get(key)


# ğŸ”¸ ĞŸÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: Ğ¿ĞµÑ€ĞµÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… mirrors Ğ¿Ğ¾ enabled live-Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ°Ğ¼
def rebuild_active_mirrors_index() -> Set[Tuple[int, int, str]]:
    mirrors: Set[Tuple[int, int, str]] = set()

    signals = get_enabled_signals()
    for s in signals:
        mode = str(s.get("mode") or "").strip().lower()
        if mode != "live":
            continue

        params = s.get("params") or {}

        # mirror params
        ms_cfg = params.get("mirror_scenario_id")
        mi_cfg = params.get("mirror_signal_id")
        if not ms_cfg or not mi_cfg:
            continue

        try:
            mirror_scenario_id = int(ms_cfg.get("value"))
            mirror_signal_id = int(mi_cfg.get("value"))
        except Exception:
            continue

        # direction Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¸Ğ· live Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ° (direction_mask)
        dm_cfg = params.get("direction_mask")
        if not dm_cfg:
            continue
        direction = str(dm_cfg.get("value") or "").strip().lower()
        if direction not in ("long", "short"):
            continue

        mirrors.add((mirror_scenario_id, mirror_signal_id, direction))

    global _active_mirrors
    _active_mirrors = mirrors
    return mirrors


# ğŸ”¸ ĞŸÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞµÑˆĞ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… mirrors
async def load_initial_mirror_caches(pg) -> None:
    mirrors = rebuild_active_mirrors_index()

    if not mirrors:
        log.info("BT_SIG_CACHE: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… mirror-Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ¾Ğ² Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ â€” ĞºĞµÑˆ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ÑÑ")
        return

    loaded = 0
    total_required = 0
    total_bad_bins = 0

    for (sc_id, sig_id, direction) in sorted(mirrors):
        req, bad_map = await _load_bad_bins_for_pair(pg, sc_id, sig_id, direction)
        _store_cache(sc_id, sig_id, direction, req, bad_map)
        loaded += 1
        total_required += len(req)
        total_bad_bins += sum(len(v) for v in bad_map.values())

    log.info(
        "BT_SIG_CACHE: initial cache loaded â€” mirrors=%s, required_pairs=%s, bad_bins=%s",
        loaded,
        total_required,
        total_bad_bins,
    )


# ğŸ”¸ Ğ’Ğ¾Ñ€ĞºĞµÑ€ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞµÑˆĞ° Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¸Ğ¼Ñƒ bt:analysis:postproc_ready
async def run_bt_signals_cache_watcher(pg, redis) -> None:
    log.debug("BT_SIG_CACHE: watcher Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ (bt:analysis:postproc_ready)")

    await load_initial_mirror_caches(pg)
    await _ensure_consumer_group(redis)

    while True:
        try:
            entries = await redis.xreadgroup(
                groupname=CACHE_CONSUMER_GROUP,
                consumername=CACHE_CONSUMER_NAME,
                streams={ANALYSIS_POSTPROC_STREAM_KEY: ">"},
                count=CACHE_STREAM_BATCH_SIZE,
                block=CACHE_STREAM_BLOCK_MS,
            )

            if not entries:
                continue

            total_msgs = 0
            refreshed = 0
            ignored = 0

            for _, messages in entries:
                for msg_id, fields in messages:
                    total_msgs += 1

                    ctx = _parse_postproc_ready(fields)
                    if not ctx:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)
                        ignored += 1
                        continue

                    scenario_id = ctx["scenario_id"]
                    signal_id = ctx["signal_id"]
                    source_finished_at = ctx["source_finished_at"]

                    # Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… mirrors (Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ/Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ live-Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ¾Ğ²)
                    mirrors = rebuild_active_mirrors_index()

                    # Ğ½Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº mirror
                    targets = [
                        (scenario_id, signal_id, d)
                        for d in ("long", "short")
                        if (scenario_id, signal_id, d) in mirrors
                    ]

                    if not targets:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)
                        ignored += 1
                        continue

                    for (sc_id, sig_id, direction) in targets:
                        req, bad_map = await _load_bad_bins_for_pair(pg, sc_id, sig_id, direction)
                        _store_cache(sc_id, sig_id, direction, req, bad_map)
                        refreshed += 1

                        log.info(
                            "BT_SIG_CACHE: cache refreshed â€” mirror=%s:%s:%s, required_pairs=%s, bad_bins=%s, source_finished_at=%s",
                            sc_id,
                            sig_id,
                            direction,
                            len(req),
                            sum(len(v) for v in bad_map.values()),
                            source_finished_at,
                        )

                    await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)

            if total_msgs:
                log.info(
                    "BT_SIG_CACHE: batch processed â€” msgs=%s, refreshed=%s, ignored=%s",
                    total_msgs,
                    refreshed,
                    ignored,
                )

        except Exception as e:
            log.error("BT_SIG_CACHE: watcher loop error: %s", e, exc_info=True)
            await asyncio.sleep(2)


# ğŸ”¸ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°/ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ consumer group
async def _ensure_consumer_group(redis) -> None:
    try:
        await redis.xgroup_create(
            name=ANALYSIS_POSTPROC_STREAM_KEY,
            groupname=CACHE_CONSUMER_GROUP,
            id="$",
            mkstream=True,
        )
        log.debug(
            "BT_SIG_CACHE: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° consumer group '%s' Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ° '%s'",
            CACHE_CONSUMER_GROUP,
            ANALYSIS_POSTPROC_STREAM_KEY,
        )
    except Exception as e:
        msg = str(e)
        if "BUSYGROUP" in msg:
            log.debug(
                "BT_SIG_CACHE: consumer group '%s' Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ° '%s' ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚",
                CACHE_CONSUMER_GROUP,
                ANALYSIS_POSTPROC_STREAM_KEY,
            )
        else:
            log.error(
                "BT_SIG_CACHE: Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ consumer group '%s': %s",
                CACHE_CONSUMER_GROUP,
                e,
                exc_info=True,
            )
            raise


# ğŸ”¸ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ postproc_ready
def _parse_postproc_ready(fields: Dict[Any, Any]) -> Optional[Dict[str, Any]]:
    try:
        # redis Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ bytes
        def _s(x):
            if isinstance(x, bytes):
                return x.decode("utf-8")
            return str(x)

        scenario_id = int(_s(fields.get("scenario_id")))
        signal_id = int(_s(fields.get("signal_id")))

        sf = fields.get("source_finished_at")
        source_finished_at = _s(sf) if sf is not None else None

        return {
            "scenario_id": scenario_id,
            "signal_id": signal_id,
            "source_finished_at": source_finished_at,
        }
    except Exception:
        return None


# ğŸ”¸ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° bad bins Ğ¸Ğ· bt_analysis_bins_labels Ğ´Ğ»Ñ mirror-Ğ¿Ğ°Ñ€Ñ‹ Ğ¸ direction
async def _load_bad_bins_for_pair(
    pg,
    scenario_id: int,
    signal_id: int,
    direction: str,
) -> Tuple[Set[Tuple[int, str]], Dict[Tuple[int, str], Set[str]]]:
    direction_l = str(direction).strip().lower()

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT analysis_id, timeframe, bin_name
            FROM bt_analysis_bins_labels
            WHERE scenario_id = $1
              AND signal_id   = $2
              AND direction   = $3
              AND state       = 'bad'
            """,
            int(scenario_id),
            int(signal_id),
            direction_l,
        )

    required_pairs: Set[Tuple[int, str]] = set()
    bad_bins_map: Dict[Tuple[int, str], Set[str]] = {}

    for r in rows:
        aid = int(r["analysis_id"])
        tf = str(r["timeframe"]).strip().lower()
        bn = str(r["bin_name"])

        pair = (aid, tf)
        required_pairs.add(pair)
        bad_bins_map.setdefault(pair, set()).add(bn)

    return required_pairs, bad_bins_map


# ğŸ”¸ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞµÑˆĞ°
def _store_cache(
    scenario_id: int,
    signal_id: int,
    direction: str,
    required_pairs: Set[Tuple[int, str]],
    bad_bins_map: Dict[Tuple[int, str], Set[str]],
) -> None:
    key = (int(scenario_id), int(signal_id), str(direction).strip().lower())
    bt_mirror_required_pairs[key] = set(required_pairs)
    bt_mirror_bad_bins_map[key] = {k: set(v) for k, v in bad_bins_map.items()}