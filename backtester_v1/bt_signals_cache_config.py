# bt_signals_cache_config.py â€” ÐºÐµÑˆ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² live-ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð² (good/bad bins) Ð¿Ð¾ mirror (scenario_id/signal_id) Ð´Ð»Ñ backtester_v1

import asyncio
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple

# ðŸ”¸ ÐšÐµÑˆÐ¸ Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð³ backtester_v1
from backtester_config import get_enabled_signals

log = logging.getLogger("BT_SIG_CACHE")

# ðŸ”¸ Ð¡Ñ‚Ñ€Ð¸Ð¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ postproc Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð²)
ANALYSIS_POSTPROC_STREAM_KEY = "bt:analysis:postproc_ready"
CACHE_CONSUMER_GROUP = "bt_signals_cache"
CACHE_CONSUMER_NAME = "bt_signals_cache_main"

CACHE_STREAM_BATCH_SIZE = 50
CACHE_STREAM_BLOCK_MS = 5000

# ðŸ”¸ ÐšÐµÑˆ: ÐºÐ»ÑŽÑ‡ (scenario_id, signal_id, direction) -> label bins (good/bad)
bt_mirror_required_pairs: Dict[Tuple[int, int, str], Set[Tuple[int, str]]] = {}
bt_mirror_bad_bins_map: Dict[Tuple[int, int, str], Dict[Tuple[int, str], Set[str]]] = {}
bt_mirror_good_bins_map: Dict[Tuple[int, int, str], Dict[Tuple[int, str], Set[str]]] = {}

# ðŸ”¸ Ð˜Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirror-Ð¿Ð°Ñ€ (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð»Ð¸ÑˆÐ½ÐµÐµ)
_active_mirrors: Set[Tuple[int, int, str]] = set()


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð³ÐµÑ‚Ñ‚ÐµÑ€: Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐµÑˆ label bins (good/bad) Ð´Ð»Ñ mirror (scenario_id, signal_id, direction)
def get_mirror_label_cache(
    mirror_scenario_id: int,
    mirror_signal_id: int,
    direction: str,
) -> Tuple[
    Optional[Set[Tuple[int, str]]],
    Optional[Dict[Tuple[int, str], Set[str]]],
    Optional[Dict[Tuple[int, str], Set[str]]],
]:
    key = (int(mirror_scenario_id), int(mirror_signal_id), str(direction).strip().lower())
    return (
        bt_mirror_required_pairs.get(key),
        bt_mirror_bad_bins_map.get(key),
        bt_mirror_good_bins_map.get(key),
    )

# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð³ÐµÑ‚Ñ‚ÐµÑ€ (backward-compatible): Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐµÑˆ bad bins Ð´Ð»Ñ mirror (scenario_id, signal_id, direction)
def get_mirror_bad_cache(
    mirror_scenario_id: int,
    mirror_signal_id: int,
    direction: str,
) -> Tuple[Optional[Set[Tuple[int, str]]], Optional[Dict[Tuple[int, str], Set[str]]]]:
    req, bad_map, _good_map = get_mirror_label_cache(mirror_scenario_id, mirror_signal_id, direction)
    return req, bad_map


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´: Ð¿ÐµÑ€ÐµÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors Ð¿Ð¾ enabled live-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ°Ð¼
def rebuild_active_mirrors_index() -> Set[Tuple[int, int, str]]:
    mirrors: Set[Tuple[int, int, str]] = set()

    signals = get_enabled_signals()
    for s in signals:
        mode = str(s.get("mode") or "").strip().lower()
        if mode != "live":
            continue

        params = s.get("params") or {}

        # mirror params
        ms_cfg = params.get("mirror_scenario_id")
        mi_cfg = params.get("mirror_signal_id")
        if not ms_cfg or not mi_cfg:
            continue

        try:
            mirror_scenario_id = int(ms_cfg.get("value"))
            mirror_signal_id = int(mi_cfg.get("value"))
        except Exception:
            continue

        # direction Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· live Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ° (direction_mask)
        dm_cfg = params.get("direction_mask")
        if not dm_cfg:
            continue
        direction = str(dm_cfg.get("value") or "").strip().lower()
        if direction not in ("long", "short"):
            continue

        mirrors.add((mirror_scenario_id, mirror_signal_id, direction))

    global _active_mirrors
    _active_mirrors = mirrors
    return mirrors

# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´: Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐºÐµÑˆÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors
async def load_initial_mirror_caches(pg) -> None:
    mirrors = rebuild_active_mirrors_index()

    if not mirrors:
        log.info("BT_SIG_CACHE: Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirror-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ â€” ÐºÐµÑˆ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ÑÑ")
        return

    loaded = 0
    total_required = 0
    total_bad_bins = 0
    total_good_bins = 0

    for (sc_id, sig_id, direction) in sorted(mirrors):
        req, bad_map, good_map = await _load_label_bins_for_pair(pg, sc_id, sig_id, direction)
        _store_cache(sc_id, sig_id, direction, req, bad_map, good_map)
        loaded += 1
        total_required += len(req)
        total_bad_bins += sum(len(v) for v in bad_map.values())
        total_good_bins += sum(len(v) for v in good_map.values())

    log.info(
        "BT_SIG_CACHE: initial cache loaded â€” mirrors=%s, required_pairs=%s, bad_bins=%s, good_bins=%s",
        loaded,
        total_required,
        total_bad_bins,
        total_good_bins,
    )

# ðŸ”¸ Ð’Ð¾Ñ€ÐºÐµÑ€ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÐµÑˆÐ° Ð¿Ð¾ ÑÑ‚Ñ€Ð¸Ð¼Ñƒ bt:analysis:postproc_ready
async def run_bt_signals_cache_watcher(pg, redis) -> None:
    log.debug("BT_SIG_CACHE: watcher Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ (bt:analysis:postproc_ready)")

    await load_initial_mirror_caches(pg)
    await _ensure_consumer_group(redis)

    while True:
        try:
            entries = await redis.xreadgroup(
                groupname=CACHE_CONSUMER_GROUP,
                consumername=CACHE_CONSUMER_NAME,
                streams={ANALYSIS_POSTPROC_STREAM_KEY: ">"},
                count=CACHE_STREAM_BATCH_SIZE,
                block=CACHE_STREAM_BLOCK_MS,
            )

            if not entries:
                continue

            total_msgs = 0
            refreshed = 0
            ignored = 0

            for _, messages in entries:
                for msg_id, fields in messages:
                    total_msgs += 1

                    ctx = _parse_postproc_ready(fields)
                    if not ctx:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)
                        ignored += 1
                        continue

                    scenario_id = ctx["scenario_id"]
                    signal_id = ctx["signal_id"]
                    source_finished_at = ctx["source_finished_at"]

                    # Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors (Ð½Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ/Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ live-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð²)
                    mirrors = rebuild_active_mirrors_index()

                    # Ð½Ð°Ð¼ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ°Ðº mirror
                    targets = [
                        (scenario_id, signal_id, d)
                        for d in ("long", "short")
                        if (scenario_id, signal_id, d) in mirrors
                    ]

                    if not targets:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)
                        ignored += 1
                        continue

                    for (sc_id, sig_id, direction) in targets:
                        req, bad_map, good_map = await _load_label_bins_for_pair(pg, sc_id, sig_id, direction)
                        _store_cache(sc_id, sig_id, direction, req, bad_map, good_map)
                        refreshed += 1

                        log.info(
                            "BT_SIG_CACHE: cache refreshed â€” mirror=%s:%s:%s, required_pairs=%s, bad_bins=%s, good_bins=%s, source_finished_at=%s",
                            sc_id,
                            sig_id,
                            direction,
                            len(req),
                            sum(len(v) for v in bad_map.values()),
                            sum(len(v) for v in good_map.values()),
                            source_finished_at,
                        )

                    await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY, CACHE_CONSUMER_GROUP, msg_id)

            if total_msgs:
                log.info(
                    "BT_SIG_CACHE: batch processed â€” msgs=%s, refreshed=%s, ignored=%s",
                    total_msgs,
                    refreshed,
                    ignored,
                )

        except Exception as e:
            log.error("BT_SIG_CACHE: watcher loop error: %s", e, exc_info=True)
            await asyncio.sleep(2)


# ðŸ”¸ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°/ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ consumer group
async def _ensure_consumer_group(redis) -> None:
    try:
        await redis.xgroup_create(
            name=ANALYSIS_POSTPROC_STREAM_KEY,
            groupname=CACHE_CONSUMER_GROUP,
            id="$",
            mkstream=True,
        )
        log.debug(
            "BT_SIG_CACHE: ÑÐ¾Ð·Ð´Ð°Ð½Ð° consumer group '%s' Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° '%s'",
            CACHE_CONSUMER_GROUP,
            ANALYSIS_POSTPROC_STREAM_KEY,
        )
    except Exception as e:
        msg = str(e)
        if "BUSYGROUP" in msg:
            log.debug(
                "BT_SIG_CACHE: consumer group '%s' Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° '%s' ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚",
                CACHE_CONSUMER_GROUP,
                ANALYSIS_POSTPROC_STREAM_KEY,
            )
        else:
            log.error(
                "BT_SIG_CACHE: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ consumer group '%s': %s",
                CACHE_CONSUMER_GROUP,
                e,
                exc_info=True,
            )
            raise


# ðŸ”¸ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ postproc_ready
def _parse_postproc_ready(fields: Dict[Any, Any]) -> Optional[Dict[str, Any]]:
    try:
        # redis Ð¼Ð¾Ð¶ÐµÑ‚ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ bytes
        def _s(x):
            if isinstance(x, bytes):
                return x.decode("utf-8")
            return str(x)

        scenario_id = int(_s(fields.get("scenario_id")))
        signal_id = int(_s(fields.get("signal_id")))

        sf = fields.get("source_finished_at")
        source_finished_at = _s(sf) if sf is not None else None

        return {
            "scenario_id": scenario_id,
            "signal_id": signal_id,
            "source_finished_at": source_finished_at,
        }
    except Exception:
        return None

# ðŸ”¸ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° label bins (good/bad) Ð¸Ð· bt_analysis_bins_labels Ð´Ð»Ñ mirror-Ð¿Ð°Ñ€Ñ‹ Ð¸ direction
async def _load_label_bins_for_pair(
    pg,
    scenario_id: int,
    signal_id: int,
    direction: str,
) -> Tuple[
    Set[Tuple[int, str]],
    Dict[Tuple[int, str], Set[str]],
    Dict[Tuple[int, str], Set[str]],
]:
    direction_l = str(direction).strip().lower()

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT analysis_id, timeframe, bin_name, state
            FROM bt_analysis_bins_labels
            WHERE scenario_id = $1
              AND signal_id   = $2
              AND direction   = $3
              AND state      IN ('bad', 'good')
            """,
            int(scenario_id),
            int(signal_id),
            direction_l,
        )

    required_pairs: Set[Tuple[int, str]] = set()
    bad_bins_map: Dict[Tuple[int, str], Set[str]] = {}
    good_bins_map: Dict[Tuple[int, str], Set[str]] = {}

    for r in rows:
        aid = int(r["analysis_id"])
        tf = str(r["timeframe"]).strip().lower()
        bn = str(r["bin_name"])
        st = str(r["state"] or "").strip().lower()

        pair = (aid, tf)
        required_pairs.add(pair)

        if st == "bad":
            bad_bins_map.setdefault(pair, set()).add(bn)
            continue

        if st == "good":
            good_bins_map.setdefault(pair, set()).add(bn)
            continue

    return required_pairs, bad_bins_map, good_bins_map

# ðŸ”¸ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ ÐºÐµÑˆÐ°
def _store_cache(
    scenario_id: int,
    signal_id: int,
    direction: str,
    required_pairs: Set[Tuple[int, str]],
    bad_bins_map: Dict[Tuple[int, str], Set[str]],
    good_bins_map: Dict[Tuple[int, str], Set[str]],
) -> None:
    key = (int(scenario_id), int(signal_id), str(direction).strip().lower())
    bt_mirror_required_pairs[key] = set(required_pairs)
    bt_mirror_bad_bins_map[key] = {k: set(v) for k, v in bad_bins_map.items()}
    bt_mirror_good_bins_map[key] = {k: set(v) for k, v in good_bins_map.items()}