# bt_scenarios_analysis.py ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ–∑–∏—Ü–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ (–±–∏–Ω–æ–≤—ã–µ —Ñ–∏—á–∏ –ø–æ ATR/ADX/Supertrend/EMA/RSI/LR)

import asyncio
import logging
import json
from datetime import datetime
from decimal import Decimal, getcontext
from typing import Any, Dict, List, Optional, Tuple

log = logging.getLogger("BT_SCENARIOS_ANALYSIS")

# üî∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Decimal
getcontext().prec = 28

# üî∏ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å—Ç—Ä–∏–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
ANALYSIS_STREAM_KEY = "bt:postproc:ready"
ANALYSIS_CONSUMER_GROUP = "bt_scenarios_analysis"
ANALYSIS_CONSUMER_NAME = "bt_scenarios_analysis_main"

# üî∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
ANALYSIS_STREAM_BATCH_SIZE = 10
ANALYSIS_STREAM_BLOCK_MS = 5000

# üî∏ –ë–∏–Ω—ã –¥–ª—è ATR% –Ω–∞ m5 (–ø—Ä–æ—Ü–µ–Ω—Ç—ã)
ATR_BINS: List[Tuple[Optional[float], Optional[float], str]] = [
    (0.0, 0.25, "0.00-0.25"),
    (0.25, 0.50, "0.25-0.50"),
    (0.50, 0.75, "0.50-0.75"),
    (0.75, 1.00, "0.75-1.00"),
    (1.00, None, ">=1.00"),
]

# üî∏ –ë–∏–Ω—ã –¥–ª—è ADX (m15/h1)
ADX_BINS: List[Tuple[Optional[float], Optional[float], str]] = [
    (0.0, 10.0, "0-10"),
    (10.0, 20.0, "10-20"),
    (20.0, 30.0, "20-30"),
    (30.0, None, ">=30"),
]

# üî∏ –ë–∏–Ω—ã –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ EMA200 (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
# dist = (price - ema200) / price * 100
DIST_EMA_BINS: List[Tuple[Optional[float], Optional[float], str]] = [
    (None, -2.0, "<=-2"),   # –æ—á–µ–Ω—å –Ω–∏–∂–µ EMA200
    (-2.0, 0.0, "-2-0"),
    (0.0, 2.0, "0-2"),
    (2.0, None, ">=2"),     # —Å–∏–ª—å–Ω–æ –≤—ã—à–µ EMA200
]

# üî∏ –ë–∏–Ω—ã –¥–ª—è RSI14
RSI_BINS: List[Tuple[Optional[float], Optional[float], str]] = [
    (0.0, 30.0, "0-30"),
    (30.0, 50.0, "30-50"),
    (50.0, 70.0, "50-70"),
    (70.0, None, ">=70"),
]

# üî∏ –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ LR —É–≥–ª–∞
LR_ANGLE_EPS = 0.005  # –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ—Ä–æ–≥ –¥–ª—è "flat"


# üî∏ –ü—É–±–ª–∏—á–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
async def run_bt_scenarios_analysis(pg, redis) -> None:
    log.info("BT_SCENARIOS_ANALYSIS: –≤–æ—Ä–∫–µ—Ä –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∑–∞–ø—É—â–µ–Ω")

    await _ensure_consumer_group(redis)

    # –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞ –∏ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
    while True:
        try:
            messages = await _read_from_stream(redis)

            if not messages:
                continue

            total_msgs = 0
            total_scenarios_processed = 0

            for stream_key, entries in messages:
                if stream_key != ANALYSIS_STREAM_KEY:
                    # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á—É–∂–∏–µ —Å—Ç—Ä–∏–º—ã
                    continue

                for entry_id, fields in entries:
                    total_msgs += 1

                    ctx = _parse_postproc_message(fields)
                    if not ctx:
                        # –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî ACK –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)
                        continue

                    scenario_id = ctx["scenario_id"]
                    signal_id = ctx["signal_id"]
                    finished_at = ctx["finished_at"]
                    processed = ctx.get("processed")
                    skipped = ctx.get("skipped")
                    errors = ctx.get("errors")

                    log.info(
                        "BT_SCENARIOS_ANALYSIS: –ø–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ "
                        "scenario_id=%s, signal_id=%s, processed=%s, skipped=%s, errors=%s, finished_at=%s, stream_id=%s",
                        scenario_id,
                        signal_id,
                        processed,
                        skipped,
                        errors,
                        finished_at,
                        entry_id,
                    )

                    # –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–æ —ç—Ç–æ–º—É —Å—Ü–µ–Ω–∞—Ä–∏—é/—Å–∏–≥–Ω–∞–ª—É
                    await _run_analysis_for_scenario(pg, scenario_id, signal_id)
                    total_scenarios_processed += 1

                    # –ø–æ–º–µ—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ
                    await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)

            log.info(
                "BT_SCENARIOS_ANALYSIS: –ø–∞–∫–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω ‚Äî —Å–æ–æ–±—â–µ–Ω–∏–π=%s, —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤=%s",
                total_msgs,
                total_scenarios_processed,
            )

        except Exception as e:
            log.error(
                "BT_SCENARIOS_ANALYSIS: –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ –≤–æ—Ä–∫–µ—Ä–∞: %s",
                e,
                exc_info=True,
            )
            # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ –∫—Ä—É—Ç–∏—Ç—å CPU –ø—Ä–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –æ—à–∏–±–∫–µ
            await asyncio.sleep(2)


# üî∏ –ü—Ä–æ–≤–µ—Ä–∫–∞/—Å–æ–∑–¥–∞–Ω–∏–µ consumer group –¥–ª—è —Å—Ç—Ä–∏–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
async def _ensure_consumer_group(redis) -> None:
    try:
        await redis.xgroup_create(
            name=ANALYSIS_STREAM_KEY,
            groupname=ANALYSIS_CONSUMER_GROUP,
            id="$",
            mkstream=True,
        )
        log.info(
            "BT_SCENARIOS_ANALYSIS: —Å–æ–∑–¥–∞–Ω–∞ consumer group '%s' –¥–ª—è —Å—Ç—Ä–∏–º–∞ '%s'",
            ANALYSIS_CONSUMER_GROUP,
            ANALYSIS_STREAM_KEY,
        )
    except Exception as e:
        msg = str(e)
        if "BUSYGROUP" in msg:
            log.info(
                "BT_SCENARIOS_ANALYSIS: consumer group '%s' –¥–ª—è —Å—Ç—Ä–∏–º–∞ '%s' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç",
                ANALYSIS_CONSUMER_GROUP,
                ANALYSIS_STREAM_KEY,
            )
        else:
            log.error(
                "BT_SCENARIOS_ANALYSIS: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ consumer group '%s': %s",
                ANALYSIS_CONSUMER_GROUP,
                e,
                exc_info=True,
            )
            raise


# üî∏ –ß—Ç–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Å—Ç—Ä–∏–º–∞ bt:postproc:ready
async def _read_from_stream(redis) -> List[Any]:
    entries = await redis.xreadgroup(
        groupname=ANALYSIS_CONSUMER_GROUP,
        consumername=ANALYSIS_CONSUMER_NAME,
        streams={ANALYSIS_STREAM_KEY: ">"},
        count=ANALYSIS_STREAM_BATCH_SIZE,
        block=ANALYSIS_STREAM_BLOCK_MS,
    )

    if not entries:
        return []

    parsed: List[Any] = []
    for stream_key, messages in entries:
        if isinstance(stream_key, bytes):
            stream_key = stream_key.decode("utf-8")

        stream_entries: List[Any] = []
        for msg_id, fields in messages:
            if isinstance(msg_id, bytes):
                msg_id = msg_id.decode("utf-8")

            str_fields: Dict[str, str] = {}
            for k, v in fields.items():
                key_str = k.decode("utf-8") if isinstance(k, bytes) else str(k)
                val_str = v.decode("utf-8") if isinstance(v, bytes) else str(v)
                str_fields[key_str] = val_str

            stream_entries.append((msg_id, str_fields))

        parsed.append((stream_key, stream_entries))

    return parsed


# üî∏ –†–∞–∑–±–æ—Ä –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —Å—Ç—Ä–∏–º–∞ bt:postproc:ready
def _parse_postproc_message(fields: Dict[str, str]) -> Optional[Dict[str, Any]]:
    try:
        scenario_id_str = fields.get("scenario_id")
        signal_id_str = fields.get("signal_id")
        finished_at_str = fields.get("finished_at")

        if not (scenario_id_str and signal_id_str and finished_at_str):
            return None

        scenario_id = int(scenario_id_str)
        signal_id = int(signal_id_str)
        finished_at = datetime.fromisoformat(finished_at_str)

        ctx: Dict[str, Any] = {
            "scenario_id": scenario_id,
            "signal_id": signal_id,
            "finished_at": finished_at,
        }

        # –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        for key in ("processed", "skipped", "errors"):
            if key in fields:
                try:
                    ctx[key] = int(fields[key])
                except Exception:
                    ctx[key] = None

        return ctx
    except Exception as e:
        log.error(
            "BT_SCENARIOS_ANALYSIS: –æ—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞ bt:postproc:ready: %s, fields=%s",
            e,
            fields,
            exc_info=True,
        )
        return None


# üî∏ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è/—Å–∏–≥–Ω–∞–ª–∞
async def _run_analysis_for_scenario(pg, scenario_id: int, signal_id: int) -> None:
    log.info(
        "BT_SCENARIOS_ANALYSIS: —Å—Ç–∞—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è scenario_id=%s, signal_id=%s",
        scenario_id,
        signal_id,
    )

    # –ø–æ–ª—É—á–∞–µ–º –¥–µ–ø–æ–∑–∏—Ç —Å—Ü–µ–Ω–∞—Ä–∏—è
    deposit = await _load_scenario_deposit(pg, scenario_id)
    if deposit is None or deposit <= Decimal("0"):
        log.error(
            "BT_SCENARIOS_ANALYSIS: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π deposit –¥–ª—è scenario_id=%s",
            scenario_id,
        )
        return

    # –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ø–æ–∑–∏—Ü–∏–∏ —Å postproc=true
    async with pg.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT
                id,
                direction,
                entry_price,
                pnl_abs,
                raw_stat
            FROM bt_scenario_positions
            WHERE scenario_id = $1
              AND signal_id   = $2
              AND postproc    = true
            """,
            scenario_id,
            signal_id,
        )

    if not rows:
        log.info(
            "BT_SCENARIOS_ANALYSIS: –Ω–µ—Ç –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (scenario_id=%s, signal_id=%s)",
            scenario_id,
            signal_id,
        )
        return

    total_positions = len(rows)
    log.info(
        "BT_SCENARIOS_ANALYSIS: –∑–∞–≥—Ä—É–∂–µ–Ω–æ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: %s (scenario_id=%s, signal_id=%s)",
        total_positions,
        scenario_id,
        signal_id,
    )

    # –∞–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ (direction, feature_name, bin_label)
    stats: Dict[Tuple[str, str, str], Dict[str, Any]] = {}

    skipped_positions = 0

    for r in rows:
        pos_id = r["id"]
        direction = r["direction"]
        entry_price_val = r["entry_price"]
        pnl_abs_val = r["pnl_abs"]
        raw_stat_val = r["raw_stat"]

        # —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å long/short
        if direction not in ("long", "short"):
            continue

        # –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã
        try:
            entry_price = Decimal(str(entry_price_val))
            pnl_abs = Decimal(str(pnl_abs_val))
        except Exception as e:
            log.warning(
                "BT_SCENARIOS_ANALYSIS: –ø–æ–∑–∏—Ü–∏—è id=%s ‚Äî –æ—à–∏–±–∫–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–∏–ø–æ–≤ entry_price/pnl_abs: %s",
                pos_id,
                e,
            )
            skipped_positions += 1
            continue

        if entry_price <= Decimal("0"):
            skipped_positions += 1
            continue

        # –ø–∞—Ä—Å–∏–º raw_stat (jsonb)
        raw = _ensure_dict_from_json(raw_stat_val)
        if not raw:
            skipped_positions += 1
            continue

        tf_block = raw.get("tf") or {}
        m5_block = tf_block.get("m5") or {}
        m15_block = tf_block.get("m15") or {}
        h1_block = tf_block.get("h1") or {}

        m5_ind = m5_block.get("indicators") or {}
        m15_ind = m15_block.get("indicators") or {}
        h1_ind = h1_block.get("indicators") or {}

        # üî∏ ATR% m5
        atr_agg = m5_ind.get("atr") or {}
        atr14_m5 = _safe_get_float(atr_agg, "atr14")
        if atr14_m5 is not None:
            atr_pct_m5 = float((Decimal(str(atr14_m5)) / entry_price) * Decimal("100"))
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="atr_pct_m5",
                value=atr_pct_m5,
                bins=ATR_BINS,
                pnl_abs=pnl_abs,
            )

        # üî∏ ADX14 m15
        adx_m15_agg = m15_ind.get("adx_dmi") or {}
        adx14_m15 = _safe_get_float(adx_m15_agg, "adx_dmi14_adx")
        if adx14_m15 is not None:
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="adx14_m15",
                value=adx14_m15,
                bins=ADX_BINS,
                pnl_abs=pnl_abs,
            )

        # üî∏ ADX14 h1
        adx_h1_agg = h1_ind.get("adx_dmi") or {}
        adx14_h1 = _safe_get_float(adx_h1_agg, "adx_dmi14_adx")
        if adx14_h1 is not None:
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="adx14_h1",
                value=adx14_h1,
                bins=ADX_BINS,
                pnl_abs=pnl_abs,
            )

        # üî∏ –¢—Ä–µ–Ω–¥ Supertrend m15
        super_m15_agg = m15_ind.get("supertrend") or {}
        super_m15_trend = _safe_get_float(super_m15_agg, "supertrend10_3_0_trend")
        if super_m15_trend is not None:
            label_m15 = _trend_label(direction, super_m15_trend)
        else:
            label_m15 = "none"
        _accumulate_categorical_feature(
            stats,
            direction=direction,
            feature_name="trend_super_m15",
            bin_label=label_m15,
            pnl_abs=pnl_abs,
        )

        # üî∏ –¢—Ä–µ–Ω–¥ Supertrend h1
        super_h1_agg = h1_ind.get("supertrend") or {}
        super_h1_trend = _safe_get_float(super_h1_agg, "supertrend10_3_0_trend")
        if super_h1_trend is not None:
            label_h1 = _trend_label(direction, super_h1_trend)
        else:
            label_h1 = "none"
        _accumulate_categorical_feature(
            stats,
            direction=direction,
            feature_name="trend_super_h1",
            bin_label=label_h1,
            pnl_abs=pnl_abs,
        )

        # üî∏ Distance to EMA200 (m5/m15/h1)
        ema_m5_agg = m5_ind.get("ema") or {}
        ema_m15_agg = m15_ind.get("ema") or {}
        ema_h1_agg = h1_ind.get("ema") or {}

        ema200_m5 = _safe_get_float(ema_m5_agg, "ema200")
        ema200_m15 = _safe_get_float(ema_m15_agg, "ema200")
        ema200_h1 = _safe_get_float(ema_h1_agg, "ema200")

        if ema200_m5 is not None:
            dist_m5 = float((Decimal(str(ema200_m5)) - entry_price) / entry_price * Decimal("100"))
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="dist_ema200_m5",
                value=dist_m5,
                bins=DIST_EMA_BINS,
                pnl_abs=pnl_abs,
            )

        if ema200_m15 is not None:
            dist_m15 = float((Decimal(str(ema200_m15)) - entry_price) / entry_price * Decimal("100"))
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="dist_ema200_m15",
                value=dist_m15,
                bins=DIST_EMA_BINS,
                pnl_abs=pnl_abs,
            )

        if ema200_h1 is not None:
            dist_h1 = float((Decimal(str(ema200_h1)) - entry_price) / entry_price * Decimal("100"))
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="dist_ema200_h1",
                value=dist_h1,
                bins=DIST_EMA_BINS,
                pnl_abs=pnl_abs,
            )

        # üî∏ RSI14 (m5/m15)
        rsi_m5_agg = m5_ind.get("rsi") or {}
        rsi_m15_agg = m15_ind.get("rsi") or {}

        rsi14_m5 = _safe_get_float(rsi_m5_agg, "rsi14")
        if rsi14_m5 is not None:
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="rsi14_m5",
                value=rsi14_m5,
                bins=RSI_BINS,
                pnl_abs=pnl_abs,
            )

        rsi14_m15 = _safe_get_float(rsi_m15_agg, "rsi14")
        if rsi14_m15 is not None:
            _accumulate_numeric_feature(
                stats,
                direction=direction,
                feature_name="rsi14_m15",
                value=rsi14_m15,
                bins=RSI_BINS,
                pnl_abs=pnl_abs,
            )

        # üî∏ LR50 angle (m15/h1) ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ: flat / up / down
        lr_m15_agg = m15_ind.get("lr") or {}
        lr_m15_angle = _safe_get_float(lr_m15_agg, "lr50_angle")
        if lr_m15_angle is not None:
            label_lr_m15 = _lr_angle_label(lr_m15_angle)
            _accumulate_categorical_feature(
                stats,
                direction=direction,
                feature_name="lr50_angle_m15",
                bin_label=label_lr_m15,
                pnl_abs=pnl_abs,
            )

        lr_h1_agg = h1_ind.get("lr") or {}
        lr_h1_angle = _safe_get_float(lr_h1_agg, "lr50_angle")
        if lr_h1_angle is not None:
            label_lr_h1 = _lr_angle_label(lr_h1_angle)
            _accumulate_categorical_feature(
                stats,
                direction=direction,
                feature_name="lr50_angle_h1",
                bin_label=label_lr_h1,
                pnl_abs=pnl_abs,
            )

    log.info(
        "BT_SCENARIOS_ANALYSIS: –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –¥–ª—è scenario_id=%s, signal_id=%s ‚Äî "
        "–ø–æ–∑–∏—Ü–∏–π –≤—Å–µ–≥–æ=%s, –ø—Ä–æ–ø—É—â–µ–Ω–æ=%s, –±–∏–Ω–æ–≤=%s",
        scenario_id,
        signal_id,
        total_positions,
        skipped_positions,
        len(stats),
    )

    # –ø–µ—Ä–µ—Å—á—ë—Ç winrate/roi –∏ –∑–∞–ø–∏—Å—å –≤ —Ç–∞–±–ª–∏—Ü—É
    await _write_feature_bins(pg, scenario_id, signal_id, deposit, stats)


# üî∏ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–ø–æ–∑–∏—Ç–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è
async def _load_scenario_deposit(pg, scenario_id: int) -> Optional[Decimal]:
    async with pg.acquire() as conn:
        row = await conn.fetchrow(
            """
            SELECT param_value
            FROM bt_scenario_parameters
            WHERE scenario_id = $1
              AND param_name  = 'deposit'
            """,
            scenario_id,
        )

    if not row:
        return None

    try:
        return Decimal(str(row["param_value"]))
    except Exception as e:
        log.error(
            "BT_SCENARIOS_ANALYSIS: –æ—à–∏–±–∫–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è deposit –¥–ª—è scenario_id=%s: %s",
            scenario_id,
            e,
        )
        return None


# üî∏ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ raw_stat (jsonb) –≤ dict
def _ensure_dict_from_json(raw_stat_val: Any) -> Dict[str, Any]:
    if raw_stat_val is None:
        return {}
    if isinstance(raw_stat_val, dict):
        return raw_stat_val
    try:
        # asyncpg –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å str –¥–ª—è jsonb
        return json.loads(raw_stat_val)
    except Exception:
        return {}


# üî∏ –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ float –∏–∑ —Å–ª–æ–≤–∞—Ä—è
def _safe_get_float(d: Dict[str, Any], key: str) -> Optional[float]:
    try:
        v = d.get(key)
        if v is None:
            return None
        return float(v)
    except Exception:
        return None


# üî∏ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–∫–∏ —Ç—Ä–µ–Ω–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–¥–µ–ª–∫–∏
def _trend_label(direction: str, trend_value: float) -> str:
    # direction: 'long' –∏–ª–∏ 'short'
    if trend_value == 0:
        return "none"

    if direction == "long":
        if trend_value > 0:
            return "with_trend"
        if trend_value < 0:
            return "against_trend"
    elif direction == "short":
        if trend_value < 0:
            return "with_trend"
        if trend_value > 0:
            return "against_trend"

    return "none"


# üî∏ –ú–µ—Ç–∫–∞ LR-—É–≥–ª–∞: flat / up / down
def _lr_angle_label(angle: float) -> str:
    if angle > LR_ANGLE_EPS:
        return "up"
    if angle < -LR_ANGLE_EPS:
        return "down"
    return "flat"


# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤–æ–π —Ñ–∏—á–∏ –ø–æ –±–∏–Ω–∞–º
def _accumulate_numeric_feature(
    stats: Dict[Tuple[str, str, str], Dict[str, Any]],
    direction: str,
    feature_name: str,
    value: float,
    bins: List[Tuple[Optional[float], Optional[float], str]],
    pnl_abs: Decimal,
) -> None:
    bin_label = None
    bin_from: Optional[float] = None
    bin_to: Optional[float] = None

    for b_from, b_to, label in bins:
        # –Ω–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞
        if b_from is not None and value < b_from:
            continue
        # –≤–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞
        if b_to is not None and value >= b_to:
            continue

        bin_label = label
        bin_from = b_from
        bin_to = b_to
        break

    if bin_label is None:
        return

    key = (direction, feature_name, bin_label)
    bucket = stats.setdefault(
        key,
        {
            "bin_from": bin_from,
            "bin_to": bin_to,
            "trades": 0,
            "wins": 0,
            "losses": 0,
            "pnl_abs_total": Decimal("0"),
        },
    )

    bucket["trades"] += 1
    if pnl_abs > Decimal("0"):
        bucket["wins"] += 1
    elif pnl_abs < Decimal("0"):
        bucket["losses"] += 1

    bucket["pnl_abs_total"] += pnl_abs


# üî∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Ñ–∏—á–∏
def _accumulate_categorical_feature(
    stats: Dict[Tuple[str, str, str], Dict[str, Any]],
    direction: str,
    feature_name: str,
    bin_label: str,
    pnl_abs: Decimal,
) -> None:
    key = (direction, feature_name, bin_label)
    bucket = stats.setdefault(
        key,
        {
            "bin_from": None,
            "bin_to": None,
            "trades": 0,
            "wins": 0,
            "losses": 0,
            "pnl_abs_total": Decimal("0"),
        },
    )

    bucket["trades"] += 1
    if pnl_abs > Decimal("0"):
        bucket["wins"] += 1
    elif pnl_abs < Decimal("0"):
        bucket["losses"] += 1

    bucket["pnl_abs_total"] += pnl_abs


# üî∏ –ó–∞–ø–∏—Å—å –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ –≤ bt_scenario_feature_bins
async def _write_feature_bins(
    pg,
    scenario_id: int,
    signal_id: int,
    deposit: Decimal,
    stats: Dict[Tuple[str, str, str], Dict[str, Any]],
) -> None:
    if not stats:
        log.info(
            "BT_SCENARIOS_ANALYSIS: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ bt_scenario_feature_bins (scenario_id=%s, signal_id=%s)",
            scenario_id,
            signal_id,
        )
        return

    rows_to_insert: List[Tuple[Any, ...]] = []

    for (direction, feature_name, bin_label), data in stats.items():
        trades = data["trades"]
        wins = data["wins"]
        losses = data["losses"]
        pnl_abs_total: Decimal = data["pnl_abs_total"]
        bin_from = data["bin_from"]
        bin_to = data["bin_to"]

        if trades > 0:
            winrate = (Decimal(wins) / Decimal(trades)).quantize(Decimal("0.0001"))
        else:
            winrate = Decimal("0")

        if deposit != 0:
            roi = (pnl_abs_total / deposit).quantize(Decimal("0.0001"))
        else:
            roi = Decimal("0")

        rows_to_insert.append(
            (
                scenario_id,
                signal_id,
                direction,
                feature_name,
                bin_label,
                bin_from,
                bin_to,
                trades,
                wins,
                losses,
                pnl_abs_total,
                winrate,
                roi,
            )
        )

    async with pg.acquire() as conn:
        # —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è/—Å–∏–≥–Ω–∞–ª–∞
        await conn.execute(
            """
            DELETE FROM bt_scenario_feature_bins
            WHERE scenario_id = $1
              AND signal_id   = $2
            """,
            scenario_id,
            signal_id,
        )

        # –≤—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
        await conn.executemany(
            """
            INSERT INTO bt_scenario_feature_bins (
                scenario_id,
                signal_id,
                direction,
                feature_name,
                bin_label,
                bin_from,
                bin_to,
                trades,
                wins,
                losses,
                pnl_abs_total,
                winrate,
                roi,
                created_at
            )
            VALUES (
                $1, $2, $3,
                $4, $5,
                $6, $7,
                $8, $9, $10,
                $11, $12, $13,
                now()
            )
            """,
            rows_to_insert,
        )

    log.info(
        "BT_SCENARIOS_ANALYSIS: –∑–∞–ø–∏—Å–∞–Ω–æ —Å—Ç—Ä–æ–∫ –≤ bt_scenario_feature_bins=%s –¥–ª—è scenario_id=%s, signal_id=%s",
        len(rows_to_insert),
        scenario_id,
        signal_id,
    )