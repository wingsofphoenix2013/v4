# bt_analysis_main.py â€” Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² backtester_v1

import asyncio
import logging
from datetime import datetime
from decimal import Decimal
from typing import Dict, Any, List, Optional, Callable, Awaitable, Tuple

# ðŸ”¸ ÐšÐ¾Ð½Ñ„Ð¸Ð³ Ð¸ ÐºÐµÑˆÐ¸ backtester_v1
from backtester_config import (
    get_analysis_connections_for_scenario_signal,
    get_analysis_instance,
)

# ðŸ”¸ Ð¢Ð¸Ð¿ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°:
#    (analysis_cfg, analysis_ctx, pg_pool, redis_client) -> {"rows": [...], "summary": {...}}
AnalysisHandler = Callable[
    [Dict[str, Any], Dict[str, Any], Any, Any],
    Awaitable[Dict[str, Any]]
]

# ðŸ”¸ Ð’Ð¾Ñ€ÐºÐµÑ€Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² (Ð¸Ð· Ð¿Ð°ÐºÐµÑ‚Ð° analysis/)
from analysis.bt_analysis_rsi_bin import run_rsi_bin_analysis
from analysis.bt_analysis_mfi_bin import run_mfi_bin_analysis
from analysis.bt_analysis_adx_bin import run_adx_bin_analysis
from analysis.bt_analysis_bb_band_bin import run_bb_band_bin_analysis
from analysis.bt_analysis_lr_band_bin import run_lr_band_bin_analysis
from analysis.bt_analysis_lr_angle_bin import run_lr_angle_bin_analysis
from analysis.bt_analysis_atr_bin import run_atr_bin_analysis
from analysis.bt_analysis_dmigap_bin import run_dmigap_bin_analysis
from analysis.bt_analysis_supertrend_bin import run_supertrend_bin_analysis
from analysis.bt_analysis_lr_angle_mtf import run_lr_angle_mtf_analysis
from analysis.bt_analysis_rsimfi_mtf import run_rsimfi_mtf_analysis
from analysis.bt_analysis_rsi_mtf import run_rsi_mtf_analysis
from analysis.bt_analysis_mfi_mtf import run_mfi_mtf_analysis
from analysis.bt_analysis_lr_mtf import run_lr_mtf_analysis
from analysis.bt_analysis_bb_mtf import run_bb_mtf_analysis

# ðŸ”¸ Ð ÐµÐµÑÑ‚Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²: (family_key, key) â†’ handler
ANALYSIS_HANDLERS: Dict[Tuple[str, str], AnalysisHandler] = {
    ("rsi", "rsi_bin"): run_rsi_bin_analysis,
    ("mfi", "mfi_bin"): run_mfi_bin_analysis,
    ("adx_dmi", "adx_bin"): run_adx_bin_analysis,
    ("bb", "bb_band_bin"): run_bb_band_bin_analysis,
    ("lr", "lr_band_bin"): run_lr_band_bin_analysis,
    ("lr", "lr_angle_bin"): run_lr_angle_bin_analysis,
    ("atr", "atr_bin"): run_atr_bin_analysis,
    ("adx_dmi", "dmigap_bin"): run_dmigap_bin_analysis,
    ("supertrend", "supertrend_bin"): run_supertrend_bin_analysis,
    ("lr", "lr_angle_mtf"): run_lr_angle_mtf_analysis,
    ("rsimfi", "rsimfi_mtf"): run_rsimfi_mtf_analysis,
    ("rsi", "rsi_mtf"): run_rsi_mtf_analysis,
    ("mfi", "mfi_mtf"): run_mfi_mtf_analysis,
    ("lr", "lr_mtf"): run_lr_mtf_analysis,
    ("bb", "bb_mtf"): run_bb_mtf_analysis,
}

# ðŸ”¸ ÐšÐ¾Ð½ÑÑ‚Ð°Ð½Ñ‚Ñ‹ ÑÑ‚Ñ€Ð¸Ð¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
ANALYSIS_STREAM_KEY = "bt:postproc:ready"
ANALYSIS_CONSUMER_GROUP = "bt_analysis"
ANALYSIS_CONSUMER_NAME = "bt_analysis_main"

# ðŸ”¸ Ð¡Ñ‚Ñ€Ð¸Ð¼ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
ANALYSIS_READY_STREAM_KEY = "bt:analysis:ready"

# ðŸ”¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð°
ANALYSIS_STREAM_BATCH_SIZE = 10
ANALYSIS_STREAM_BLOCK_MS = 5000

# ðŸ”¸ ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
ANALYSIS_MAX_CONCURRENCY = 12

# ðŸ”¸ ÐšÐµÑˆ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… finished_at Ð¿Ð¾ (scenario_id, signal_id) Ð´Ð»Ñ Ð¾Ñ‚ÑÐµÑ‡ÐºÐ¸ Ð´ÑƒÐ±Ð»ÐµÐ¹
_last_postproc_finished_at: Dict[Tuple[int, int], datetime] = {}

log = logging.getLogger("BT_ANALYSIS_MAIN")


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð°: Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
async def run_bt_analysis_orchestrator(pg, redis):
    log.debug("BT_ANALYSIS_MAIN: Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½")

    await _ensure_consumer_group(redis)

    # Ð¾Ð±Ñ‰Ð¸Ð¹ ÑÐµÐ¼Ð°Ñ„Ð¾Ñ€ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
    analysis_sema = asyncio.Semaphore(ANALYSIS_MAX_CONCURRENCY)

    # Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ñ‡Ñ‚ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
    while True:
        try:
            messages = await _read_from_stream(redis)

            if not messages:
                continue

            total_msgs = 0
            total_pairs = 0
            total_analyses_planned = 0
            total_analyses_ok = 0
            total_analyses_failed = 0
            total_rows_inserted = 0
            total_bins_rows = 0

            # ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ°Ð¼
            total_cleanup_raw = 0
            total_cleanup_bins = 0
            total_cleanup_model = 0
            total_cleanup_labels = 0
            total_cleanup_postproc = 0
            total_cleanup_scenario_stat = 0
            total_cleanup_total = 0

            for stream_key, entries in messages:
                if stream_key != ANALYSIS_STREAM_KEY:
                    # Ð½Ð° Ð²ÑÑÐºÐ¸Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹ Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‡ÑƒÐ¶Ð¸Ðµ ÑÑ‚Ñ€Ð¸Ð¼Ñ‹
                    continue

                for entry_id, fields in entries:
                    total_msgs += 1

                    ctx = _parse_postproc_message(fields)
                    if not ctx:
                        # ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ð¿Ð¾Ð»Ñ â€” ACK Ð¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼
                        await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)
                        continue

                    scenario_id = ctx["scenario_id"]
                    signal_id = ctx["signal_id"]
                    finished_at = ctx["finished_at"]

                    pair_key = (scenario_id, signal_id)
                    last_finished = _last_postproc_finished_at.get(pair_key)

                    # Ð¾Ñ‚ÑÐµÑ‡ÐºÐ° Ð´ÑƒÐ±Ð»ÐµÐ¹ Ð¿Ð¾ Ñ€Ð°Ð²Ð½Ð¾Ð¼Ñƒ finished_at
                    if last_finished is not None and last_finished == finished_at:
                        log.debug(
                            "BT_ANALYSIS_MAIN: Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð´Ð»Ñ scenario_id=%s, signal_id=%s, "
                            "finished_at=%s, stream_id=%s â€” Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ñ‹ Ð½Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÑŽÑ‚ÑÑ",
                            scenario_id,
                            signal_id,
                            finished_at,
                            entry_id,
                        )
                        await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)
                        continue

                    _last_postproc_finished_at[pair_key] = finished_at
                    total_pairs += 1

                    log.debug(
                        "BT_ANALYSIS_MAIN: Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÑ‚Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð° "
                        "scenario_id=%s, signal_id=%s, finished_at=%s, stream_id=%s",
                        scenario_id,
                        signal_id,
                        finished_at,
                        entry_id,
                    )

                    # Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð²ÑÐµÐ³Ð¾ ÐºÐ¾Ð½Ñ‚ÑƒÑ€Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð¿Ð¾ Ð¿Ð°Ñ€Ðµ (scenario_id, signal_id) Ð¿ÐµÑ€ÐµÐ´ Ð¿Ñ€Ð¾Ð³Ð¾Ð½Ð¾Ð¼ "Ñ Ñ‡Ð¸ÑÑ‚Ð¾Ð³Ð¾ Ð»Ð¸ÑÑ‚Ð°"
                    try:
                        cleanup = await _cleanup_analysis_tables_for_pair(pg, scenario_id, signal_id)
                        total_cleanup_raw += cleanup["raw"]
                        total_cleanup_bins += cleanup["bins"]
                        total_cleanup_model += cleanup["model_opt"]
                        total_cleanup_labels += cleanup["bins_labels"]
                        total_cleanup_postproc += cleanup["positions_postproc"]
                        total_cleanup_scenario_stat += cleanup["scenario_stat"]
                        total_cleanup_total += cleanup["total"]

                        log.info(
                            "BT_ANALYSIS_MAIN: cleanup Ð¿ÐµÑ€ÐµÐ´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼ scenario_id=%s, signal_id=%s â€” "
                            "deleted_raw=%s, deleted_bins=%s, deleted_model_opt=%s, deleted_bins_labels=%s, "
                            "deleted_positions_postproc=%s, deleted_scenario_stat=%s, deleted_total=%s",
                            scenario_id,
                            signal_id,
                            cleanup["raw"],
                            cleanup["bins"],
                            cleanup["model_opt"],
                            cleanup["bins_labels"],
                            cleanup["positions_postproc"],
                            cleanup["scenario_stat"],
                            cleanup["total"],
                        )
                    except Exception as e:
                        log.error(
                            "BT_ANALYSIS_MAIN: Ð¾ÑˆÐ¸Ð±ÐºÐ° cleanup Ð¿ÐµÑ€ÐµÐ´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼ scenario_id=%s, signal_id=%s: %s",
                            scenario_id,
                            signal_id,
                            e,
                            exc_info=True,
                        )

                    # Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ ÑÐ²ÑÐ·ÐºÐ¸ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ â†” ÑÐ¸Ð³Ð½Ð°Ð» â†” Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
                    links = get_analysis_connections_for_scenario_signal(scenario_id, signal_id)
                    if not links:
                        log.debug(
                            "BT_ANALYSIS_MAIN: Ð´Ð»Ñ scenario_id=%s, signal_id=%s Ð½ÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… ÑÐ²ÑÐ·Ð¾Ðº Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð², "
                            "ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ %s Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾Ð¼ÐµÑ‡ÐµÐ½Ð¾ ÐºÐ°Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ðµ",
                            scenario_id,
                            signal_id,
                            entry_id,
                        )
                        # Ð¿ÑƒÐ±Ð»Ð¸ÐºÑƒÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                        await _publish_analysis_ready(
                            redis=redis,
                            scenario_id=scenario_id,
                            signal_id=signal_id,
                            analyses_total=0,
                            analyses_ok=0,
                            analyses_failed=0,
                            rows_inserted=0,
                            bins_rows=0,
                        )
                        await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)
                        continue

                    # Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ°
                    analyses_to_run: List[Dict[str, Any]] = []
                    for link in links:
                        analysis_id = link.get("analysis_id")
                        analysis_cfg = get_analysis_instance(analysis_id)
                        if not analysis_cfg:
                            log.warning(
                                "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² ÐºÐµÑˆÐµ, "
                                "scenario_id=%s, signal_id=%s, ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ=%s",
                                analysis_id,
                                scenario_id,
                                signal_id,
                                entry_id,
                            )
                            continue

                        if not analysis_cfg.get("enabled"):
                            log.debug(
                                "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½, "
                                "scenario_id=%s, signal_id=%s, ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ=%s",
                                analysis_id,
                                scenario_id,
                                signal_id,
                                entry_id,
                            )
                            continue

                        analyses_to_run.append(analysis_cfg)

                    if not analyses_to_run:
                        log.debug(
                            "BT_ANALYSIS_MAIN: Ð´Ð»Ñ scenario_id=%s, signal_id=%s Ð½ÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² "
                            "(Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð° Ð¿Ð¾ ÐºÐµÑˆÑƒ), ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ %s Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾Ð¼ÐµÑ‡ÐµÐ½Ð¾ ÐºÐ°Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ðµ",
                            scenario_id,
                            signal_id,
                            entry_id,
                        )
                        await _publish_analysis_ready(
                            redis=redis,
                            scenario_id=scenario_id,
                            signal_id=signal_id,
                            analyses_total=0,
                            analyses_ok=0,
                            analyses_failed=0,
                            rows_inserted=0,
                            bins_rows=0,
                        )
                        await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)
                        continue

                    # Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ð´Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ð°Ñ€Ñ‹ Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾ ÑÐµÐ¼Ð°Ñ„Ð¾Ñ€Ñƒ
                    tasks: List[asyncio.Task] = []
                    for analysis in analyses_to_run:
                        task = asyncio.create_task(
                            _run_analysis_with_semaphore(
                                analysis=analysis,
                                scenario_id=scenario_id,
                                signal_id=signal_id,
                                pg=pg,
                                redis=redis,
                                sema=analysis_sema,
                            ),
                            name=f"BT_ANALYSIS_{analysis.get('id')}_SC_{scenario_id}_SIG_{signal_id}",
                        )
                        tasks.append(task)

                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    analyses_total = len(analyses_to_run)
                    analyses_ok = 0
                    analyses_failed = 0
                    rows_inserted = 0
                    bins_rows_for_pair = 0

                    for res in results:
                        if isinstance(res, Exception):
                            analyses_failed += 1
                            continue

                        status = res.get("status")
                        inserted = res.get("rows_inserted", 0)
                        bins_rows = res.get("bins_rows", 0)

                        if status == "ok":
                            analyses_ok += 1
                        elif status == "skipped":
                            # ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐºÐ°Ðº "ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, Ð½Ð¾ Ð±ÐµÐ· Ð´Ð°Ð½Ð½Ñ‹Ñ…"
                            analyses_ok += 1
                        else:
                            analyses_failed += 1

                        rows_inserted += inserted
                        bins_rows_for_pair += bins_rows

                    total_analyses_planned += analyses_total
                    total_analyses_ok += analyses_ok
                    total_analyses_failed += analyses_failed
                    total_rows_inserted += rows_inserted
                    total_bins_rows += bins_rows_for_pair

                    log.info(
                        "BT_ANALYSIS_MAIN: scenario_id=%s, signal_id=%s â€” Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð²ÑÐµÐ³Ð¾=%s, "
                        "ÑƒÑÐ¿ÐµÑˆÐ½Ð¾=%s, Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸=%s, ÑÑ‚Ñ€Ð¾Ðº Ð² raw=%s, ÑÑ‚Ñ€Ð¾Ðº Ð² bins_stat=%s",
                        scenario_id,
                        signal_id,
                        analyses_total,
                        analyses_ok,
                        analyses_failed,
                        rows_inserted,
                        bins_rows_for_pair,
                    )

                    # Ð¿ÑƒÐ±Ð»Ð¸ÐºÑƒÐµÐ¼ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð² bt:analysis:ready
                    await _publish_analysis_ready(
                        redis=redis,
                        scenario_id=scenario_id,
                        signal_id=signal_id,
                        analyses_total=analyses_total,
                        analyses_ok=analyses_ok,
                        analyses_failed=analyses_failed,
                        rows_inserted=rows_inserted,
                        bins_rows=bins_rows_for_pair,
                    )

                    # Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ ÐºÐ°Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ðµ Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
                    await redis.xack(ANALYSIS_STREAM_KEY, ANALYSIS_CONSUMER_GROUP, entry_id)

            log.debug(
                "BT_ANALYSIS_MAIN: Ð¿Ð°ÐºÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½ â€” ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹=%s, Ð¿Ð°Ñ€=%s, "
                "cleanup_total=%s, Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²_Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¾ÑÑŒ=%s, ÑƒÑÐ¿ÐµÑ…Ð¾Ð²=%s, Ð¾ÑˆÐ¸Ð±Ð¾Ðº=%s, ÑÑ‚Ñ€Ð¾Ðº_raw=%s, ÑÑ‚Ñ€Ð¾Ðº_bins=%s",
                total_msgs,
                total_pairs,
                total_cleanup_total,
                total_analyses_planned,
                total_analyses_ok,
                total_analyses_failed,
                total_rows_inserted,
                total_bins_rows,
            )
            log.info(
                "BT_ANALYSIS_MAIN: Ð¸Ñ‚Ð¾Ð³ Ð¿Ð¾ Ð¿Ð°ÐºÐµÑ‚Ñƒ â€” ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹=%s, Ð¿Ð°Ñ€=%s, "
                "cleanup_raw=%s, cleanup_bins=%s, cleanup_model_opt=%s, cleanup_bins_labels=%s, cleanup_positions_postproc=%s, cleanup_scenario_stat=%s, cleanup_total=%s, "
                "Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð²ÑÐµÐ³Ð¾=%s, ÑƒÑÐ¿ÐµÑˆÐ½Ð¾=%s, Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸=%s, ÑÑ‚Ñ€Ð¾Ðº Ð² raw=%s, ÑÑ‚Ñ€Ð¾Ðº Ð² bins_stat=%s",
                total_msgs,
                total_pairs,
                total_cleanup_raw,
                total_cleanup_bins,
                total_cleanup_model,
                total_cleanup_labels,
                total_cleanup_postproc,
                total_cleanup_scenario_stat,
                total_cleanup_total,
                total_analyses_planned,
                total_analyses_ok,
                total_analyses_failed,
                total_rows_inserted,
                total_bins_rows,
            )

        except Exception as e:
            log.error(
                "BT_ANALYSIS_MAIN: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¼ Ñ†Ð¸ÐºÐ»Ðµ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ð°: %s",
                e,
                exc_info=True,
            )
            # Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð¿Ð°ÑƒÐ·Ð° Ð¿ÐµÑ€ÐµÐ´ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¾Ð¹, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÐºÑ€ÑƒÑ‚Ð¸Ñ‚ÑŒ CPU Ð¿Ñ€Ð¸ Ð¿Ð¾ÑÑ‚Ð¾ÑÐ½Ð½Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐµ
            await asyncio.sleep(2)


# ðŸ”¸ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°/ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ consumer group Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
async def _ensure_consumer_group(redis) -> None:
    try:
        # Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ; MKSTREAM ÑÐ¾Ð·Ð´Ð°ÑÑ‚ ÑÑ‚Ñ€Ð¸Ð¼, ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ ÐµÑ‰Ñ‘ Ð½ÐµÑ‚
        await redis.xgroup_create(
            name=ANALYSIS_STREAM_KEY,
            groupname=ANALYSIS_CONSUMER_GROUP,
            id="$",
            mkstream=True,
        )
        log.debug(
            "BT_ANALYSIS_MAIN: ÑÐ¾Ð·Ð´Ð°Ð½Ð° consumer group '%s' Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° '%s'",
            ANALYSIS_CONSUMER_GROUP,
            ANALYSIS_STREAM_KEY,
        )
    except Exception as e:
        msg = str(e)
        if "BUSYGROUP" in msg:
            log.debug(
                "BT_ANALYSIS_MAIN: consumer group '%s' Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° '%s' ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚",
                ANALYSIS_CONSUMER_GROUP,
                ANALYSIS_STREAM_KEY,
            )
        else:
            log.error(
                "BT_ANALYSIS_MAIN: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ consumer group '%s': %s",
                ANALYSIS_CONSUMER_GROUP,
                e,
                exc_info=True,
            )
            raise


# ðŸ”¸ Ð§Ñ‚ÐµÐ½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸Ð· ÑÑ‚Ñ€Ð¸Ð¼Ð° bt:postproc:ready
async def _read_from_stream(redis) -> List[Any]:
    entries = await redis.xreadgroup(
        groupname=ANALYSIS_CONSUMER_GROUP,
        consumername=ANALYSIS_CONSUMER_NAME,
        streams={ANALYSIS_STREAM_KEY: ">"},
        count=ANALYSIS_STREAM_BATCH_SIZE,
        block=ANALYSIS_STREAM_BLOCK_MS,
    )

    if not entries:
        return []

    parsed: List[Any] = []
    for stream_key, messages in entries:
        if isinstance(stream_key, bytes):
            stream_key = stream_key.decode("utf-8")

        stream_entries: List[Any] = []
        for msg_id, fields in messages:
            if isinstance(msg_id, bytes):
                msg_id = msg_id.decode("utf-8")

            str_fields: Dict[str, str] = {}
            for k, v in fields.items():
                key_str = k.decode("utf-8") if isinstance(k, bytes) else str(k)
                val_str = v.decode("utf-8") if isinstance(v, bytes) else str(v)
                str_fields[key_str] = val_str

            stream_entries.append((msg_id, str_fields))

        parsed.append((stream_key, stream_entries))

    return parsed


# ðŸ”¸ Ð Ð°Ð·Ð±Ð¾Ñ€ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¸Ð· ÑÑ‚Ñ€Ð¸Ð¼Ð° bt:postproc:ready
def _parse_postproc_message(fields: Dict[str, str]) -> Optional[Dict[str, Any]]:
    try:
        scenario_id_str = fields.get("scenario_id")
        signal_id_str = fields.get("signal_id")
        finished_at_str = fields.get("finished_at")

        if not (scenario_id_str and signal_id_str and finished_at_str):
            # Ð½Ðµ Ñ…Ð²Ð°Ñ‚Ð°ÐµÑ‚ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÐ¹
            return None

        scenario_id = int(scenario_id_str)
        signal_id = int(signal_id_str)
        finished_at = datetime.fromisoformat(finished_at_str)

        # Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ (processed/skipped/errors) ÑÐµÐ¹Ñ‡Ð°Ñ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼, Ð½Ð¾ Ð¼Ð¾Ð¶ÐµÐ¼ Ð·Ð°Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        return {
            "scenario_id": scenario_id,
            "signal_id": signal_id,
            "finished_at": finished_at,
        }
    except Exception as e:
        log.error(
            "BT_ANALYSIS_MAIN: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°Ð·Ð±Ð¾Ñ€Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° bt:postproc:ready: %s, fields=%s",
            e,
            fields,
            exc_info=True,
        )
        return None


# ðŸ”¸ ÐžÐ±Ñ‘Ñ€Ñ‚ÐºÐ° Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ñ ÑÐµÐ¼Ð°Ñ„Ð¾Ñ€Ð¾Ð¼
async def _run_analysis_with_semaphore(
    analysis: Dict[str, Any],
    scenario_id: int,
    signal_id: int,
    pg,
    redis,
    sema: asyncio.Semaphore,
) -> Dict[str, Any]:
    async with sema:
        try:
            return await _run_single_analysis(
                analysis=analysis,
                scenario_id=scenario_id,
                signal_id=signal_id,
                pg=pg,
                redis=redis,
            )
        except Exception as e:
            analysis_id = analysis.get("id")
            family_key = analysis.get("family_key")
            key = analysis.get("key")
            log.error(
                "BT_ANALYSIS_MAIN: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° id=%s (family=%s, key=%s) "
                "Ð´Ð»Ñ scenario_id=%s, signal_id=%s: %s",
                analysis_id,
                family_key,
                key,
                scenario_id,
                signal_id,
                e,
                exc_info=True,
            )
            return {
                "analysis_id": analysis_id,
                "status": "error",
                "rows_inserted": 0,
                "bins_rows": 0,
            }


# ðŸ”¸ Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°: Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð², Ð·Ð°Ð¿ÑƒÑÐº Ð²Ð¾Ñ€ÐºÐµÑ€Ð°, Ð·Ð°Ð¿Ð¸ÑÑŒ raw Ð¸ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ bin-ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
async def _run_single_analysis(
    analysis: Dict[str, Any],
    scenario_id: int,
    signal_id: int,
    pg,
    redis,
) -> Dict[str, Any]:
    analysis_id = analysis.get("id")
    family_key = str(analysis.get("family_key") or "").strip()
    analysis_key = str(analysis.get("key") or "").strip()
    name = analysis.get("name")
    params = analysis.get("params") or {}

    handler = ANALYSIS_HANDLERS.get((family_key, analysis_key))
    if handler is None:
        log.debug(
            "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s (family=%s, key=%s, name=%s) Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ñ€ÐµÐµÑÑ‚Ñ€Ð¾Ð¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð²",
            analysis_id,
            family_key,
            analysis_key,
            name,
        )
        return {
            "analysis_id": analysis_id,
            "status": "skipped",
            "rows_inserted": 0,
            "bins_rows": 0,
        }

    # indicator_param â€” Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ rsi14 / rsi21 / lr50_angle Ð¸ Ñ‚.Ð¿.
    indicator_param_cfg = params.get("param_name")
    if indicator_param_cfg is not None:
        indicator_param = str(indicator_param_cfg.get("value") or "").strip() or None
    else:
        indicator_param = None

    log.debug(
        "BT_ANALYSIS_MAIN: Ð·Ð°Ð¿ÑƒÑÐº Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° id=%s (family=%s, key=%s, name=%s, indicator_param=%s) "
        "Ð´Ð»Ñ scenario_id=%s, signal_id=%s",
        analysis_id,
        family_key,
        analysis_key,
        name,
        indicator_param,
        scenario_id,
        signal_id,
    )

    # Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð´Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ Ð¿Ð°Ñ€Ñ‹ (ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¹, ÑÐ¸Ð³Ð½Ð°Ð»)
    async with pg.acquire() as conn:
        await conn.execute(
            """
            DELETE FROM bt_analysis_positions_raw
            WHERE analysis_id = $1
              AND scenario_id = $2
              AND signal_id = $3
            """,
            analysis_id,
            scenario_id,
            signal_id,
        )
        await conn.execute(
            """
            DELETE FROM bt_analysis_bins_stat
            WHERE analysis_id = $1
              AND scenario_id = $2
              AND signal_id = $3
            """,
            analysis_id,
            scenario_id,
            signal_id,
        )

    # ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
    analysis_ctx: Dict[str, Any] = {
        "scenario_id": scenario_id,
        "signal_id": signal_id,
    }

    # Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð±Ð¸Ð·Ð½ÐµÑ-Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
    result: Dict[str, Any] = await handler(analysis, analysis_ctx, pg, redis)
    rows: List[Dict[str, Any]] = (result or {}).get("rows") or []

    if not rows:
        log.debug(
            "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s (family=%s, key=%s) Ð´Ð»Ñ scenario_id=%s, signal_id=%s "
            "Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÐ» ÑÑ‚Ñ€Ð¾Ðº Ð´Ð»Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ (raw)",
            analysis_id,
            family_key,
            analysis_key,
            scenario_id,
            signal_id,
        )
        return {
            "analysis_id": analysis_id,
            "status": "ok",
            "rows_inserted": 0,
            "bins_rows": 0,
        }

    # Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð¹ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð² bt_analysis_positions_raw
    to_insert: List[Tuple[Any, ...]] = []
    for row in rows:
        position_uid = row.get("position_uid")
        timeframe = row.get("timeframe")
        direction = row.get("direction")
        bin_name = row.get("bin_name")
        value = row.get("value")
        pnl_abs = row.get("pnl_abs")

        if not position_uid or timeframe is None or direction is None or bin_name is None:
            # Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸
            log.debug(
                "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s (family=%s, key=%s) Ð²ÐµÑ€Ð½ÑƒÐ» Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ, "
                "Ð¾Ð½Ð° Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð°: %s",
                analysis_id,
                family_key,
                analysis_key,
                row,
            )
            continue

        to_insert.append(
            (
                analysis_id,
                position_uid,
                scenario_id,
                signal_id,
                family_key,
                analysis_key,
                timeframe,
                direction,
                bin_name,
                value,
                pnl_abs,
            )
        )

    inserted = 0
    bins_rows = 0

    if to_insert:
        async with pg.acquire() as conn:
            await conn.executemany(
                """
                INSERT INTO bt_analysis_positions_raw (
                    analysis_id,
                    position_uid,
                    scenario_id,
                    signal_id,
                    family_key,
                    "key",
                    timeframe,
                    direction,
                    bin_name,
                    value,
                    pnl_abs
                )
                VALUES (
                    $1, $2, $3, $4,
                    $5, $6, $7, $8,
                    $9, $10, $11
                )
                """,
                to_insert,
            )
        inserted = len(to_insert)

        # Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¿Ð¾ Ð±Ð¸Ð½Ð½Ð°Ð¼ Ð´Ð»Ñ Ð´Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ Ð¿Ð°Ñ€Ñ‹
        bins_rows, trades_total = await _recalc_bins_stat(
            pg=pg,
            analysis_id=analysis_id,
            scenario_id=scenario_id,
            signal_id=signal_id,
            indicator_param=indicator_param,
        )

        log.info(
            "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s (family=%s, key=%s, name=%s) "
            "Ð´Ð»Ñ scenario_id=%s, signal_id=%s Ð·Ð°Ð¿Ð¸ÑÐ°Ð» raw ÑÑ‚Ñ€Ð¾Ðº=%s Ð¸ bins ÑÑ‚Ñ€Ð¾Ðº=%s (trades_total=%s)",
            analysis_id,
            family_key,
            analysis_key,
            name,
            scenario_id,
            signal_id,
            inserted,
            bins_rows,
            trades_total,
        )
    else:
        log.debug(
            "BT_ANALYSIS_MAIN: Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ id=%s (family=%s, key=%s, name=%s) Ð´Ð»Ñ scenario_id=%s, signal_id=%s "
            "Ð½Ðµ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð» Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ñ… ÑÑ‚Ñ€Ð¾Ðº raw Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸",
            analysis_id,
            family_key,
            analysis_key,
            name,
            scenario_id,
            signal_id,
        )

    return {
        "analysis_id": analysis_id,
        "status": "ok",
        "rows_inserted": inserted,
        "bins_rows": bins_rows,
    }


# ðŸ”¸ ÐŸÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¿Ð¾ Ð±Ð¸Ð½Ð½Ð°Ð¼ Ð² bt_analysis_bins_stat
async def _recalc_bins_stat(
    pg,
    analysis_id: int,
    scenario_id: int,
    signal_id: int,
    indicator_param: Optional[str],
) -> Tuple[int, int]:
    async with pg.acquire() as conn:
        rows = await conn.fetch(
            """
            SELECT
                timeframe,
                direction,
                bin_name,
                COUNT(*)                                         AS trades,
                COUNT(*) FILTER (WHERE pnl_abs > 0)              AS wins,
                COALESCE(SUM(pnl_abs), 0)                        AS pnl_abs_total
            FROM bt_analysis_positions_raw
            WHERE analysis_id = $1
              AND scenario_id = $2
              AND signal_id   = $3
            GROUP BY timeframe, direction, bin_name
            """,
            analysis_id,
            scenario_id,
            signal_id,
        )

        if not rows:
            log.debug(
                "BT_ANALYSIS_MAIN: Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° id=%s, scenario_id=%s, signal_id=%s Ð½ÐµÑ‚ ÑÑ‚Ñ€Ð¾Ðº raw Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚Ð° bins_stat",
                analysis_id,
                scenario_id,
                signal_id,
            )
            return 0, 0

        # ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸ bins_stat Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸ Ð¿Ð°Ñ€Ñ‹
        await conn.execute(
            """
            DELETE FROM bt_analysis_bins_stat
            WHERE analysis_id = $1
              AND scenario_id = $2
              AND signal_id   = $3
            """,
            analysis_id,
            scenario_id,
            signal_id,
        )

        to_insert: List[Tuple[Any, ...]] = []
        total_trades = 0

        for r in rows:
            timeframe = r["timeframe"]
            direction = r["direction"]
            bin_name = r["bin_name"]
            trades = int(r["trades"])
            wins = int(r["wins"])
            pnl_abs_total = Decimal(str(r["pnl_abs_total"]))

            total_trades += trades

            if trades > 0:
                winrate = Decimal(wins) / Decimal(trades)
            else:
                winrate = Decimal("0")

            # Ð»Ñ‘Ð³ÐºÐ°Ñ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·ÑƒÐµÐ¼Ð¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸
            pnl_abs_q = pnl_abs_total.quantize(Decimal("0.0001"))
            winrate_q = winrate.quantize(Decimal("0.0001"))

            to_insert.append(
                (
                    analysis_id,
                    scenario_id,
                    signal_id,
                    indicator_param,
                    timeframe,
                    direction,
                    bin_name,
                    trades,
                    pnl_abs_q,
                    winrate_q,
                )
            )

        await conn.executemany(
            """
            INSERT INTO bt_analysis_bins_stat (
                analysis_id,
                scenario_id,
                signal_id,
                indicator_param,
                timeframe,
                direction,
                bin_name,
                trades,
                pnl_abs,
                winrate
            )
            VALUES (
                $1, $2, $3, $4,
                $5, $6, $7,
                $8, $9, $10
            )
            """,
            to_insert,
        )

    bins_count = len(to_insert)

    log.debug(
        "BT_ANALYSIS_MAIN: Ð¿ÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ð°Ð½Ð° ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° bins_stat Ð´Ð»Ñ analysis_id=%s, scenario_id=%s, signal_id=%s â€” "
        "Ð±Ð¸Ð½Ð¾Ð²=%s, trades_total=%s",
        analysis_id,
        scenario_id,
        signal_id,
        bins_count,
        total_trades,
    )
    return bins_count, total_trades


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð² bt:analysis:ready
async def _publish_analysis_ready(
    redis,
    scenario_id: int,
    signal_id: int,
    analyses_total: int,
    analyses_ok: int,
    analyses_failed: int,
    rows_inserted: int,
    bins_rows: int,
) -> None:
    finished_at = datetime.utcnow()

    try:
        await redis.xadd(
            ANALYSIS_READY_STREAM_KEY,
            {
                "scenario_id": str(scenario_id),
                "signal_id": str(signal_id),
                "analyses_total": str(analyses_total),
                "analyses_ok": str(analyses_ok),
                "analyses_failed": str(analyses_failed),
                "rows_raw": str(rows_inserted),
                "rows_bins": str(bins_rows),
                "finished_at": finished_at.isoformat(),
            },
        )
        log.debug(
            "BT_ANALYSIS_MAIN: Ð¾Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ð½Ð¾ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð² ÑÑ‚Ñ€Ð¸Ð¼ '%s' "
            "Ð´Ð»Ñ scenario_id=%s, signal_id=%s, analyses_total=%s, analyses_ok=%s, "
            "analyses_failed=%s, rows_raw=%s, rows_bins=%s, finished_at=%s",
            ANALYSIS_READY_STREAM_KEY,
            scenario_id,
            signal_id,
            analyses_total,
            analyses_ok,
            analyses_failed,
            rows_inserted,
            bins_rows,
            finished_at,
        )
    except Exception as e:
        log.error(
            "BT_ANALYSIS_MAIN: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¸Ð¼ '%s' "
            "Ð´Ð»Ñ scenario_id=%s, signal_id=%s: %s",
            ANALYSIS_READY_STREAM_KEY,
            scenario_id,
            signal_id,
            e,
            exc_info=True,
        )

# ðŸ”¸ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð²ÑÐµÑ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÐºÐ¾Ð½Ñ‚ÑƒÑ€Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð¿Ð¾ Ð¿Ð°Ñ€Ðµ (scenario_id, signal_id)
async def _cleanup_analysis_tables_for_pair(pg, scenario_id: int, signal_id: int) -> Dict[str, int]:
    deleted_raw = 0
    deleted_bins = 0
    deleted_model = 0
    deleted_labels = 0
    deleted_postproc = 0
    deleted_scenario_stat = 0
    deleted_adaptive_bins = 0

    async with pg.acquire() as conn:
        # Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
        async with conn.transaction():
            # ÑÐ½Ð°Ñ‡Ð°Ð»Ð° labels (Ð½Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ ON DELETE CASCADE Ð¿Ð¾ model_id)
            res_labels = await conn.execute(
                """
                DELETE FROM bt_analysis_bins_labels
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_labels = _parse_pg_execute_count(res_labels)

            # Ð·Ð°Ñ‚ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð¸ Ð²ÑÑ‘, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÐ²ÑÐ·Ð°Ð½Ð¾ ÐºÐ°ÑÐºÐ°Ð´Ð¾Ð¼)
            res_model = await conn.execute(
                """
                DELETE FROM bt_analysis_model_opt
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_model = _parse_pg_execute_count(res_model)

            # Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð±Ð¸Ð½Ð½Ð¾Ð² (Ð¿ÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð° ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´)
            res_adapt = await conn.execute(
                """
                DELETE FROM bt_analysis_bin_dict_adaptive
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_adaptive_bins = _parse_pg_execute_count(res_adapt)

            # Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹
            res_postproc = await conn.execute(
                """
                DELETE FROM bt_analysis_positions_postproc
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_postproc = _parse_pg_execute_count(res_postproc)

            # Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑŽ
            res_sc_stat = await conn.execute(
                """
                DELETE FROM bt_analysis_scenario_stat
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_scenario_stat = _parse_pg_execute_count(res_sc_stat)

            # ÑÑ‹Ñ€Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¸ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ñ‹ Ð¿Ð¾ Ð±Ð¸Ð½Ð½Ð°Ð¼
            res_bins = await conn.execute(
                """
                DELETE FROM bt_analysis_bins_stat
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_bins = _parse_pg_execute_count(res_bins)

            res_raw = await conn.execute(
                """
                DELETE FROM bt_analysis_positions_raw
                WHERE scenario_id = $1
                  AND signal_id   = $2
                """,
                scenario_id,
                signal_id,
            )
            deleted_raw = _parse_pg_execute_count(res_raw)

    return {
        "raw": deleted_raw,
        "bins": deleted_bins,
        "model_opt": deleted_model,
        "bins_labels": deleted_labels,
        "positions_postproc": deleted_postproc,
        "scenario_stat": deleted_scenario_stat,
        "adaptive_bins": deleted_adaptive_bins,
        "total": (
            deleted_raw
            + deleted_bins
            + deleted_model
            + deleted_labels
            + deleted_postproc
            + deleted_scenario_stat
            + deleted_adaptive_bins
        ),
    }

# ðŸ”¸ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° asyncpg conn.execute Ð²Ð¸Ð´Ð° "DELETE 123"
def _parse_pg_execute_count(res: Any) -> int:
    try:
        return int(str(res).split()[-1])
    except Exception:
        return 0