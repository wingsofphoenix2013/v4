# bt_signals_cache_config_v2.py â€” ÐºÐµÑˆ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² live-ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð² v2 (good bins) Ð¿Ð¾ mirror1/mirror2 (scenario_id/signal_id/run_id) Ð´Ð»Ñ backtester_v1

import asyncio
import logging
from typing import Any, Dict, Optional, Set, Tuple, List

# ðŸ”¸ ÐšÐµÑˆÐ¸ Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð³ backtester_v1
from backtester_config import get_enabled_signals

log = logging.getLogger("BT_SIG_CACHE_V2")

# ðŸ”¸ Ð¡Ñ‚Ñ€Ð¸Ð¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¹ v2 (Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ postproc v2)
ANALYSIS_POSTPROC_STREAM_KEY_V2 = "bt:analysis:postproc_ready_v2"
CACHE_CONSUMER_GROUP_V2 = "bt_signals_cache_v2"
CACHE_CONSUMER_NAME_V2 = "bt_signals_cache_v2_main"

CACHE_STREAM_BATCH_SIZE = 50
CACHE_STREAM_BLOCK_MS = 5000

# ðŸ”¸ Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° labels v2
LABELS_V2_TABLE = "bt_analysis_bins_labels_v2"
SCORE_VERSION = "v1"

# ðŸ”¸ ÐšÐµÑˆ: ÐºÐ»ÑŽÑ‡ (scenario_id, signal_id, direction) -> required_pairs + good_bins_map + Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼Ñ‹Ð¹ run_id
bt_mirror_required_pairs_v2: Dict[Tuple[int, int, str], Set[Tuple[int, str]]] = {}
bt_mirror_good_bins_map_v2: Dict[Tuple[int, int, str], Dict[Tuple[int, str], Set[str]]] = {}
bt_mirror_run_id_v2: Dict[Tuple[int, int, str], int] = {}

# ðŸ”¸ Ð˜Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirror-Ð¿Ð°Ñ€ (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð»Ð¸ÑˆÐ½ÐµÐµ)
_active_mirrors_v2: Set[Tuple[int, int, str]] = set()


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð³ÐµÑ‚Ñ‚ÐµÑ€: Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐµÑˆ good bins Ð´Ð»Ñ mirror (scenario_id, signal_id, direction)
def get_mirror_label_cache_v2(
    mirror_scenario_id: int,
    mirror_signal_id: int,
    direction: str,
) -> Tuple[
    Optional[Set[Tuple[int, str]]],
    Optional[Dict[Tuple[int, str], Set[str]]],
]:
    key = (int(mirror_scenario_id), int(mirror_signal_id), str(direction).strip().lower())
    return (
        bt_mirror_required_pairs_v2.get(key),
        bt_mirror_good_bins_map_v2.get(key),
    )


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð³ÐµÑ‚Ñ‚ÐµÑ€: Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼Ñ‹Ð¹ run_id Ð´Ð»Ñ mirror
def get_mirror_run_id_v2(
    mirror_scenario_id: int,
    mirror_signal_id: int,
    direction: str,
) -> Optional[int]:
    key = (int(mirror_scenario_id), int(mirror_signal_id), str(direction).strip().lower())
    return bt_mirror_run_id_v2.get(key)


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´: Ð¿ÐµÑ€ÐµÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors Ð¿Ð¾ enabled live-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ°Ð¼
def rebuild_active_mirrors_index_v2() -> Set[Tuple[int, int, str]]:
    mirrors: Set[Tuple[int, int, str]] = set()

    signals = get_enabled_signals()
    for s in signals:
        mode = str(s.get("mode") or "").strip().lower()
        if mode != "live":
            continue

        params = s.get("params") or {}

        # direction Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· live Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ° (direction_mask)
        dm_cfg = params.get("direction_mask")
        if not dm_cfg:
            continue
        direction = str(dm_cfg.get("value") or "").strip().lower()
        if direction not in ("long", "short"):
            continue

        # mirror layer 1
        m1s_cfg = params.get("mirror1_scenario_id")
        m1i_cfg = params.get("mirror1_signal_id")
        if m1s_cfg and m1i_cfg:
            try:
                sc1 = int(m1s_cfg.get("value"))
                si1 = int(m1i_cfg.get("value"))
                if sc1 > 0 and si1 > 0:
                    mirrors.add((sc1, si1, direction))
            except Exception:
                pass

        # mirror layer 2
        m2s_cfg = params.get("mirror2_scenario_id")
        m2i_cfg = params.get("mirror2_signal_id")
        if m2s_cfg and m2i_cfg:
            try:
                sc2 = int(m2s_cfg.get("value"))
                si2 = int(m2i_cfg.get("value"))
                if sc2 > 0 and si2 > 0:
                    mirrors.add((sc2, si2, direction))
            except Exception:
                pass

    global _active_mirrors_v2
    _active_mirrors_v2 = mirrors
    return mirrors


# ðŸ”¸ ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´: Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐºÐµÑˆÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors (Ð±ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ run_id Ð¸Ð· labels_v2)
async def load_initial_mirror_caches_v2(pg) -> None:
    mirrors = rebuild_active_mirrors_index_v2()

    if not mirrors:
        log.info("BT_SIG_CACHE_V2: Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirror-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ â€” ÐºÐµÑˆ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² v2 Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ÑÑ")
        return

    loaded = 0
    total_required = 0
    total_good_bins = 0

    for (sc_id, sig_id, direction) in sorted(mirrors):
        run_id = await _load_latest_run_id_for_pair(pg, sc_id, sig_id, direction)
        if run_id is None:
            continue

        req, good_map = await _load_good_bins_for_pair(pg, sc_id, sig_id, direction, run_id)
        _store_cache(sc_id, sig_id, direction, req, good_map, run_id)

        loaded += 1
        total_required += len(req)
        total_good_bins += sum(len(v) for v in good_map.values())

    log.info(
        "BT_SIG_CACHE_V2: initial cache loaded â€” mirrors=%s, required_pairs=%s, good_bins=%s",
        loaded,
        total_required,
        total_good_bins,
    )


# ðŸ”¸ Ð’Ð¾Ñ€ÐºÐµÑ€ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÐµÑˆÐ° Ð¿Ð¾ ÑÑ‚Ñ€Ð¸Ð¼Ñƒ bt:analysis:postproc_ready_v2
async def run_bt_signals_cache_watcher_v2(pg, redis) -> None:
    log.debug("BT_SIG_CACHE_V2: watcher Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ (bt:analysis:postproc_ready_v2)")

    await load_initial_mirror_caches_v2(pg)
    await _ensure_consumer_group(redis)

    while True:
        try:
            try:
                entries = await redis.xreadgroup(
                    groupname=CACHE_CONSUMER_GROUP_V2,
                    consumername=CACHE_CONSUMER_NAME_V2,
                    streams={ANALYSIS_POSTPROC_STREAM_KEY_V2: ">"},
                    count=CACHE_STREAM_BATCH_SIZE,
                    block=CACHE_STREAM_BLOCK_MS,
                )
            except Exception as e:
                msg = str(e)
                if "NOGROUP" in msg:
                    log.warning(
                        "BT_SIG_CACHE_V2: NOGROUP Ð¿Ñ€Ð¸ XREADGROUP â€” Ð¿ÐµÑ€ÐµÐ¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ Ð¸ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼",
                    )
                    await _ensure_consumer_group(redis)
                    continue
                raise

            if not entries:
                continue

            total_msgs = 0
            refreshed = 0
            ignored = 0

            for _, messages in entries:
                for msg_id, fields in messages:
                    total_msgs += 1

                    ctx = _parse_postproc_ready_v2(fields)
                    if not ctx:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY_V2, CACHE_CONSUMER_GROUP_V2, msg_id)
                        ignored += 1
                        continue

                    scenario_id = ctx["scenario_id"]
                    signal_id = ctx["signal_id"]
                    run_id = ctx["run_id"]
                    finished_at = ctx.get("finished_at")

                    # Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… mirrors (Ð½Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ/Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ live-Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ¾Ð²)
                    mirrors = rebuild_active_mirrors_index_v2()

                    # Ð½Ð°Ð¼ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ°Ðº mirror
                    targets = [
                        (scenario_id, signal_id, d)
                        for d in ("long", "short")
                        if (scenario_id, signal_id, d) in mirrors
                    ]

                    if not targets:
                        await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY_V2, CACHE_CONSUMER_GROUP_V2, msg_id)
                        ignored += 1
                        continue

                    for (sc_id, sig_id, direction) in targets:
                        req, good_map = await _load_good_bins_for_pair(pg, sc_id, sig_id, direction, run_id)
                        _store_cache(sc_id, sig_id, direction, req, good_map, run_id)
                        refreshed += 1

                        log.info(
                            "BT_SIG_CACHE_V2: cache refreshed â€” mirror=%s:%s:%s, run_id=%s, required_pairs=%s, good_bins=%s, finished_at=%s",
                            sc_id,
                            sig_id,
                            direction,
                            run_id,
                            len(req),
                            sum(len(v) for v in good_map.values()),
                            finished_at,
                        )

                    await redis.xack(ANALYSIS_POSTPROC_STREAM_KEY_V2, CACHE_CONSUMER_GROUP_V2, msg_id)

            if total_msgs:
                log.info(
                    "BT_SIG_CACHE_V2: batch processed â€” msgs=%s, refreshed=%s, ignored=%s",
                    total_msgs,
                    refreshed,
                    ignored,
                )

        except Exception as e:
            log.error("BT_SIG_CACHE_V2: watcher loop error: %s", e, exc_info=True)
            await asyncio.sleep(2)


# ðŸ”¸ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°/ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ consumer group
async def _ensure_consumer_group(redis) -> None:
    try:
        await redis.xgroup_create(
            name=ANALYSIS_POSTPROC_STREAM_KEY_V2,
            groupname=CACHE_CONSUMER_GROUP_V2,
            id="$",
            mkstream=True,
        )
        log.debug(
            "BT_SIG_CACHE_V2: ÑÐ¾Ð·Ð´Ð°Ð½Ð° consumer group '%s' Ð´Ð»Ñ ÑÑ‚Ñ€Ð¸Ð¼Ð° '%s'",
            CACHE_CONSUMER_GROUP_V2,
            ANALYSIS_POSTPROC_STREAM_KEY_V2,
        )
    except Exception as e:
        msg = str(e)
        if "BUSYGROUP" in msg:
            log.info(
                "BT_SIG_CACHE_V2: consumer group '%s' ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ â€” SETID '$' Ð´Ð»Ñ Ð¸Ð³Ð½Ð¾Ñ€Ð° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð¾ ÑÑ‚Ð°Ñ€Ñ‚Ð°",
                CACHE_CONSUMER_GROUP_V2,
            )

            await redis.execute_command(
                "XGROUP",
                "SETID",
                ANALYSIS_POSTPROC_STREAM_KEY_V2,
                CACHE_CONSUMER_GROUP_V2,
                "$",
            )

            log.debug(
                "BT_SIG_CACHE_V2: consumer group '%s' SETID='$' Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ Ð´Ð»Ñ '%s'",
                CACHE_CONSUMER_GROUP_V2,
                ANALYSIS_POSTPROC_STREAM_KEY_V2,
            )
        else:
            log.error(
                "BT_SIG_CACHE_V2: Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ consumer group '%s': %s",
                CACHE_CONSUMER_GROUP_V2,
                e,
                exc_info=True,
            )
            raise


# ðŸ”¸ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ postproc_ready_v2 (run-aware)
def _parse_postproc_ready_v2(fields: Dict[Any, Any]) -> Optional[Dict[str, Any]]:
    try:
        # redis Ð¼Ð¾Ð¶ÐµÑ‚ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ bytes
        def _s(x):
            if isinstance(x, bytes):
                return x.decode("utf-8")
            return str(x)

        scenario_id = int(_s(fields.get("scenario_id")))
        signal_id = int(_s(fields.get("signal_id")))
        run_id = int(_s(fields.get("run_id")))

        fa = fields.get("finished_at")
        finished_at = _s(fa) if fa is not None else None

        return {
            "scenario_id": scenario_id,
            "signal_id": signal_id,
            "run_id": run_id,
            "finished_at": finished_at,
        }
    except Exception:
        return None


# ðŸ”¸ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ run_id Ð´Ð»Ñ mirror-Ð¿Ð°Ñ€Ñ‹ Ð¸Ð· labels_v2 (good-only)
async def _load_latest_run_id_for_pair(
    pg,
    scenario_id: int,
    signal_id: int,
    direction: str,
) -> Optional[int]:
    direction_l = str(direction).strip().lower()

    async with pg.acquire() as conn:
        row = await conn.fetchrow(
            f"""
            SELECT MAX(run_id) AS run_id
            FROM {LABELS_V2_TABLE}
            WHERE scenario_id   = $1
              AND signal_id     = $2
              AND direction     = $3
              AND score_version = $4
              AND state         = 'good'
            """,
            int(scenario_id),
            int(signal_id),
            direction_l,
            str(SCORE_VERSION),
        )

    if not row or row["run_id"] is None:
        return None

    try:
        return int(row["run_id"])
    except Exception:
        return None


# ðŸ”¸ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° good bins Ð¸Ð· bt_analysis_bins_labels_v2 Ð´Ð»Ñ mirror-Ð¿Ð°Ñ€Ñ‹ Ð¸ direction, ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ run_id
async def _load_good_bins_for_pair(
    pg,
    scenario_id: int,
    signal_id: int,
    direction: str,
    run_id: int,
) -> Tuple[
    Set[Tuple[int, str]],
    Dict[Tuple[int, str], Set[str]],
]:
    direction_l = str(direction).strip().lower()

    async with pg.acquire() as conn:
        rows = await conn.fetch(
            f"""
            SELECT analysis_id, timeframe, bin_name
            FROM {LABELS_V2_TABLE}
            WHERE scenario_id   = $1
              AND signal_id     = $2
              AND direction     = $3
              AND run_id        = $4
              AND score_version = $5
              AND state         = 'good'
            """,
            int(scenario_id),
            int(signal_id),
            direction_l,
            int(run_id),
            str(SCORE_VERSION),
        )

    required_pairs: Set[Tuple[int, str]] = set()
    good_bins_map: Dict[Tuple[int, str], Set[str]] = {}

    for r in rows:
        aid = int(r["analysis_id"])
        tf = str(r["timeframe"]).strip().lower()
        bn = str(r["bin_name"])

        pair = (aid, tf)
        required_pairs.add(pair)
        good_bins_map.setdefault(pair, set()).add(bn)

    return required_pairs, good_bins_map


# ðŸ”¸ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ ÐºÐµÑˆÐ° (Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼Ñ‹Ð¹ run_id)
def _store_cache(
    scenario_id: int,
    signal_id: int,
    direction: str,
    required_pairs: Set[Tuple[int, str]],
    good_bins_map: Dict[Tuple[int, str], Set[str]],
    run_id: int,
) -> None:
    key = (int(scenario_id), int(signal_id), str(direction).strip().lower())
    bt_mirror_required_pairs_v2[key] = set(required_pairs)
    bt_mirror_good_bins_map_v2[key] = {k: set(v) for k, v in good_bins_map.items()}
    bt_mirror_run_id_v2[key] = int(run_id)